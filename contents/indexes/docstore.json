{"docstore/data": {"b144d770-ed66-439d-b796-339942c740a9": {"__data__": {"id_": "b144d770-ed66-439d-b796-339942c740a9", "embedding": null, "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b899e98b40a00777ab41aa45a3976ce07da9c95c855ee28dddc7575ed7cbfa28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "883ba310-18f0-4fcb-ba31-17848a93338d", "node_type": "1", "metadata": {}, "hash": "668baf02eef54f328c309d7c4d741d223c52e1cc44c91a747a09596d9c7fff6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 W ORDS :\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy\u2217,\u2020, Lucas Beyer\u2217, Alexander Kolesnikov\u2217, Dirk Weissenborn\u2217,\nXiaohua Zhai\u2217, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\u2217,\u2020\n\u2217equal technical contribution, \u2020equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "883ba310-18f0-4fcb-ba31-17848a93338d": {"__data__": {"id_": "883ba310-18f0-4fcb-ba31-17848a93338d", "embedding": null, "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b899e98b40a00777ab41aa45a3976ce07da9c95c855ee28dddc7575ed7cbfa28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b144d770-ed66-439d-b796-339942c740a9", "node_type": "1", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "df13153167bfa09cae6efeedd1523ccee99c5ae6e1b10771c194f1922eea3f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66", "node_type": "1", "metadata": {}, "hash": "6acab5aa8c24e2aa373047cbfafd4ed4eda0e14f0d8130181ddfbf702312742f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1 I NTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then \ufb01ne-tune on a smaller task-speci\ufb01c dataset (Devlin et al., 2019). Thanks\nto Transformers\u2019 computational ef\ufb01ciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016).", "mimetype": "text/plain", "start_char_idx": 813, "end_char_idx": 2047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66": {"__data__": {"id_": "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66", "embedding": null, "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b899e98b40a00777ab41aa45a3976ce07da9c95c855ee28dddc7575ed7cbfa28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "883ba310-18f0-4fcb-ba31-17848a93338d", "node_type": "1", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "e3a36628186d7bd0f8399ae1d38439c8a8d852a81ad1c7ea90597113b407faf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90cd2df7-b681-4b99-9a45-c4bbb01d4ade", "node_type": "1", "metadata": {}, "hash": "4e0b9f546282ec22a5b77bbff5bf9eba77e5838c901db97b9911eefc2b00e420", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically ef\ufb01cient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modi\ufb01cations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classi\ufb01cation in supervised fashion.", "mimetype": "text/plain", "start_char_idx": 1823, "end_char_idx": 3065, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90cd2df7-b681-4b99-9a45-c4bbb01d4ade": {"__data__": {"id_": "90cd2df7-b681-4b99-9a45-c4bbb01d4ade", "embedding": null, "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b899e98b40a00777ab41aa45a3976ce07da9c95c855ee28dddc7575ed7cbfa28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66", "node_type": "1", "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "c2933915ce726f7894471f4af04395e49e349abe6030c8b4b496ded454450b59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classi\ufb01cation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning code and pre-trained models are available at https://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021", "mimetype": "text/plain", "start_char_idx": 2785, "end_char_idx": 3503, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "563d970a-c800-4cee-8285-490e75ba2b8c": {"__data__": {"id_": "563d970a-c800-4cee-8285-490e75ba2b8c", "embedding": null, "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af81649dd7361dd17e4d79bf495d5e94d79d2cdc5fd742148bc800b83c8d638a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f418125-d917-46db-9f20-6335ed7f06cb", "node_type": "1", "metadata": {}, "hash": "21f51288a07f5cc8bde04b65f1e7c35ca68251b7cb976bd44210ef3736298e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insuf\ufb01cient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\n\ufb01nd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at suf\ufb01cient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2 R ELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f418125-d917-46db-9f20-6335ed7f06cb": {"__data__": {"id_": "3f418125-d917-46db-9f20-6335ed7f06cb", "embedding": null, "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af81649dd7361dd17e4d79bf495d5e94d79d2cdc5fd742148bc800b83c8d638a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "563d970a-c800-4cee-8285-490e75ba2b8c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "45e044e0cf759102cb3e8b58bf7db120a079f3460d1e165694f065de8b61994e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf043cec-f9e4-4b0a-9293-9852b27bb969", "node_type": "1", "metadata": {}, "hash": "27a4c5a2dc8d435d87a65d7fc88114f14d2556b25ef5cb2899836889aa176b1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 R ELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then \ufb01ne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images.", "mimetype": "text/plain", "start_char_idx": 811, "end_char_idx": 2043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf043cec-f9e4-4b0a-9293-9852b27bb969": {"__data__": {"id_": "bf043cec-f9e4-4b0a-9293-9852b27bb969", "embedding": null, "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af81649dd7361dd17e4d79bf495d5e94d79d2cdc5fd742148bc800b83c8d638a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f418125-d917-46db-9f20-6335ed7f06cb", "node_type": "1", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "cb19b1dd7e15ae217fae393166f486501a7c5ef96279d5ffa565e1778d9e4707", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46620723-f0be-43c2-86b4-f9b00894b230", "node_type": "1", "metadata": {}, "hash": "982729bd9ef4c39a476b02e8a36b4bdce6454a40c2225ecc8bebe9571cfe65cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nef\ufb01ciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 \u00d72\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 \u00d72 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g.", "mimetype": "text/plain", "start_char_idx": 1723, "end_char_idx": 3098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46620723-f0be-43c2-86b4-f9b00894b230": {"__data__": {"id_": "46620723-f0be-43c2-86b4-f9b00894b230", "embedding": null, "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af81649dd7361dd17e4d79bf495d5e94d79d2cdc5fd742148bc800b83c8d638a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf043cec-f9e4-4b0a-9293-9852b27bb969", "node_type": "1", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "2c04e46613b5f4e832b7b68d7c6fbe50adc4c1201717736ad4ae73a8f4c317ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a103c75-ea57-4da0-a080-5de28ef59e30", "node_type": "1", "metadata": {}, "hash": "cd2a184661f332a923052ae93bdb616217d5495b3795fb8d89eb33d4a1a894ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 \u00d72 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classi\ufb01cation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classi\ufb01cation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uni\ufb01ed text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019).\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be \ufb01ne-tuned or\nprobed linearly for classi\ufb01cation performance, achieving a maximal accuracy of 72% on ImageNet.", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 3928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a103c75-ea57-4da0-a080-5de28ef59e30": {"__data__": {"id_": "2a103c75-ea57-4da0-a080-5de28ef59e30", "embedding": null, "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af81649dd7361dd17e4d79bf495d5e94d79d2cdc5fd742148bc800b83c8d638a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46620723-f0be-43c2-86b4-f9b00894b230", "node_type": "1", "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "f57364fdd673f6eb6435d20440cbc81cdc577d89c5c1399699b92e0b55ec006d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be \ufb01ne-tuned or\nprobed linearly for classi\ufb01cation performance, achieving a maximal accuracy of 72% on ImageNet.\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works.\n2", "mimetype": "text/plain", "start_char_idx": 3539, "end_char_idx": 4607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50f0260c-d4e9-44ea-8ff3-275f316b76c1": {"__data__": {"id_": "50f0260c-d4e9-44ea-8ff3-275f316b76c1", "embedding": null, "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeeb9665-8572-464a-9fff-49286047c93b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "153a23ada16fa0c249b74d6f3929309398b5d78f3797d8ffc8d0ea070c2d8c98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "640874fe-676f-47b9-a525-d40b98e5972e", "node_type": "1", "metadata": {}, "hash": "df5751c928cdbf706ed6ea37fefdd54f762aba134fb4afefc77854998701f495", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nTransformer \nEncoder\nMLP \nHead\n Vision Transformer (ViT)\n*\nLinear \nProjection \nof \nFlattened \nPatches\n*\n \nExtra \nlearnable\n     \n[class]\n \nembedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch \n+ \nPosition \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL\n \nx\n+\n Transformer Encoder\nFigure 1: Model overview. We split an image into \ufb01xed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classi\ufb01cation, we use the standard approach of adding an extra learnable\n\u201cclassi\ufb01cation token\u201d to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3 M ETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and\ntheir ef\ufb01cient implementations \u2013 can be used almost out of the box.\n3.1 V ISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "640874fe-676f-47b9-a525-d40b98e5972e": {"__data__": {"id_": "640874fe-676f-47b9-a525-d40b98e5972e", "embedding": null, "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeeb9665-8572-464a-9fff-49286047c93b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "153a23ada16fa0c249b74d6f3929309398b5d78f3797d8ffc8d0ea070c2d8c98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50f0260c-d4e9-44ea-8ff3-275f316b76c1", "node_type": "1", "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "20fd0e71bc5cdf8bcb5511ea6161f4dfba1b2c363347124d6cdc91f644cdc15f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf4c6fa-3009-4557-b35f-e788dc2fda5c", "node_type": "1", "metadata": {}, "hash": "251659d95bc58b1a5d156ded71ec893f47c310533413e31fd217d7693ddcebe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and\ntheir ef\ufb01cient implementations \u2013 can be used almost out of the box.\n3.1 V ISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x \u2208RH\u00d7W\u00d7C into a\nsequence of \ufb02attened 2D patches xp \u2208RN\u00d7(P2\u00b7C), where (H,W ) is the resolution of the original\nimage, Cis the number of channels,(P,P ) is the resolution of each image patch, andN = HW/P2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\n\ufb02atten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s[class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder ( z0\nL) serves as the\nimage representation y (Eq. 4).", "mimetype": "text/plain", "start_char_idx": 876, "end_char_idx": 2026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdf4c6fa-3009-4557-b35f-e788dc2fda5c": {"__data__": {"id_": "bdf4c6fa-3009-4557-b35f-e788dc2fda5c", "embedding": null, "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeeb9665-8572-464a-9fff-49286047c93b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "153a23ada16fa0c249b74d6f3929309398b5d78f3797d8ffc8d0ea070c2d8c98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "640874fe-676f-47b9-a525-d40b98e5972e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "795d286d61dadb1681005a22a8e3daf802c3ee064dadbbf7e5f48e4bcce5fb11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s[class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder ( z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and \ufb01ne-tuning, a classi\ufb01cation head is at-\ntached to z0\nL. The classi\ufb01cation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at \ufb01ne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signi\ufb01cant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3", "mimetype": "text/plain", "start_char_idx": 1729, "end_char_idx": 2882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5968fd51-f382-415e-bbb9-7e89100f6d69": {"__data__": {"id_": "5968fd51-f382-415e-bbb9-7e89100f6d69", "embedding": null, "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1cae0d09031f331c24a40e323cafbb2890773ec2ccda007fff6d281985b6ae3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd", "node_type": "1", "metadata": {}, "hash": "5add70b8cbcd3b4df9a0665fade2b7e9f72a1e4bf3499b912567fad05e7ccacf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x1\npE; x2\npE; \u00b7\u00b7\u00b7 ; xN\np E] +Epos, E \u2208R(P2\u00b7C)\u00d7D, Epos \u2208R(N+1)\u00d7D (1)\nz\u2032\n\u2113 = MSA(LN(z\u2113\u22121)) +z\u2113\u22121, \u2113 = 1...L (2)\nz\u2113 = MLP(LN(z\u2032\n\u2113)) +z\u2032\n\u2113, \u2113 = 1...L (3)\ny = LN(z0\nL) (4)\nInductive bias. We note that Vision Transformer has much less image-speci\ufb01c inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat \ufb01ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd": {"__data__": {"id_": "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd", "embedding": null, "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1cae0d09031f331c24a40e323cafbb2890773ec2ccda007fff6d281985b6ae3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5968fd51-f382-415e-bbb9-7e89100f6d69", "node_type": "1", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "dcd2f1ad02c511152f0a5b996c13323ed78ab24757b40803fa60b6ff2340dfb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ec7195e-73f2-4b0f-babb-6b01d8b7f778", "node_type": "1", "metadata": {}, "hash": "f40481d93dbbe3c1bfd4073c103c92748bb064249e83d7f90760b4d783daa4c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat \ufb01ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\n\ufb02attening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classi\ufb01cation input embedding and position embeddings are added as described above.\n3.2 F INE -TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and \ufb01ne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D\u00d7K feedforward\nlayer, where K is the number of downstream classes.", "mimetype": "text/plain", "start_char_idx": 638, "end_char_idx": 1934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ec7195e-73f2-4b0f-babb-6b01d8b7f778": {"__data__": {"id_": "2ec7195e-73f2-4b0f-babb-6b01d8b7f778", "embedding": null, "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1cae0d09031f331c24a40e323cafbb2890773ec2ccda007fff6d281985b6ae3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd", "node_type": "1", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "bf41ba3be0364308b94b85cf6ea588865b1a518a15fac296d64a4f8079f7f31f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1", "node_type": "1", "metadata": {}, "hash": "f2c97067882dcf86c86b5af3d3a88fd9c5de23dd1080d0c54f78b54c1155d0aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The classi\ufb01cation input embedding and position embeddings are added as described above.\n3.2 F INE -TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and \ufb01ne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D\u00d7K feedforward\nlayer, where K is the number of downstream classes. It is often bene\ufb01cial to \ufb01ne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4 E XPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks.", "mimetype": "text/plain", "start_char_idx": 1566, "end_char_idx": 2916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1": {"__data__": {"id_": "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1", "embedding": null, "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1cae0d09031f331c24a40e323cafbb2890773ec2ccda007fff6d281985b6ae3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ec7195e-73f2-4b0f-babb-6b01d8b7f778", "node_type": "1", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "a5d21bddd5455b2e253f11b260c97ad9cd57849b8eeabddfb0782cc35fa6f85d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec59e1cd-2b6e-4d97-986c-660e3fdd1b21", "node_type": "1", "metadata": {}, "hash": "9ce6011b28632ee4277c2b13865ed67e57de3916bb40d48502e75e23dd4af4fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4 E XPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future.\n4.1 S ETUP\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020).", "mimetype": "text/plain", "start_char_idx": 2477, "end_char_idx": 3675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec59e1cd-2b6e-4d97-986c-660e3fdd1b21": {"__data__": {"id_": "ec59e1cd-2b6e-4d97-986c-660e3fdd1b21", "embedding": null, "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1cae0d09031f331c24a40e323cafbb2890773ec2ccda007fff6d281985b6ae3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1", "node_type": "1", "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "edff1ea60978b2b22329bba3b0f23d3ed9d9c017e5aabb3b72cfb225c7487ffd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020).\n4", "mimetype": "text/plain", "start_char_idx": 3553, "end_char_idx": 4044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f20c27c-c94e-4d83-8c72-fc616794e9cf": {"__data__": {"id_": "7f20c27c-c94e-4d83-8c72-fc616794e9cf", "embedding": null, "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "91b6c7ed0f3cd558732325a449cd2d1148b7bd8888b36015606f527b270bdd15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39608719-deac-44a6-b356-183e421e9b0b", "node_type": "1", "metadata": {}, "hash": "f536eefeccadea566ec69d44010d0f3f7e6c084cf009a6e077b55548877912dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nModel Layers Hidden size D MLP size Heads Params\nViT-Base 12 768 3072 12 86M\nViT-Large 24 1024 4096 16 307M\nViT-Huge 32 1280 5120 16 632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classi\ufb01cation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural \u2013 tasks like the above, Pets, CIFAR, etc. Specialized \u2013 medical and satellite\nimagery, and Structured \u2013 tasks that require geometric understanding like localization.\nModel Variants. We base ViT con\ufb01gurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we\nadd the larger \u201cHuge\u201d model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with16\u00d716 input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39608719-deac-44a6-b356-183e421e9b0b": {"__data__": {"id_": "39608719-deac-44a6-b356-183e421e9b0b", "embedding": null, "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "91b6c7ed0f3cd558732325a449cd2d1148b7bd8888b36015606f527b270bdd15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f20c27c-c94e-4d83-8c72-fc616794e9cf", "node_type": "1", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "a47117c76337c3c22e026bd893149049538c4873ed7bc135151b4d8b6da05cd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa35cd7e-af44-4ddd-94ea-99718de1030a", "node_type": "1", "metadata": {}, "hash": "db1707477150d075bcea0c626b015e0115439095f6c24949879e0ac6dd504a45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we\nadd the larger \u201cHuge\u201d model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with16\u00d716 input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive.\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modi\ufb01cations improve transfer (Kolesnikov et al., 2020),\nand we denote the modi\ufb01ed model \u201cResNet (BiT)\u201d. For the hybrids, we feed the intermediate fea-\nture maps into ViT with patch size of one \u201cpixel\u201d. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\nTraining & Fine-tuning.", "mimetype": "text/plain", "start_char_idx": 737, "end_char_idx": 1977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa35cd7e-af44-4ddd-94ea-99718de1030a": {"__data__": {"id_": "aa35cd7e-af44-4ddd-94ea-99718de1030a", "embedding": null, "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "91b6c7ed0f3cd558732325a449cd2d1148b7bd8888b36015606f527b270bdd15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39608719-deac-44a6-b356-183e421e9b0b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1e5d15ece21fc5de149495095f9463dfc10cecb799df80c6dd1158c4ae4ee0a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17637a6d-05f0-4e0d-ab34-71d637a33a80", "node_type": "1", "metadata": {}, "hash": "963336d1cfb29c606bac10b2e5962921c65f539011df97aab1feaf051b14e788", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with \u03b21 = 0.9, \u03b22 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For \ufb01ne-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we \ufb01ne-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\nMetrics.", "mimetype": "text/plain", "start_char_idx": 1609, "end_char_idx": 2722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17637a6d-05f0-4e0d-ab34-71d637a33a80": {"__data__": {"id_": "17637a6d-05f0-4e0d-ab34-71d637a33a80", "embedding": null, "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "91b6c7ed0f3cd558732325a449cd2d1148b7bd8888b36015606f527b270bdd15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa35cd7e-af44-4ddd-94ea-99718de1030a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "acc115e8a262e759b834d00aa4e9b370617335d941028e5c2edfefc3b4b948f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8eba80f-75db-4fc6-877e-3683541d81c9", "node_type": "1", "metadata": {}, "hash": "af7a1b69635db4958b11bbc48c9f07c9aba09da518991489680db8e17b5672b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For ImageNet results in Table 2, we \ufb01ne-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\nMetrics. We report results on downstream datasets either through few-shot or \ufb01ne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after \ufb01ne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to{\u22121,1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\n\ufb01ne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-\ufb02y evaluation\nwhere \ufb01ne-tuning would be too costly.\n4.2 C OMPARISON TO STATE OF THE ART\nWe \ufb01rst compare our largest models \u2013 ViT-H/14 and ViT-L/16 \u2013 to state-of-the-art CNNs from\nthe literature. The \ufb01rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets.", "mimetype": "text/plain", "start_char_idx": 2485, "end_char_idx": 3622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8eba80f-75db-4fc6-877e-3683541d81c9": {"__data__": {"id_": "b8eba80f-75db-4fc6-877e-3683541d81c9", "embedding": null, "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "91b6c7ed0f3cd558732325a449cd2d1148b7bd8888b36015606f527b270bdd15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17637a6d-05f0-4e0d-ab34-71d637a33a80", "node_type": "1", "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "85a677b4c3c5b704688e46a2be625d89e4fb41ad11ba6abb1a1ae51a0193c05b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2 C OMPARISON TO STATE OF THE ART\nWe \ufb01rst compare our largest models \u2013 ViT-H/14 and ViT-L/16 \u2013 to state-of-the-art CNNs from\nthe literature. The \ufb01rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large Ef\ufb01cientNet trained using semi-supervised learning on ImageNet and JFT-\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days.\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets \u2013 ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\n5", "mimetype": "text/plain", "start_char_idx": 3341, "end_char_idx": 4520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdfa5eb3-e20b-4f60-bc25-a1a696e33700": {"__data__": {"id_": "bdfa5eb3-e20b-4f60-bc25-a1a696e33700", "embedding": null, "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3fa427eea45c7fb11d907602eda75d76e1ebb09c471fde5154aa22da4c77e494", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f", "node_type": "1", "metadata": {}, "hash": "417d02fb8e8f71ab6a72c6680849afaab0854d6cb421be1abccdc02851a8fb3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nOurs-JFT Ours-JFT Ours-I21k BiT-L Noisy Student\n(ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (Ef\ufb01cientNet-L2)\nImageNet 88.55\u00b10.04 87.76\u00b10.03 85.30\u00b10.02 87.54\u00b10.02 88.4/88.5\u2217\nImageNet ReaL 90.72\u00b10.05 90.54\u00b10.03 88.62\u00b10.05 90.54 90 .55\nCIFAR-10 99.50\u00b10.06 99.42\u00b10.03 99.15\u00b10.03 99.37\u00b10.06 \u2212\nCIFAR-100 94.55\u00b10.04 93.90\u00b10.05 93.25\u00b10.05 93.51\u00b10.08 \u2212\nOxford-IIIT Pets 97.56\u00b10.03 97.32\u00b10.11 94.67\u00b10.15 96.62\u00b10.23 \u2212\nOxford Flowers-102 99.68\u00b10.02 99.74\u00b10.00 99.61\u00b10.02 99.63\u00b10.03 \u2212\nVTAB (19 tasks) 77.63\u00b10.23 76.28\u00b10.46 72.72\u00b10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f": {"__data__": {"id_": "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f", "embedding": null, "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3fa427eea45c7fb11d907602eda75d76e1ebb09c471fde5154aa22da4c77e494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdfa5eb3-e20b-4f60-bc25-a1a696e33700", "node_type": "1", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "d2ef704ee55a01c9ddebe9b2b0348e36c16d84b603ee35a4d7b7a0d49b6f3f65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acf28545-6e3d-481f-9a02-b526848e8e86", "node_type": "1", "metadata": {}, "hash": "42d00660a798572ddfedf346bbd7bff58f6f9382c4282e9657a7b11afa5ed7bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "56\u00b10.03 97.32\u00b10.11 94.67\u00b10.15 96.62\u00b10.23 \u2212\nOxford Flowers-102 99.68\u00b10.02 99.74\u00b10.00 99.61\u00b10.02 99.63\u00b10.03 \u2212\nVTAB (19 tasks) 77.63\u00b10.23 76.28\u00b10.46 72.72\u00b10.21 76.29\u00b11.70 \u2212\nTPUv3-core-days 2.5k 0.68k 0.23k 9.9k 12.3k\nTable 2: Comparison with state of the art on popular image classi\ufb01cation benchmarks. We re-\nport mean and standard deviation of the accuracies, averaged over three \ufb01ne-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. \u2217Slightly improved 88.5% result reported\nin Touvron et al. (2020).", "mimetype": "text/plain", "start_char_idx": 412, "end_char_idx": 1134, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acf28545-6e3d-481f-9a02-b526848e8e86": {"__data__": {"id_": "acf28545-6e3d-481f-9a02-b526848e8e86", "embedding": null, "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3fa427eea45c7fb11d907602eda75d76e1ebb09c471fde5154aa22da4c77e494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "e4a00f855d5b96ea94d492add2c5c5bece36c8d22027f480d9b7b537531f793e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62a82209-0918-4048-b6a1-e01a25bf2b9e", "node_type": "1", "metadata": {}, "hash": "ccdcba2611055691002b9fde26db486edaca19e4a084dbfa8622af247e944d73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. \u2217Slightly improved 88.5% result reported\nin Touvron et al. (2020).\nVTAB (19 tasks)\n65\n70\n75\n80Accuracy [%]\nNatural (7 tasks)\n70\n80\n90\nSpecialized (4 tasks)\n80\n82\n85\n88\n90\nStructured (8 tasks)\n50\n60\n70ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training ef\ufb01ciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4.", "mimetype": "text/plain", "start_char_idx": 807, "end_char_idx": 1788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62a82209-0918-4048-b6a1-e01a25bf2b9e": {"__data__": {"id_": "62a82209-0918-4048-b6a1-e01a25bf2b9e", "embedding": null, "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3fa427eea45c7fb11d907602eda75d76e1ebb09c471fde5154aa22da4c77e494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acf28545-6e3d-481f-9a02-b526848e8e86", "node_type": "1", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "0fc470ae0995977a09fdd03ac4738df7c0194838f65dda25baa82ed7eedfd09b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1123bc84-d94e-4869-9b2b-f784db425726", "node_type": "1", "metadata": {}, "hash": "0f24347e2989f8ba9addf008461d04845136bac234c39c0161c12775e71cb300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training ef\ufb01ciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\nproximately 30 days.\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI \u2013 a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L \u2013 supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on theNatural and Structured tasks. On the\nSpecialized the performance of the top two models is similar.\n4.3 P RE-TRAINING DATA REQUIREMENTS\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.", "mimetype": "text/plain", "start_char_idx": 1418, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1123bc84-d94e-4869-9b2b-f784db425726": {"__data__": {"id_": "1123bc84-d94e-4869-9b2b-f784db425726", "embedding": null, "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3fa427eea45c7fb11d907602eda75d76e1ebb09c471fde5154aa22da4c77e494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62a82209-0918-4048-b6a1-e01a25bf2b9e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9c7d68e22a437a16460ce9463ecb025094ab593c43280dfcd0d23eba757c637d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the\nSpecialized the performance of the top two models is similar.\n4.3 P RE-TRAINING DATA REQUIREMENTS\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters \u2013 weight decay, dropout, and label smoothing. Figure 3 shows the results after \ufb01ne-\ntuning to ImageNet (results on other datasets are shown in Table 5) 2. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full bene\ufb01t of larger models. Figure 3 also shows the performance\n2Note that the ImageNet pre-trained models are also \ufb01ne-tuned, but again on ImageNet. This is because the\nresolution increase during \ufb01ne-tuning improves the performance.\n6", "mimetype": "text/plain", "start_char_idx": 2410, "end_char_idx": 3560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d99ce8e-a37c-472d-bf41-5d546e0f16ee": {"__data__": {"id_": "1d99ce8e-a37c-472d-bf41-5d546e0f16ee", "embedding": null, "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47", "node_type": "4", "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "ee52625ce207933feaa195645b2bb1dea196aafade44233120b7cccddb1cbd8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b23cd4da-dbaa-48a0-9916-63163abe75df", "node_type": "1", "metadata": {}, "hash": "f276cfff0341b9daf5673143cc76b00ebd31adc8d843caf040a214aa2752fd88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nImageNet ImageNet-21k JFT-300M\nPre-training dataset\n70\n75\n80\n85\n90ImageNet Top1 Accuracy [%]\nBiT\nViT-B/32\nViT-B/16\nViT-L/32\nViT-L/16\nViT-H/14\nFigure 3: Transfer to ImageNet. While\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows.\n10 M 30 M 100 M 300 M\nNumber of JFT pre-training samples\n30\n40\n50\n60\n70Linear 5-shot ImageNet Top1 [%]\nViT-L/16\nViT-L/32\nViT-B/32\nViT-b/32\nResNet50x1 (BiT)\nResNet152x2 (BiT)\nFigure 4: Linear few-shot evaluation on Ima-\ngeNet versus pre-training size. ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b23cd4da-dbaa-48a0-9916-63163abe75df": {"__data__": {"id_": "b23cd4da-dbaa-48a0-9916-63163abe75df", "embedding": null, "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47", "node_type": "4", "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "ee52625ce207933feaa195645b2bb1dea196aafade44233120b7cccddb1cbd8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d99ce8e-a37c-472d-bf41-5d546e0f16ee", "node_type": "1", "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "fb6a67086c366393355a9c930f72db5a1a629f8ffcb42151741a008cccb83693", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee7bda4b-062b-4789-80c6-ec392050980d", "node_type": "1", "metadata": {}, "hash": "95b1ac6a07fa3b87c6fbbab0994f62415d711a3d5e0f3025191bf3273dd89142", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved.\n102 103\n90\n95Transfer accuracy [%]\nAverage-5\nTransformer (ViT)\nResNet (BiT)\nHybrid\n102 10375\n80\n85\n90 ImageNet\nTransformer (ViT)\nResNet (BiT)\nHybrid\nTotal pre-training compute [exaFLOPs]\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models.\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes.\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full \ufb01ne-\ntuning accuracy.", "mimetype": "text/plain", "start_char_idx": 697, "end_char_idx": 2024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee7bda4b-062b-4789-80c6-ec392050980d": {"__data__": {"id_": "ee7bda4b-062b-4789-80c6-ec392050980d", "embedding": null, "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47", "node_type": "4", "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "ee52625ce207933feaa195645b2bb1dea196aafade44233120b7cccddb1cbd8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b23cd4da-dbaa-48a0-9916-63163abe75df", "node_type": "1", "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "52f19a861fb0f08ff490f4aa484bae9b28b7500bbd049aea2fb39697c0e593d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full \ufb01ne-\ntuning accuracy. Figure 4 contains the results. Vision Transformers over\ufb01t more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is suf\ufb01cient, even bene\ufb01cial.\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work.\n7", "mimetype": "text/plain", "start_char_idx": 1624, "end_char_idx": 2773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4c79ce4-b09c-4e94-899f-95d7e148a2e1": {"__data__": {"id_": "f4c79ce4-b09c-4e94-899f-95d7e148a2e1", "embedding": null, "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8d01ed6-342b-427b-a352-ec7ff615d512", "node_type": "4", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "59d402b3fcd25bca28978ef08d438355fbb279963dcac10ed976006dae9b89b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42c3a271-0203-4a60-8301-c4d87ff68d46", "node_type": "1", "metadata": {}, "hash": "a1125ba927b1987ae2b940c6577f8d137b79f2fba32df873169ad23dc5c900fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\n4.4 S CALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models\u2019 performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42c3a271-0203-4a60-8301-c4d87ff68d46": {"__data__": {"id_": "42c3a271-0203-4a60-8301-c4d87ff68d46", "embedding": null, "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8d01ed6-342b-427b-a352-ec7ff615d512", "node_type": "4", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "59d402b3fcd25bca28978ef08d438355fbb279963dcac10ed976006dae9b89b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4c79ce4-b09c-4e94-899f-95d7e148a2e1", "node_type": "1", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "22e7c630aac754e58aa28590694a4ccd5798448e8216eeae9ed92d0266c4e728", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed", "node_type": "1", "metadata": {}, "hash": "eeae0942d6439d549c94fea799c2f4046483dbe8887c79492cb133038395db3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 \u22124\u00d7less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5 I NSPECTING VISION TRANSFORMER\nInput\n Attention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The \ufb01rst layer of\nthe Vision Transformer linearly projects the \ufb02attened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding \ufb01lters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the \ufb01ne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations.", "mimetype": "text/plain", "start_char_idx": 844, "end_char_idx": 2330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed": {"__data__": {"id_": "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed", "embedding": null, "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8d01ed6-342b-427b-a352-ec7ff615d512", "node_type": "4", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "59d402b3fcd25bca28978ef08d438355fbb279963dcac10ed976006dae9b89b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c3a271-0203-4a60-8301-c4d87ff68d46", "node_type": "1", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "5156b8e0268eceec4106559a6bcd0b3ab577ea3f4c3dc6ad94b7905ac229b048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b6a9438-8faf-4fac-aa12-af91063209fa", "node_type": "1", "metadata": {}, "hash": "741b7d87b4f8b480ab3215f866a3b79847ba7ae61010ddd2bba8df260ddfb122", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \ufb01rst layer of\nthe Vision Transformer linearly projects the \ufb02attened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding \ufb01lters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the \ufb01ne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speci\ufb01cally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n\u201cattention distance\u201d is analogous to receptive \ufb01eld size in CNNs.", "mimetype": "text/plain", "start_char_idx": 1902, "end_char_idx": 3268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b6a9438-8faf-4fac-aa12-af91063209fa": {"__data__": {"id_": "9b6a9438-8faf-4fac-aa12-af91063209fa", "embedding": null, "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8d01ed6-342b-427b-a352-ec7ff615d512", "node_type": "4", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "59d402b3fcd25bca28978ef08d438355fbb279963dcac10ed976006dae9b89b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed", "node_type": "1", "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6891ff47459d8ac8a2f4c13269830fc6564d623bc40a8ebd92a89c8a5bf1cfb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Self-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speci\ufb01cally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n\u201cattention distance\u201d is analogous to receptive \ufb01eld size in CNNs.\nWe \ufb01nd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we \ufb01nd that the model attends to image\nregions that are semantically relevant for classi\ufb01cation (Figure 6).\n4.6 S ELF -SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8", "mimetype": "text/plain", "start_char_idx": 2874, "end_char_idx": 4141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7762844-4e14-4c58-9088-788b80895a9b": {"__data__": {"id_": "b7762844-4e14-4c58-9088-788b80895a9b", "embedding": null, "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df", "node_type": "4", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8cdd3c24b78f595ee819d4962def4bb146d5eefe9e5a089df0332639d2f7b531", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68969e8e-6781-420b-a298-b93aa173c0d8", "node_type": "1", "metadata": {}, "hash": "fd20220b53a09300078aea300866aaf8ab4c7bc46407308db859df4cb4d155ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nRGB embedding filters\n(first 28 principal components)\n1 2 3 4 5 6 7\nInput patch column\n1\n2\n3\n4\n5\n6\n7 Input patch row\nPosition embedding similarity\n1\n1\nCosine similarity\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigni\ufb01cant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68969e8e-6781-420b-a298-b93aa173c0d8": {"__data__": {"id_": "68969e8e-6781-420b-a298-b93aa173c0d8", "embedding": null, "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df", "node_type": "4", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8cdd3c24b78f595ee819d4962def4bb146d5eefe9e5a089df0332639d2f7b531", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7762844-4e14-4c58-9088-788b80895a9b", "node_type": "1", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "2abac4047f0e87cc8e45b40230b711e5c302583cd25718c0c6a2f2d0248daef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4950bbbb-4301-46e9-8575-5c98ac072755", "node_type": "1", "metadata": {}, "hash": "53ab8b9b6ade4287c4c6e2c7698c08941a697cf9e9a343bf37e72d25a3a7eaa5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigni\ufb01cant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; H\u00b4enaff et al., 2020) to future work.\n5 C ONCLUSION\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-speci\ufb01c inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classi\ufb01cation\ndatasets, whilst being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al.", "mimetype": "text/plain", "start_char_idx": 852, "end_char_idx": 2244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4950bbbb-4301-46e9-8575-5c98ac072755": {"__data__": {"id_": "4950bbbb-4301-46e9-8575-5c98ac072755", "embedding": null, "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df", "node_type": "4", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8cdd3c24b78f595ee819d4962def4bb146d5eefe9e5a089df0332639d2f7b531", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68969e8e-6781-420b-a298-b93aa173c0d8", "node_type": "1", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "f97727c53b8e6649062db0b100010a22aad6650ee5e731e369e9302b717db286", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d0f2cc9-191e-4aaa-a1b7-9642378d47b0", "node_type": "1", "metadata": {}, "hash": "4ba6527d2ff5651913eddeac55289e041c65ff4d5b0b9fbe6c3375a4c17c47b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classi\ufb01cation\ndatasets, whilst being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\nACKNOWLEDGEMENTS\nThe work was performed in Berlin, Z \u00a8urich, and Amsterdam. We thank many colleagues at Google\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu\u02c7ci\u00b4c, Noam\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention \ufb02ow in transformers. In ACL, 2020.", "mimetype": "text/plain", "start_char_idx": 1782, "end_char_idx": 3188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d0f2cc9-191e-4aaa-a1b7-9642378d47b0": {"__data__": {"id_": "6d0f2cc9-191e-4aaa-a1b7-9642378d47b0", "embedding": null, "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df", "node_type": "4", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8cdd3c24b78f595ee819d4962def4bb146d5eefe9e5a089df0332639d2f7b531", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4950bbbb-4301-46e9-8575-5c98ac072755", "node_type": "1", "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4ee7a8ca06c7a1a64ae11afb9b31e3850b68faf7d34fbb12926dac1395451463", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "REFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention \ufb02ow in transformers. In ACL, 2020.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019.\n9", "mimetype": "text/plain", "start_char_idx": 3088, "end_char_idx": 3335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "226c4cea-b963-4017-be6c-2e7735506f37": {"__data__": {"id_": "226c4cea-b963-4017-be6c-2e7735506f37", "embedding": null, "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da764545-8a00-4201-b4f9-35dbc73b4625", "node_type": "4", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9189f4ed19c308007c1154d49d03217a34ab710694be54664344e0a48e3fa5a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8260ab2-ae39-42ca-a090-6010130e29a3", "node_type": "1", "metadata": {}, "hash": "6872817f134e9dce9deeded4b00e64dd891f89260bb9a39e90d524cd21fcc5e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nIn ICCV, 2019.\nLucas Beyer, Olivier J. H\u00b4enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8aron van den Oord. Are\nwe done with imagenet? arXiv, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8260ab2-ae39-42ca-a090-6010130e29a3": {"__data__": {"id_": "c8260ab2-ae39-42ca-a090-6010130e29a3", "embedding": null, "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da764545-8a00-4201-b4f9-35dbc73b4625", "node_type": "4", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9189f4ed19c308007c1154d49d03217a34ab710694be54664344e0a48e3fa5a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "226c4cea-b963-4017-be6c-2e7735506f37", "node_type": "1", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b5eb5cee3f0049a6c8bd99ef7cc52678dad54fb597f95c8ea12fc08fa7b923e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb93f585-0c62-458c-972c-05a15b191fbb", "node_type": "1", "metadata": {}, "hash": "edba7aa5363b6a5d23acd3ca0a1be4bf461f7323b8a43d48d885501a0a5d8aab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "End-to-end object detection with transformers. In ECCV, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020b.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv, 2019.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR, 2020.\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.", "mimetype": "text/plain", "start_char_idx": 723, "end_char_idx": 1792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb93f585-0c62-458c-972c-05a15b191fbb": {"__data__": {"id_": "bb93f585-0c62-458c-972c-05a15b191fbb", "embedding": null, "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da764545-8a00-4201-b4f9-35dbc73b4625", "node_type": "4", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9189f4ed19c308007c1154d49d03217a34ab710694be54664344e0a48e3fa5a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8260ab2-ae39-42ca-a090-6010130e29a3", "node_type": "1", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "2d63bd59b0b22235370880d97d9d3cf7ebd88099064c3ea8f6b448cc1462f27a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd242c0c-cebc-413d-8413-0aa0bdf0fd4c", "node_type": "1", "metadata": {}, "hash": "900cd52c986683fba2e97f3fc5b8457fbaa5aeb3e746fc69ba1dd984f718a54a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In ICLR, 2020.\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\nlutional neural networks. arXiv, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv, 2019.", "mimetype": "text/plain", "start_char_idx": 1486, "end_char_idx": 2500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd242c0c-cebc-413d-8413-0aa0bdf0fd4c": {"__data__": {"id_": "fd242c0c-cebc-413d-8413-0aa0bdf0fd4c", "embedding": null, "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da764545-8a00-4201-b4f9-35dbc73b4625", "node_type": "4", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9189f4ed19c308007c1154d49d03217a34ab710694be54664344e0a48e3fa5a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb93f585-0c62-458c-972c-05a15b191fbb", "node_type": "1", "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8230afb67254516b427f038c86d8f0072a641d2e63352c19168cbc5f61d6543e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv, 2019.\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn ICCV, 2019.\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\nOlivier J. H\u00b4enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-ef\ufb01cient image recognition with contrastive predictive coding. In\nICML, 2020.\n10", "mimetype": "text/plain", "start_char_idx": 2207, "end_char_idx": 3110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70beb800-2972-4a76-8389-d09dace825d2": {"__data__": {"id_": "70beb800-2972-4a76-8389-d09dace825d2", "embedding": null, "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52c69155-3db6-4504-b556-e7fde92a6d72", "node_type": "4", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "020406394267af4392de8007a14922027694981c4e5d952513418cbc68931bfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a6cfc93-1d0c-4914-862c-09d18094083b", "node_type": "1", "metadata": {}, "hash": "e5eb54f24250b47f2859864d893e2992431239f5b170ff388f521c26c96e10bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In NIPS, 2012.\nY . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a6cfc93-1d0c-4914-862c-09d18094083b": {"__data__": {"id_": "3a6cfc93-1d0c-4914-862c-09d18094083b", "embedding": null, "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52c69155-3db6-4504-b556-e7fde92a6d72", "node_type": "4", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "020406394267af4392de8007a14922027694981c4e5d952513418cbc68931bfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70beb800-2972-4a76-8389-d09dace825d2", "node_type": "1", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "33e23c856868f15e1370db7738406addd4e333a8f305c64aa0ae78e2cf8f295c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9", "node_type": "1", "metadata": {}, "hash": "59d14569dcbbc9a67e560fd70f488cfcf622b2139b43e6e5202c04955b63eb92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv, 2020.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.", "mimetype": "text/plain", "start_char_idx": 779, "end_char_idx": 1780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9": {"__data__": {"id_": "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9", "embedding": null, "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52c69155-3db6-4504-b556-e7fde92a6d72", "node_type": "4", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "020406394267af4392de8007a14922027694981c4e5d952513418cbc68931bfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a6cfc93-1d0c-4914-862c-09d18094083b", "node_type": "1", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "420e9cacaf5d68e96f716987e56e9f7faed0c8875390ff40c03da086145b98c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b2725fb-0a74-412c-9959-823022bea0e0", "node_type": "1", "metadata": {}, "hash": "1d7ac8977152bf53849edfb45cc3107f1cbafcc8fc1732ec582897413eecb970", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. In ECCV, 2018.\nM. Nilsback and A. Zisserman. Automated \ufb02ower classi\ufb01cation over a large number of classes. In\nICVGIP, 2008.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR,\n2012.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992. doi: 10.1137/0330046. URL\nhttps://doi.org/10.1137/0330046.\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille.", "mimetype": "text/plain", "start_char_idx": 1529, "end_char_idx": 2473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b2725fb-0a74-412c-9959-823022bea0e0": {"__data__": {"id_": "5b2725fb-0a74-412c-9959-823022bea0e0", "embedding": null, "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52c69155-3db6-4504-b556-e7fde92a6d72", "node_type": "4", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "020406394267af4392de8007a14922027694981c4e5d952513418cbc68931bfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9", "node_type": "1", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "228636087a0b830cc981c1ac1d47f35df4cda1b16f9c3284680efbb544626708", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "821224bf-2c56-44e7-b0c7-042a2d75eb03", "node_type": "1", "metadata": {}, "hash": "2912768eb7f15b4c5a34ca845f86a9e5d23374f2c4e7480ba89e76b63615c453", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992. doi: 10.1137/0330046. URL\nhttps://doi.org/10.1137/0330046.\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, 2017.", "mimetype": "text/plain", "start_char_idx": 2200, "end_char_idx": 3159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "821224bf-2c56-44e7-b0c7-042a2d75eb03": {"__data__": {"id_": "821224bf-2c56-44e7-b0c7-042a2d75eb03", "embedding": null, "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52c69155-3db6-4504-b556-e7fde92a6d72", "node_type": "4", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "020406394267af4392de8007a14922027694981c4e5d952513418cbc68931bfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b2725fb-0a74-412c-9959-823022bea0e0", "node_type": "1", "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "a98466043c8becffcf8c7c2ce6b89605eb9dbfefd6c7a6982e423ee7f633f262", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, 2017.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV, 2019.\n11", "mimetype": "text/plain", "start_char_idx": 2947, "end_char_idx": 3325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4b79a84-0951-4e38-bcc6-18f662ab3a1f": {"__data__": {"id_": "a4b79a84-0951-4e38-bcc6-18f662ab3a1f", "embedding": null, "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2", "node_type": "4", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "7235ceb3c1f17b9879291a60de62b508f8f8fe148ef05f46fde16c40b2f1d24b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df957665-53aa-4210-ba27-eb61f5709a8e", "node_type": "1", "metadata": {}, "hash": "3dab3a9e608c36d46e60c26422a417c8fa405749b84a894af80d2f54142dbec7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS. 2019.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixef\ufb01cientnet. arXiv preprint arXiv:2003.08237, 2020.\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df957665-53aa-4210-ba27-eb61f5709a8e": {"__data__": {"id_": "df957665-53aa-4210-ba27-eb61f5709a8e", "embedding": null, "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2", "node_type": "4", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "7235ceb3c1f17b9879291a60de62b508f8f8fe148ef05f46fde16c40b2f1d24b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4b79a84-0951-4e38-bcc6-18f662ab3a1f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "a3c0e57f86032bc89a3b16bfcfbd3d300815b5a5ce427d02f3a6189c2c633377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9", "node_type": "1", "metadata": {}, "hash": "010ddd5b1601c83d71fdc4bfb206ea45a04f4194a9263698d15bebf7a69854ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Attention is all you need. In NIPS, 2017.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint\narXiv:2003.07853, 2020b.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\nDirk Weissenborn, Oscar T\u00a8ackstr\u00a8om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR, 2019.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020.", "mimetype": "text/plain", "start_char_idx": 745, "end_char_idx": 1752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9": {"__data__": {"id_": "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9", "embedding": null, "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2", "node_type": "4", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "7235ceb3c1f17b9879291a60de62b508f8f8fe148ef05f46fde16c40b2f1d24b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df957665-53aa-4210-ba27-eb61f5709a8e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "66d619fa976620493f13903667e7f68bb9299af5712d49041223764b533e923a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "526f086a-62ef-46a9-8769-d4815ee67a1c", "node_type": "1", "metadata": {}, "hash": "831586d392df04e91b73d3fcfee60c2d00bfcf35a78b91975f6578f1c3de01e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Scaling autoregressive video models. In\nICLR, 2019.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V . Le. Self-training with noisy student\nimproves imagenet classi\ufb01cation. In CVPR, 2020.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\nSupervised Learning. In ICCV, 2019a.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.", "mimetype": "text/plain", "start_char_idx": 1482, "end_char_idx": 2488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "526f086a-62ef-46a9-8769-d4815ee67a1c": {"__data__": {"id_": "526f086a-62ef-46a9-8769-d4815ee67a1c", "embedding": null, "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2", "node_type": "4", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "7235ceb3c1f17b9879291a60de62b508f8f8fe148ef05f46fde16c40b2f1d24b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9", "node_type": "1", "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "3ed1776b5215751d457719d9c8675f03fb18d8af56f3e1fd6f918e53660e7ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR, 2020.\n12", "mimetype": "text/plain", "start_char_idx": 2263, "end_char_idx": 2506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ff55443-bee6-42ab-aff4-584f69287f0b": {"__data__": {"id_": "1ff55443-bee6-42ab-aff4-584f69287f0b", "embedding": null, "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "979a756d-2dac-425d-a204-36f18bae9b77", "node_type": "4", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b492473d9f7469d5977baae39732a8da464c9299ca85b8c2859e9b2e1a33a173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2105de9b-9a8e-4186-8d5a-932beb3357fe", "node_type": "1", "metadata": {}, "hash": "9080af9839e23d5da6d9f979b6eededef43b6d5c52f80679527d1dc1c047b45f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nModels Dataset Epochs Base LR LR decay Weight decay Dropout\nViT-B/{16,32} JFT-300M 7 8 \u00b710\u22124 linear 0.1 0.0\nViT-L/32 JFT-300M 7 6 \u00b710\u22124 linear 0.1 0.0\nViT-L/16 JFT-300M 7/14 4 \u00b710\u22124 linear 0.1 0.0\nViT-H/14 JFT-300M 14 3 \u00b710\u22124 linear 0.1 0.0\nR50x{1,2} JFT-300M 7 10\u22123 linear 0.1 0.0\nR101x1 JFT-300M 7 8 \u00b710\u22124 linear 0.1 0.0\nR152x{1,2} JFT-300M 7 6 \u00b710\u22124 linear 0.1 0.0\nR50+ViT-B/{16,32} JFT-300M 7 8 \u00b710\u22124 linear 0.1 0.0\nR50+ViT-L/32 JFT-300M 7 2 \u00b710\u22124 linear 0.1 0.0\nR50+ViT-L/16 JFT-300M 7/14 4 \u00b710\u22124 linear 0.1 0.0\nViT-B/{16,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2105de9b-9a8e-4186-8d5a-932beb3357fe": {"__data__": {"id_": "2105de9b-9a8e-4186-8d5a-932beb3357fe", "embedding": null, "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "979a756d-2dac-425d-a204-36f18bae9b77", "node_type": "4", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b492473d9f7469d5977baae39732a8da464c9299ca85b8c2859e9b2e1a33a173", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ff55443-bee6-42ab-aff4-584f69287f0b", "node_type": "1", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "5f30eddee76a92b044d215c4d84ee0f3789c993781c3e8beca0b82139996428c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79374460-d5be-45d3-a4be-49c1d7f87947", "node_type": "1", "metadata": {}, "hash": "ceb6f5fbb53444d41c6102a2e647d694d453886a920af33cfc2e42a757220c35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32} JFT-300M 7 8 \u00b710\u22124 linear 0.1 0.0\nR50+ViT-L/32 JFT-300M 7 2 \u00b710\u22124 linear 0.1 0.0\nR50+ViT-L/16 JFT-300M 7/14 4 \u00b710\u22124 linear 0.1 0.0\nViT-B/{16,32} ImageNet-21k 90 10\u22123 linear 0.03 0.1\nViT-L/{16,32} ImageNet-21k 30/90 10\u22123 linear 0.03 0.1\nViT-\u2217 ImageNet 300 3 \u00b710\u22123 cosine 0.3 0.1\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\ning rate warmup of 10k steps. For ImageNet we found it bene\ufb01cial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224.\nAPPENDIX\nA M ULTIHEAD SELF -ATTENTION\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z \u2208RN\u00d7D, we compute a weighted sum over all\nvalues v in the sequence.", "mimetype": "text/plain", "start_char_idx": 427, "end_char_idx": 1216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79374460-d5be-45d3-a4be-49c1d7f87947": {"__data__": {"id_": "79374460-d5be-45d3-a4be-49c1d7f87947", "embedding": null, "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "979a756d-2dac-425d-a204-36f18bae9b77", "node_type": "4", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b492473d9f7469d5977baae39732a8da464c9299ca85b8c2859e9b2e1a33a173", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2105de9b-9a8e-4186-8d5a-932beb3357fe", "node_type": "1", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "d02a39527cd5b541a044f0d4133510bea076aad5599648718e17e363b7ca887d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b48293f4-10e3-4010-8f15-423abe7d7c52", "node_type": "1", "metadata": {}, "hash": "7caf6cf1cc27adb6a3328d5d2ee7a0790f9e26e0c6a535548bce0143811045de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training resolution is 224.\nAPPENDIX\nA M ULTIHEAD SELF -ATTENTION\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z \u2208RN\u00d7D, we compute a weighted sum over all\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\ntwo elements of the sequence and their respective query qi and key kj representations.\n[q,k,v] =zUqkv Uqkv \u2208RD\u00d73Dh, (5)\nA= softmax\n(\nqk\u22a4/\n\u221a\nDh\n)\nA\u2208RN\u00d7N , (6)\nSA(z) =Av . (7)\nMultihead self-attention (MSA) is an extension of SA in which we run kself-attention operations,\ncalled \u201cheads\u201d, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\nMSA(z) = [SA1(z); SA2(z); \u00b7\u00b7\u00b7 ; SAk(z)] Umsa Umsa \u2208Rk\u00b7Dh\u00d7D (8)\nB E XPERIMENT DETAILS\nB.1 T RAINING\nTable 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet.", "mimetype": "text/plain", "start_char_idx": 931, "end_char_idx": 1982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b48293f4-10e3-4010-8f15-423abe7d7c52": {"__data__": {"id_": "b48293f4-10e3-4010-8f15-423abe7d7c52", "embedding": null, "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "979a756d-2dac-425d-a204-36f18bae9b77", "node_type": "4", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b492473d9f7469d5977baae39732a8da464c9299ca85b8c2859e9b2e1a33a173", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79374460-d5be-45d3-a4be-49c1d7f87947", "node_type": "1", "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4d3549adb3362095f99a20961dad2675717a4468fd653c1c6170b28eef20aac1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5) is typically set to D/k.\nMSA(z) = [SA1(z); SA2(z); \u00b7\u00b7\u00b7 ; SAk(z)] Umsa Umsa \u2208Rk\u00b7Dh\u00d7D (8)\nB E XPERIMENT DETAILS\nB.1 T RAINING\nTable 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224.\nB.1.1 F INE -TUNING\nWe \ufb01ne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For \ufb01nal results we train on the entire training set and evaluate on the respective\ntest data. For \ufb01ne-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\n13", "mimetype": "text/plain", "start_char_idx": 1703, "end_char_idx": 2860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a28eceeb-84f8-4caa-baeb-e0fbd0e31426": {"__data__": {"id_": "a28eceeb-84f8-4caa-baeb-e0fbd0e31426", "embedding": null, "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "35591ebeed864d7b9de5e01eff7aeca24fe5482a488038a048a5db640d4ea689", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d", "node_type": "1", "metadata": {}, "hash": "5d318ad09fd97c9766bf284ecfafe4188637b080b117b712ba7df4d39d018627", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nDataset Steps Base LR\nImageNet 20 000 {0.003, 0.01, 0.03, 0.06}\nCIFAR100 10 000 {0.001, 0.003, 0.01, 0.03}\nCIFAR10 10 000 {0.001, 0.003, 0.01, 0.03}\nOxford-IIIT Pets 500 {0.001, 0.003, 0.01, 0.03}\nOxford Flowers-102 500 {0.001, 0.003, 0.01, 0.03}\nVTAB (19 tasks) 2 500 0.01\nTable 4: Hyperparameters for \ufb01ne-tuning. All models are \ufb01ne-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\n\ufb01ne-tuning resolution is 384.\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all \ufb01ne-tuning experiments run at 384\nresolution (running \ufb01ne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020)).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d": {"__data__": {"id_": "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d", "embedding": null, "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "35591ebeed864d7b9de5e01eff7aeca24fe5482a488038a048a5db640d4ea689", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a28eceeb-84f8-4caa-baeb-e0fbd0e31426", "node_type": "1", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6bcbc0525a6445fdc850ec9053fda2b501505df7d5efd4adbff38a1d599082ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce655fd1-14cd-4e11-95d3-dbd468f75e6f", "node_type": "1", "metadata": {}, "hash": "c7db1bbb49a252a1045af0d1705f17bd40946a475f988913d9139cfe0401b686", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If not mentioned otherwise,\n\ufb01ne-tuning resolution is 384.\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all \ufb01ne-tuning experiments run at 384\nresolution (running \ufb01ne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020)).\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speci\ufb01c input resolutions. Instead we \ufb01nd\nthat Vision Transformer bene\ufb01ts most from a high resolution (384 \u00d7384) for all tasks.", "mimetype": "text/plain", "start_char_idx": 492, "end_char_idx": 1752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce655fd1-14cd-4e11-95d3-dbd468f75e6f": {"__data__": {"id_": "ce655fd1-14cd-4e11-95d3-dbd468f75e6f", "embedding": null, "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "35591ebeed864d7b9de5e01eff7aeca24fe5482a488038a048a5db640d4ea689", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d", "node_type": "1", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "c95d5dcff6ff2ad2ab24715b5d862e55d1ff3d34a2198a95a58226e2b0f78fe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c2e9772-20ef-4633-a491-d408e31432a7", "node_type": "1", "metadata": {}, "hash": "e8a5eb22f93a55a613c2f648c61b3d8cd939c90817be6e6cf33a9362910aff25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speci\ufb01c input resolutions. Instead we \ufb01nd\nthat Vision Transformer bene\ufb01ts most from a high resolution (384 \u00d7384) for all tasks.\nB.1.2 S ELF -SUPERVISION\nWe employ the masked patch predictionobjective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations.\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of2\u00b710\u22124, warmup of 10k steps and cosine learning rate decay.", "mimetype": "text/plain", "start_char_idx": 1531, "end_char_idx": 2481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c2e9772-20ef-4633-a491-d408e31432a7": {"__data__": {"id_": "8c2e9772-20ef-4633-a491-d408e31432a7", "embedding": null, "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "35591ebeed864d7b9de5e01eff7aeca24fe5482a488038a048a5db640d4ea689", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce655fd1-14cd-4e11-95d3-dbd468f75e6f", "node_type": "1", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "cf98eed6041f6a217f7f0fb548b5f6729612fa2e68edd51a6673954176b8c276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fb9b60d-0823-4ba1-93a1-37eb87c5b219", "node_type": "1", "metadata": {}, "hash": "50a1c0c7b80a6626ac278d42f8827ac8b263b5ad78862b33fef58c4b40336152", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations.\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of2\u00b710\u22124, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 \u00d74 downsized version of the 16 \u00d716\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report \ufb01nal results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics.\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\u2019t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classi\ufb01cation.", "mimetype": "text/plain", "start_char_idx": 2137, "end_char_idx": 3397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fb9b60d-0823-4ba1-93a1-37eb87c5b219": {"__data__": {"id_": "2fb9b60d-0823-4ba1-93a1-37eb87c5b219", "embedding": null, "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "35591ebeed864d7b9de5e01eff7aeca24fe5482a488038a048a5db640d4ea689", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c2e9772-20ef-4633-a491-d408e31432a7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1faf0823b713e58af9acbd49f6e0edf0933e8e82ec44be1bd2cdb2948539f9ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics.\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\u2019t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classi\ufb01cation. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet.\nC A DDITIONAL RESULTS\nWe report detailed results corresponding to the \ufb01gures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\n14", "mimetype": "text/plain", "start_char_idx": 3014, "end_char_idx": 3857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1718dcb1-58f8-47c9-8738-895516d5a876": {"__data__": {"id_": "1718dcb1-58f8-47c9-8738-895516d5a876", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91351451-a9c2-42fc-bfb3-3113e0a95206", "node_type": "1", "metadata": {}, "hash": "ed22618105055caa14b83d27d09568038e63137b1432fd9474cf9cb35eeebfd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nViT-B/16 ViT-B/32 ViT-L/16 ViT-L/32 ViT-H/14\nImageNet CIFAR-10 98.13 97.77 97.86 97.94 -\nCIFAR-100 87.13 86.31 86.35 87.07 -\nImageNet 77.91 73.38 76.53 71.16 -\nImageNet ReaL 83.57 79.56 82.19 77.83 -\nOxford Flowers-102 89.49 85.43 89.66 86.36 -\nOxford-IIIT-Pets 93.81 92.04 93.64 91.35 -\nImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27\nCIFAR-100 91.67 91.97 93.44 93.04 93.82\nImageNet 83.97 81.28 85.15 80.99 85.13\nImageNet ReaL 88.35 86.63 88.40 85.65 88.70\nOxford Flowers-102 99.38 99.11 99.61 99.19 99.51\nOxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82\nJFT-300M CIFAR-10 99.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91351451-a9c2-42fc-bfb3-3113e0a95206": {"__data__": {"id_": "91351451-a9c2-42fc-bfb3-3113e0a95206", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1718dcb1-58f8-47c9-8738-895516d5a876", "node_type": "1", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "ccdb7d0503ea22ac94a7b27ff565759e6ed096d5f71fa6c289fc04467309db6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55", "node_type": "1", "metadata": {}, "hash": "716789bfe3b7eaa40fcb8d44e3a6ec84fc893ba9e3120ab7f083c3618bc0d4b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "99 85.13\nImageNet ReaL 88.35 86.63 88.40 85.65 88.70\nOxford Flowers-102 99.38 99.11 99.61 99.19 99.51\nOxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82\nJFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50\nCIFAR-100 91.87 90.49 94.04 92.52 94.55\nImageNet 84.15 80.73 87.12 84.37 88.04\nImageNet ReaL 88.85 86.27 89.99 88.28 90.33\nOxford Flowers-102 99.56 99.27 99.56 99.45 99.68\nOxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare \ufb01ne-tuned at 384 resolution.", "mimetype": "text/plain", "start_char_idx": 455, "end_char_idx": 1093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55": {"__data__": {"id_": "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91351451-a9c2-42fc-bfb3-3113e0a95206", "node_type": "1", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "cba68352414d0f9bbdc1a4523d3afe96f00cace531b073517251445615b0b41f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "992a0bab-e1f5-4230-a260-245648f5eaa2", "node_type": "1", "metadata": {}, "hash": "d3a3ca8bb15148374b67ed7c8bf4b1f359a663f93aedeedc554dee6361faf2e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "45 99.68\nOxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare \ufb01ne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\nEpochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs\nname\nViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55\nViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224\nViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196\nViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783\nViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567\nViT-H/14 14 88.", "mimetype": "text/plain", "start_char_idx": 815, "end_char_idx": 1596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "992a0bab-e1f5-4230-a260-245648f5eaa2": {"__data__": {"id_": "992a0bab-e1f5-4230-a260-245648f5eaa2", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55", "node_type": "1", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "8072650ee15301e48c16fef1f4f6ad3488993588de5f975029489b4029dd36ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da39d666-015b-46a9-8c65-c34f3c9974e3", "node_type": "1", "metadata": {}, "hash": "b0d5573f5bebc11405c23a95da65f459b4c2a89f252d8b65054698bc5ea1a54c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28 99.19 92.52 95.83 99.45 196\nViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783\nViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567\nViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262\nResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50\nResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199\nResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96\nResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141\nResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563\nResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126\nResNet200x3 14 87.", "mimetype": "text/plain", "start_char_idx": 1446, "end_char_idx": 1974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da39d666-015b-46a9-8c65-c34f3c9974e3": {"__data__": {"id_": "da39d666-015b-46a9-8c65-c34f3c9974e3", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "992a0bab-e1f5-4230-a260-245648f5eaa2", "node_type": "1", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "785d4c95d26dfd06c31c44646f897822df5591fda5992ac101865443f9d36b90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5813ad5-dff4-4b63-b678-ec76a140bc69", "node_type": "1", "metadata": {}, "hash": "e3fe9c770c31407b12d5e2ae00363102ad964b978cf1eeb9693ad8bac540d8c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "96 98.82 90.22 94.17 96.94 141\nResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563\nResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126\nResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306\nR50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106\nR50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274\nR50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246\nR50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859\nR50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668\nTable 6: Detailed results of model scaling experiments.", "mimetype": "text/plain", "start_char_idx": 1815, "end_char_idx": 2354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5813ad5-dff4-4b63-b678-ec76a140bc69": {"__data__": {"id_": "b5813ad5-dff4-4b63-b678-ec76a140bc69", "embedding": null, "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d267b4a6-a619-43f6-9f4e-62443a87df40", "node_type": "4", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6688a6f58deae2160a9705e952c7f3607d0b4b6d550ce412aace7f020ae0d5d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da39d666-015b-46a9-8c65-c34f3c9974e3", "node_type": "1", "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "658db5d2348d8e0bcc8740600cdf05c19892b130eae42f3a013327d2bdb0c9ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "97 99.43 246\nR50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859\nR50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\naFLOPs).\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training.\nD A DDITIONAL ANALYSES\nD.1 SGD VS. A DAM FOR RESNETS\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the \ufb01ne-tuning\n15", "mimetype": "text/plain", "start_char_idx": 2170, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32d6da10-c900-4077-9625-b693c141f1c0": {"__data__": {"id_": "32d6da10-c900-4077-9625-b693c141f1c0", "embedding": null, "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997", "node_type": "4", "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "e03485ac4a2366807b0a5d1e1a7025acb96a62d2775a2d83d62249be64369521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "645373d0-5558-48da-bf73-fcafbe968c49", "node_type": "1", "metadata": {}, "hash": "3896ba53e895503e9a272a21e759ffc9084e4726647358885611a1426e2483cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nResNet50 ResNet152x2\nDataset Adam SGD Adam SGD\nImageNet 77.54 78 .24 84 .97 84 .37\nCIFAR10 97.67 97 .46 99 .06 99 .07\nCIFAR100 86.07 85 .17 92 .05 91 .06\nOxford-IIIT Pets 91.11 91 .00 95 .37 94 .79\nOxford Flowers-102 94.26 92 .06 98 .62 99 .32\nAverage 89.33 88 .79 94 .01 93 .72\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\n100 101\nRelative Compute\n0.2\n0.3\n0.4\n0.5\n0.6ImageNet 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\n100 101\nRelative Compute\n0.4\n0.5\n0.6\n0.7\n0.8Average 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\nFigure 8: Scaling different model dimensions of the Vision Transformer.\nperformance of two ResNets \u2013 50x1 and 152x2 \u2013 pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "645373d0-5558-48da-bf73-fcafbe968c49": {"__data__": {"id_": "645373d0-5558-48da-bf73-fcafbe968c49", "embedding": null, "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997", "node_type": "4", "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "e03485ac4a2366807b0a5d1e1a7025acb96a62d2775a2d83d62249be64369521", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32d6da10-c900-4077-9625-b693c141f1c0", "node_type": "1", "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "af21465513857c683100799238f59366b69b5fa9ef41a22f93d7483a2d12c7e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f7b813-12f9-4d37-9dd5-09130ad56d7a", "node_type": "1", "metadata": {}, "hash": "f68272badbb24c4b6ca2fd7db67605ffb0dd4b05f4d4753b5cc42b8ef3d9e338", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "performance of two ResNets \u2013 50x1 and 152x2 \u2013 pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justi\ufb01es the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor 7 epochs, not 30.\nD.2 T RANSFORMER SHAPE\nWe ran ablations on scaling different dimensions of the Transformer architecture to \ufb01nd out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different con\ufb01gurations. All con\ufb01gurations are based on a ViT model with 8 layers, D = 1024,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.", "mimetype": "text/plain", "start_char_idx": 667, "end_char_idx": 1962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16f7b813-12f9-4d37-9dd5-09130ad56d7a": {"__data__": {"id_": "16f7b813-12f9-4d37-9dd5-09130ad56d7a", "embedding": null, "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997", "node_type": "4", "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "e03485ac4a2366807b0a5d1e1a7025acb96a62d2775a2d83d62249be64369521", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "645373d0-5558-48da-bf73-fcafbe968c49", "node_type": "1", "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "675b96c3a076607c39d5bc55ed6e73a58cc0319360a4f65ef876f3a34c504ef6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese \ufb01ndings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we \ufb01nd that scaling\nall dimensions proportionally results in robust improvements.\nD.3 H EAD TYPE AND C L A S STOKEN\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then trans-\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer.\nThis design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classi\ufb01er\u2014just like ResNet\u2019s \ufb01nal feature map\u2014performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\n16", "mimetype": "text/plain", "start_char_idx": 1543, "end_char_idx": 2967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6700605-08f4-4593-bc3a-1f329edff848": {"__data__": {"id_": "b6700605-08f4-4593-bc3a-1f329edff848", "embedding": null, "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea8c924-e836-47ad-819f-823f299b261a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4917b650e767a8bc76c59bc837b340ef7bfcbd71d60a0a044915a0d4d20d89e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f", "node_type": "1", "metadata": {}, "hash": "6abce8c10a7727c9bb93ed1c370c7de478be6dfa1e4dd1b7582402bd6bddff2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\n0 1 2 3 4 5 6 7\nEpochs of training\n25\n30\n35\n40\n45\n50\n55\n60ImageNet linear 5-shot accuracy [%]\nCLS-Token, lr=8e-4\nGAP, lr=8e-4\nGAP, lr=3e-4\nFigure 9: Comparison of class-token and global average pooling classi\ufb01ers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb. Default/Stem Every Layer Every Layer-Shared\nNo Pos. Emb. 0.61382 N/A N/A\n1-D Pos. Emb. 0.64206 0.63964 0.64292\n2-D Pos. Emb. 0.64001 0.64046 0.64022\nRel. Pos. Emb. 0.64032 N/A N/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9.\nD.4 P OSITIONAL EMBEDDING\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:\n\u2022 Providing no positional information: Considering the inputs as a bag of patches.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f": {"__data__": {"id_": "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f", "embedding": null, "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea8c924-e836-47ad-819f-823f299b261a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4917b650e767a8bc76c59bc837b340ef7bfcbd71d60a0a044915a0d4d20d89e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6700605-08f4-4593-bc3a-1f329edff848", "node_type": "1", "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9f6d005df39d10a6b53d11fbc83baecbfee7a3465356db36f9fafc132bdf699f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad6e0876-ee47-4e7b-b868-4da8672be7bc", "node_type": "1", "metadata": {}, "hash": "37ebddc63ab9438d852216fa98ebcea7a730412fe45d31c33024523b391af75a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9.\nD.4 P OSITIONAL EMBEDDING\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:\n\u2022 Providing no positional information: Considering the inputs as a bag of patches.\n\u2022 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper).\n\u2022 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y-embedding, each with size D/2. Then, based on the coordinate on\nthe path in the input, we concatenate the X and Y embedding to get the \ufb01nal positional\nembedding for that patch.\n\u2022 Relative positional embeddings: Considering the relative distance between patches to en-\ncode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we de\ufb01ne the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq \u2212pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys.", "mimetype": "text/plain", "start_char_idx": 634, "end_char_idx": 2113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad6e0876-ee47-4e7b-b868-4da8672be7bc": {"__data__": {"id_": "ad6e0876-ee47-4e7b-b868-4da8672be7bc", "embedding": null, "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea8c924-e836-47ad-819f-823f299b261a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4917b650e767a8bc76c59bc837b340ef7bfcbd71d60a0a044915a0d4d20d89e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f", "node_type": "1", "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b9569ff34196dc0ae5e4006d4176be7387173e605461be9c9626c4fd10dfa471", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq \u2212pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the log-\nits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax.\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\n17", "mimetype": "text/plain", "start_char_idx": 1796, "end_char_idx": 2579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33deac83-22b5-4747-9255-c26f1c3883d4": {"__data__": {"id_": "33deac83-22b5-4747-9255-c26f1c3883d4", "embedding": null, "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f052b318-2007-4f6f-a256-64edb03f91af", "node_type": "4", "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "f38667677239267f60ac0411c31072280f2a099bf3705ab9e7ed60eb6cf200fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad", "node_type": "1", "metadata": {}, "hash": "5e330ba5f03024c6c28f1ce149814c604649d6a8bbbceba0273e8c8a5398ddb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14 Input patch row\nViT-L16\n7 epochs, LR=0.0002, WD=0.01\n1\n1\nCosine similarity\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14 Input patch row\nViT-L16\n7 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14 Input patch row\nViT-L16\n14 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\nFigure 10: Position embeddings of models trained with different hyperparameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad": {"__data__": {"id_": "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad", "embedding": null, "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f052b318-2007-4f6f-a256-64edb03f91af", "node_type": "4", "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "f38667677239267f60ac0411c31072280f2a099bf3705ab9e7ed60eb6cf200fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33deac83-22b5-4747-9255-c26f1c3883d4", "node_type": "1", "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4c2821b5ba93375a778ea80ecd4c17118b0899152bfded0ef45bb20f7c805b50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84af7591-983f-44dc-a9cd-f773421acc3a", "node_type": "1", "metadata": {}, "hash": "24e88636b63df0631552d8fec1b360cd4816463d130856a05b62c51349fbe156", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers).\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 \u00d714 as opposed to 224 \u00d7224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).", "mimetype": "text/plain", "start_char_idx": 605, "end_char_idx": 1862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84af7591-983f-44dc-a9cd-f773421acc3a": {"__data__": {"id_": "84af7591-983f-44dc-a9cd-f773421acc3a", "embedding": null, "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f052b318-2007-4f6f-a256-64edb03f91af", "node_type": "4", "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "f38667677239267f60ac0411c31072280f2a099bf3705ab9e7ed60eb6cf200fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad", "node_type": "1", "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "d540bf997a4344529152c6455edf8c7b673552564744e25da9269a2ba85bd211", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 \u00d714 as opposed to 224 \u00d7224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 E MPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18", "mimetype": "text/plain", "start_char_idx": 1434, "end_char_idx": 2665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7d4a743-80c4-477e-a31b-3a3e8cf51375": {"__data__": {"id_": "b7d4a743-80c4-477e-a31b-3a3e8cf51375", "embedding": null, "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "add561e2-28e8-4222-b1c7-fdcd671a478d", "node_type": "4", "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "bf7a67adcf7f448ff7c71bfdf85fb4209c997a8d8cfb3cee3b09bee9de451db6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fab223a-44f3-4dee-91ba-0fcbe2f1c064", "node_type": "1", "metadata": {}, "hash": "9bd6f5a23cdfc549b0fb24d59a9d0447b7f9ccb7cd41cadb331e18b8adf75197", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]\n102\n103\n104\nPeak inference speed [img/sec/core] 64 128 224 384 512\nInput size [px]\n102\n103\nLargest per-core batch-size\nR50x1\nR50x2\nViT-B/32\nViT-L/32\nViT-B/16\nViT-L/16\nViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6fab223a-44f3-4dee-91ba-0fcbe2f1c064": {"__data__": {"id_": "6fab223a-44f3-4dee-91ba-0fcbe2f1c064", "embedding": null, "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "add561e2-28e8-4222-b1c7-fdcd671a478d", "node_type": "4", "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "bf7a67adcf7f448ff7c71bfdf85fb4209c997a8d8cfb3cee3b09bee9de451db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7d4a743-80c4-477e-a31b-3a3e8cf51375", "node_type": "1", "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "2f65fa427cda956922d6f7af1214c75dcc13dd411e001cfe2c12bffd702e51a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d76ef4c-ffb6-4e12-9769-f662afccc150", "node_type": "1", "metadata": {}, "hash": "7f26f96300705d5d606f68e0af6a75010ef3235f435e30fcf38e05ec741779f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ViT models\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size \ufb01tting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-ef\ufb01cient.\nD.6 A XIAL ATTENTION\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the \ufb02attened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3 \u00d73 in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3.\nMoreover, we have modi\ufb01ed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP.", "mimetype": "text/plain", "start_char_idx": 1193, "end_char_idx": 2554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d76ef4c-ffb6-4e12-9769-f662afccc150": {"__data__": {"id_": "5d76ef4c-ffb6-4e12-9769-f662afccc150", "embedding": null, "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "add561e2-28e8-4222-b1c7-fdcd671a478d", "node_type": "4", "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "bf7a67adcf7f448ff7c71bfdf85fb4209c997a8d8cfb3cee3b09bee9de451db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fab223a-44f3-4dee-91ba-0fcbe2f1c064", "node_type": "1", "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "db22f3f9fb5be5d8706fe8d535b938bbbde348f66f6872734ee9645e73d7b268", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have implemented\nAxialResNet as a baseline model.3.\nMoreover, we have modi\ufb01ed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP.\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\n3Our implementation is based on the open-sourced PyTorch implementation inhttps://github.com/\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation.\n19", "mimetype": "text/plain", "start_char_idx": 2184, "end_char_idx": 3397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "249c1cb5-ce1a-47f5-9b25-ae9d70db3b8e": {"__data__": {"id_": "249c1cb5-ce1a-47f5-9b25-ae9d70db3b8e", "embedding": null, "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49062df5-b35f-400a-8a24-2525db09c4b6", "node_type": "4", "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9a63fdc78fdf0f9da45b1415d702ff97171b6730e728a5bd357967467280902a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95", "node_type": "1", "metadata": {}, "hash": "f86d02338e4873fc7755489886c8884f15b250396d15214a293c6e67f077c430", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\n102\nTotal compute [exaFLOPs]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650ImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\n102103\nPeak inference speed [img/sec/core]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650ImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95": {"__data__": {"id_": "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95", "embedding": null, "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49062df5-b35f-400a-8a24-2525db09c4b6", "node_type": "4", "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9a63fdc78fdf0f9da45b1415d702ff97171b6730e728a5bd357967467280902a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "249c1cb5-ce1a-47f5-9b25-ae9d70db3b8e", "node_type": "1", "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "7c4763e84ae4c2921999d53ca7ce51a28feb7c918a03bd1284f478b6712a258c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f93f969f-a5d9-4b14-9240-844f9273cc90", "node_type": "1", "metadata": {}, "hash": "7355405b4276a618c0e46834e4e01371f2258a75a3ac568fa9dd7d11a9289206", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7 A TTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This \u201cattention\ndistance\u201d is analogous to receptive \ufb01eld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens.\nD.8 A TTENTION MAPS\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brie\ufb02y, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers.", "mimetype": "text/plain", "start_char_idx": 604, "end_char_idx": 2028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f93f969f-a5d9-4b14-9240-844f9273cc90": {"__data__": {"id_": "f93f969f-a5d9-4b14-9240-844f9273cc90", "embedding": null, "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49062df5-b35f-400a-8a24-2525db09c4b6", "node_type": "4", "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "9a63fdc78fdf0f9da45b1415d702ff97171b6730e728a5bd357967467280902a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95", "node_type": "1", "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "6745a6700fa1ef919f99d5b30c98a33868bd6574eff3bf2b4cf46cd80228fb5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the second half of the network, most heads attend widely across tokens.\nD.8 A TTENTION MAPS\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brie\ufb02y, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers.\nD.9 O BJECT NET RESULTS\nWe also evaluate our \ufb02agship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10 VTAB B REAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20", "mimetype": "text/plain", "start_char_idx": 1654, "end_char_idx": 2407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d1c3c8d-db94-4efb-96e3-b9daa708e381": {"__data__": {"id_": "0d1c3c8d-db94-4efb-96e3-b9daa708e381", "embedding": null, "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8939eb83-bf35-46dd-aecc-7f2cd19055a7", "node_type": "4", "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b8286bb647196a0b4518a95a083a469104f1edb63fe6954539aeb089a48bc9fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c55e45f-c308-4e3e-a815-8dfdefd545e3", "node_type": "1", "metadata": {}, "hash": "d3a3e1a97be045c68a3ce64a5e9610a69f4bb22d0429910f060f82b90e9450d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n97\n 98\n 99\n 100\n 101\n 102\n 103\n 104\n105\n 106\n 107\n 108\n 109\n 110\n 111\n 112\n113\n 114", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c55e45f-c308-4e3e-a815-8dfdefd545e3": {"__data__": {"id_": "8c55e45f-c308-4e3e-a815-8dfdefd545e3", "embedding": null, "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8939eb83-bf35-46dd-aecc-7f2cd19055a7", "node_type": "4", "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "b8286bb647196a0b4518a95a083a469104f1edb63fe6954539aeb089a48bc9fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d1c3c8d-db94-4efb-96e3-b9daa708e381", "node_type": "1", "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "dab1bc29085acaaabd3842aa6fa8c402513bb82f262630084d93040d18105ec2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "80\n81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n97\n 98\n 99\n 100\n 101\n 102\n 103\n 104\n105\n 106\n 107\n 108\n 109\n 110\n 111\n 112\n113\n 114\n 115\n 116\n 117\n 118\n 119\n 120\n121\n 122\n 123\n 124\n 125\n 126\n 127\n 128\nFigure 14: Further example attention maps as in Figure 6 (random selection).\n21", "mimetype": "text/plain", "start_char_idx": 343, "end_char_idx": 640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7b7ee55-485f-4153-8c07-d087c3dd8ae6": {"__data__": {"id_": "c7b7ee55-485f-4153-8c07-d087c3dd8ae6", "embedding": null, "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef7c7ec-41fb-4b82-a752-27d76800397a", "node_type": "4", "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1bd319be10158a854d87be732dcec2ce23774af27b3380752ebda7fd7b67b98a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f0725b7-fe5b-4ef7-9d7f-3af4d2698922", "node_type": "1", "metadata": {}, "hash": "ef3baf7475adc69c72caabc3a6b002d8a8a2f97268dac556a85a2ae86a769d4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published as a conference paper at ICLR 2021\nTable 9: Breakdown of VTAB-1k performance across tasks.\nCaltech101\nCIFAR-100\nDTD\nFlowers102\nPets\nSun397\nSVHN\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\ndSpr-Loc\ndSpr-Ori\nKITTI-Dist\nsNORB-Azim\nsNORB-Elev\nMean\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f0725b7-fe5b-4ef7-9d7f-3af4d2698922": {"__data__": {"id_": "0f0725b7-fe5b-4ef7-9d7f-3af4d2698922", "embedding": null, "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef7c7ec-41fb-4b82-a752-27d76800397a", "node_type": "4", "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "1bd319be10158a854d87be732dcec2ce23774af27b3380752ebda7fd7b67b98a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7b7ee55-485f-4153-8c07-d087c3dd8ae6", "node_type": "1", "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}, "hash": "4e8446b59dad3de5a9d4cb9acb85c7fcb5c0067d8b76bd874c131a075e1b1a13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22", "mimetype": "text/plain", "start_char_idx": 440, "end_char_idx": 625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e96580f-5280-490c-b25d-a7b8a0ad729c": {"__data__": {"id_": "0e96580f-5280-490c-b25d-a7b8a0ad729c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a6b0105-950e-4b44-aaf5-a0b68979032f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "c5bc7f4c4dfe125b6cedad48bc12884b2790e73fd2c806c6673dde34b49b6cdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3c500d8-77b7-464e-a519-63ce7deeea99", "node_type": "1", "metadata": {}, "hash": "7ebe8dc0b9223223dda52a173dadefd8286b849260a5721080ed2d3d9ed83cb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3c500d8-77b7-464e-a519-63ce7deeea99": {"__data__": {"id_": "c3c500d8-77b7-464e-a519-63ce7deeea99", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a6b0105-950e-4b44-aaf5-a0b68979032f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "c5bc7f4c4dfe125b6cedad48bc12884b2790e73fd2c806c6673dde34b49b6cdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e96580f-5280-490c-b25d-a7b8a0ad729c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "ca4ce09c6c0a3c994407ed72df65e5111332213ddb9b8b1486d193ddca4a6286", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "mimetype": "text/plain", "start_char_idx": 1327, "end_char_idx": 2859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f748a6bd-a7e1-430b-8e44-8d0cd209d4ff": {"__data__": {"id_": "f748a6bd-a7e1-430b-8e44-8d0cd209d4ff", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "3fb6ed675928d28fbcca5b0be28a46db934a16f8d1f8558bddd0661e0cdd5e2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc955a05-8a63-4ae5-acde-f5d051e095e8", "node_type": "1", "metadata": {}, "hash": "bd4be3e1ffc39d765b9ce22ae86db482969ccf5cbeab490dbbc22ba89639dfe2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc955a05-8a63-4ae5-acde-f5d051e095e8": {"__data__": {"id_": "fc955a05-8a63-4ae5-acde-f5d051e095e8", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "3fb6ed675928d28fbcca5b0be28a46db934a16f8d1f8558bddd0661e0cdd5e2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f748a6bd-a7e1-430b-8e44-8d0cd209d4ff", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "242d36d38456b3d085fcfd758e502cfee43c2af14e165c3199bd3f9e96d88853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19cf97cb-3a42-457c-a236-fc566a281e77", "node_type": "1", "metadata": {}, "hash": "46f2cf9cf3d1225f82b70ea8a3ddf2cb71071e445202dcb92578226290c0223c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].", "mimetype": "text/plain", "start_char_idx": 1424, "end_char_idx": 3375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19cf97cb-3a42-457c-a236-fc566a281e77": {"__data__": {"id_": "19cf97cb-3a42-457c-a236-fc566a281e77", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "3fb6ed675928d28fbcca5b0be28a46db934a16f8d1f8558bddd0661e0cdd5e2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc955a05-8a63-4ae5-acde-f5d051e095e8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "f2679c9ca000057553181b582e5bba163e9aede7c24291e95ce199c3e6951b20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "mimetype": "text/plain", "start_char_idx": 2941, "end_char_idx": 4257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83e7eb31-e0e6-47f7-97d8-329e33338fb2": {"__data__": {"id_": "83e7eb31-e0e6-47f7-97d8-329e33338fb2", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ac5ff26-e88b-4026-bb66-af26e868e593", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "ce6078cbf0b296b7fe99da92427a4fa1bb2ccce68b7412b2744d2cf8472c2880", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2ded5eb-b0ed-4570-9f2b-3758b13759d0": {"__data__": {"id_": "b2ded5eb-b0ed-4570-9f2b-3758b13759d0", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d458d2e4-ec19-4d8e-b9c7-bed557725715", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "0123532679ae8ef558dc8fd360b2f71aed8e1779656cd7b3aebe91f79bd0e95e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a4f80a5-a997-49dc-a560-971e91331c9f", "node_type": "1", "metadata": {}, "hash": "e79b80da81c0bf087729074be5d6e9e6dc64ece17726eab266386628b622685a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n\u221adk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\u221adk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a4f80a5-a997-49dc-a560-971e91331c9f": {"__data__": {"id_": "8a4f80a5-a997-49dc-a560-971e91331c9f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d458d2e4-ec19-4d8e-b9c7-bed557725715", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "0123532679ae8ef558dc8fd360b2f71aed8e1779656cd7b3aebe91f79bd0e95e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2ded5eb-b0ed-4570-9f2b-3758b13759d0", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "5a382d8243972b4085d60b8c25f728bc0d411b3a3460684089ce6b981a26a8ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221adk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4", "mimetype": "text/plain", "start_char_idx": 1457, "end_char_idx": 2505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b432482c-efaa-4033-b476-4d479d475f37": {"__data__": {"id_": "b432482c-efaa-4033-b476-4d479d475f37", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "95e0448fdac5863f0db986469da133af72b92135db2c07f693a0d5c4608506eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38a3ec1e-955f-4fd1-8c84-43542d8587c5", "node_type": "1", "metadata": {}, "hash": "431bffac0367401b8df2243b6dad5acf72c08425a6331450bbde9573409ea477", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni \u2208 Rdmodel\u00d7dk , WK\ni \u2208 Rdmodel\u00d7dk , WV\ni \u2208 Rdmodel\u00d7dv\nand WO \u2208 Rhdv\u00d7dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38a3ec1e-955f-4fd1-8c84-43542d8587c5": {"__data__": {"id_": "38a3ec1e-955f-4fd1-8c84-43542d8587c5", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "95e0448fdac5863f0db986469da133af72b92135db2c07f693a0d5c4608506eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b432482c-efaa-4033-b476-4d479d475f37", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "39a99196a74a98be674590a06de5516eaf02e93a1e821f3fa065f48bcb536824", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4593f3cd-f137-4e14-be49-58babd346067", "node_type": "1", "metadata": {}, "hash": "f1667974f38cc7577f95527e059c84c59d5b39fc7f5e206005b9bd924f115b0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel.", "mimetype": "text/plain", "start_char_idx": 1155, "end_char_idx": 2840, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4593f3cd-f137-4e14-be49-58babd346067": {"__data__": {"id_": "4593f3cd-f137-4e14-be49-58babd346067", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "95e0448fdac5863f0db986469da133af72b92135db2c07f693a0d5c4608506eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38a3ec1e-955f-4fd1-8c84-43542d8587c5", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "295fef17643f47d19ff891a8315cc66e472a81824f0dfe1a47e41855583649b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5", "mimetype": "text/plain", "start_char_idx": 2480, "end_char_idx": 3188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f7d4712-1fb1-4b9d-91b9-cd9c2e1d50b3": {"__data__": {"id_": "2f7d4712-1fb1-4b9d-91b9-cd9c2e1d50b3", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "340da147-cbec-44de-8d6a-408061fab7ad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "a789a9ce8de438d0bf3d7763c48bb9090bf3c5eeb4760243cb4b72f852f64ee1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5164f84f-80c3-4388-a6ee-f0519fad1a12", "node_type": "1", "metadata": {}, "hash": "34cade5b2c2c8991068e4948d9652059ea00c7a41de9731a18481263ade12098", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 \u00b7 d) O(1) O(1)\nRecurrent O(n \u00b7 d2) O(n) O(n)\nConvolutional O(k \u00b7 n \u00b7 d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r \u00b7 n \u00b7 d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5164f84f-80c3-4388-a6ee-f0519fad1a12": {"__data__": {"id_": "5164f84f-80c3-4388-a6ee-f0519fad1a12", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "340da147-cbec-44de-8d6a-408061fab7ad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "a789a9ce8de438d0bf3d7763c48bb9090bf3c5eeb4760243cb4b72f852f64ee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f7d4712-1fb1-4b9d-91b9-cd9c2e1d50b3", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "b0135dea84a29f2dde67f0b049eb647e2ff6fe30c772a91749e5b16f77ced284", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19e54b55-db0e-46b6-b903-546fe9fd4da7", "node_type": "1", "metadata": {}, "hash": "c0a2709441318a04cf1b94b9fd7a63108a8c65530caf10edfe848fabd164b1c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.", "mimetype": "text/plain", "start_char_idx": 1270, "end_char_idx": 3176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19e54b55-db0e-46b6-b903-546fe9fd4da7": {"__data__": {"id_": "19e54b55-db0e-46b6-b903-546fe9fd4da7", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "340da147-cbec-44de-8d6a-408061fab7ad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "a789a9ce8de438d0bf3d7763c48bb9090bf3c5eeb4760243cb4b72f852f64ee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5164f84f-80c3-4388-a6ee-f0519fad1a12", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "f321cad0e3a88f33f19cdb194733551201dfe395ce74b8f0555a4699d97b84b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "mimetype": "text/plain", "start_char_idx": 2732, "end_char_idx": 3479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "822f888f-2a76-4aea-a5fd-8f8b27a7291d": {"__data__": {"id_": "822f888f-2a76-4aea-a5fd-8f8b27a7291d", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74a2e74e-aa84-428a-9941-d78449b36773", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "2249ccc5814cfc9627cac321e3280aa31bc25ac81b4999491098aab2a14359b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dced52d2-77e2-46e9-a450-6244526fd64e", "node_type": "1", "metadata": {}, "hash": "83e3405588873c0ab24127a39182ece7e46f68a613835d6011fadbca694bd91b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dced52d2-77e2-46e9-a450-6244526fd64e": {"__data__": {"id_": "dced52d2-77e2-46e9-a450-6244526fd64e", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74a2e74e-aa84-428a-9941-d78449b36773", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "2249ccc5814cfc9627cac321e3280aa31bc25ac81b4999491098aab2a14359b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "822f888f-2a76-4aea-a5fd-8f8b27a7291d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "92d3e6215b517c8d9798e14cc4ebdf1ab2c6fd3eca847c692e6a8cee94e4e652", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e0e2811-3e94-41e1-8fde-9621c5ccf686", "node_type": "1", "metadata": {}, "hash": "3aa3c52609b97500e77ac38e4ded6dda4073b828b40bfe275a77080d36a098a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129.", "mimetype": "text/plain", "start_char_idx": 1384, "end_char_idx": 2867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e0e2811-3e94-41e1-8fde-9621c5ccf686": {"__data__": {"id_": "8e0e2811-3e94-41e1-8fde-9621c5ccf686", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74a2e74e-aa84-428a-9941-d78449b36773", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "2249ccc5814cfc9627cac321e3280aa31bc25ac81b4999491098aab2a14359b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dced52d2-77e2-46e9-a450-6244526fd64e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "2d8d5146094c96a1667dec6fd3381d2bf82361269259204e764170fb92aecedf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "mimetype": "text/plain", "start_char_idx": 2636, "end_char_idx": 3322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6622e9a-e276-46fe-b628-f23144968c34": {"__data__": {"id_": "c6622e9a-e276-46fe-b628-f23144968c34", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "67cbe1cb9fbdce64712843865473f3cc4f6d9f128ce0c2e4c7d23a4c0d0ae90a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "377a72bb-a912-4dcf-af45-63f847084974", "node_type": "1", "metadata": {}, "hash": "07ee97a24249d0cbb0cc2103677918668abe1ac3301d1f74646fff6fc7c9a90d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 \u00b7 1020\nGNMT + RL [38] 24.6 39.92 2.3 \u00b7 1019 1.4 \u00b7 1020\nConvS2S [9] 25.16 40.46 9.6 \u00b7 1018 1.5 \u00b7 1020\nMoE [32] 26.03 40.56 2.0 \u00b7 1019 1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \u00b7 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 \u00b7 1020 1.1 \u00b7 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 \u00b7 1019 1.2 \u00b7 1021\nTransformer (base model) 27.3 38.1 3.3 \u00b7 1018\nTransformer (big) 28.4 41.8 2.3 \u00b7 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "377a72bb-a912-4dcf-af45-63f847084974": {"__data__": {"id_": "377a72bb-a912-4dcf-af45-63f847084974", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "67cbe1cb9fbdce64712843865473f3cc4f6d9f128ce0c2e4c7d23a4c0d0ae90a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6622e9a-e276-46fe-b628-f23144968c34", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "ba8f9caacc28ebf6fba202003952b5b21620701abae9c5cb6727ee2cd990bfb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de4d97c3-46e1-4c23-9d05-3a2ec58c4835", "node_type": "1", "metadata": {}, "hash": "0c9143efd3f7987ab34aff9272ae8a8e92c86da315026d4c352cecfd443527e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38].", "mimetype": "text/plain", "start_char_idx": 821, "end_char_idx": 2338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de4d97c3-46e1-4c23-9d05-3a2ec58c4835": {"__data__": {"id_": "de4d97c3-46e1-4c23-9d05-3a2ec58c4835", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "67cbe1cb9fbdce64712843865473f3cc4f6d9f128ce0c2e4c7d23a4c0d0ae90a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "377a72bb-a912-4dcf-af45-63f847084974", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "702a09e64253432649ec5f5a17f26f3292733501376816f4ecc0623ee17836f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "mimetype": "text/plain", "start_char_idx": 1969, "end_char_idx": 3193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa58536d-cbc8-4ed0-b34d-00fec279b4b3": {"__data__": {"id_": "fa58536d-cbc8-4ed0-b34d-00fec279b4b3", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "04157217cf11c2d89c8481f78a0a901b5075e6126911f837d94be8f329f37820", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d422cf13-cb86-4873-be7f-abdf32b0d248", "node_type": "1", "metadata": {}, "hash": "5714b640b0e0123371470889adf7a4fe2fccd469d28a87161474ddb8302c6978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d422cf13-cb86-4873-be7f-abdf32b0d248": {"__data__": {"id_": "d422cf13-cb86-4873-be7f-abdf32b0d248", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "04157217cf11c2d89c8481f78a0a901b5075e6126911f837d94be8f329f37820", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa58536d-cbc8-4ed0-b34d-00fec279b4b3", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "c1a21d22ee8e6688e1a32a8768ffa50925e112326019dedd98f90e0d20f31dbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7f43918-6198-4332-8e11-6505ba0c01ff", "node_type": "1", "metadata": {}, "hash": "63bc8c086dc536e2c922ef8ed3e5d76efd2ba70a705e24aabcfc4fa96b2772ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop \u03f5ls\ntrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7f43918-6198-4332-8e11-6505ba0c01ff": {"__data__": {"id_": "f7f43918-6198-4332-8e11-6505ba0c01ff", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "04157217cf11c2d89c8481f78a0a901b5075e6126911f837d94be8f329f37820", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d422cf13-cb86-4873-be7f-abdf32b0d248", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "580be3b6ea91cabef97e5ab3ed80b4c7ba75bd1f17055f64cca738128f97b8a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "166fd453-7e7a-4e19-b82a-d104916c8632", "node_type": "1", "metadata": {}, "hash": "14eb2817908af7f781286e62beb593898718c577998d175f941063968b4d4e6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.", "mimetype": "text/plain", "start_char_idx": 894, "end_char_idx": 2681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "166fd453-7e7a-4e19-b82a-d104916c8632": {"__data__": {"id_": "166fd453-7e7a-4e19-b82a-d104916c8632", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "04157217cf11c2d89c8481f78a0a901b5075e6126911f837d94be8f329f37820", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7f43918-6198-4332-8e11-6505ba0c01ff", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "3b96d8135b607dea620d391a52aa9d35dac7a98653273b99e5f640f2601b54b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "mimetype": "text/plain", "start_char_idx": 2406, "end_char_idx": 2973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f084646-c2dc-4eec-bec3-9376ecdb8965": {"__data__": {"id_": "4f084646-c2dc-4eec-bec3-9376ecdb8965", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "02c90af44ab3d05a72bb96c1e08df1fb293cf49859df6f1c9bc2d3cce456a632", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f08aefde-f665-47fc-b1bb-3dc532fcb72e", "node_type": "1", "metadata": {}, "hash": "faa764d21320b12313c12a662a6ea1ca5694909aa4a257115bfb5e9ee45a013d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f08aefde-f665-47fc-b1bb-3dc532fcb72e": {"__data__": {"id_": "f08aefde-f665-47fc-b1bb-3dc532fcb72e", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "02c90af44ab3d05a72bb96c1e08df1fb293cf49859df6f1c9bc2d3cce456a632", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f084646-c2dc-4eec-bec3-9376ecdb8965", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "8a4dde3b4d7d58dc5b7dab886d568e123079905fc091dff8ec4581d68f7df19d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ed194c9-0211-4aaa-97cf-6c86afdfec79", "node_type": "1", "metadata": {}, "hash": "40bbafcbf30817a646656fe62bd8857ee13932cd8b2b4b75bcb450aa691fed26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.", "mimetype": "text/plain", "start_char_idx": 797, "end_char_idx": 2656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ed194c9-0211-4aaa-97cf-6c86afdfec79": {"__data__": {"id_": "0ed194c9-0211-4aaa-97cf-6c86afdfec79", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "02c90af44ab3d05a72bb96c1e08df1fb293cf49859df6f1c9bc2d3cce456a632", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f08aefde-f665-47fc-b1bb-3dc532fcb72e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "080e7652fde35f6ae45b0f282fdd6cdf062114c69529efbeaa3656b58cd05596", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10", "mimetype": "text/plain", "start_char_idx": 2286, "end_char_idx": 3112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54cf1130-e1bc-46db-85fc-305e4431a37d": {"__data__": {"id_": "54cf1130-e1bc-46db-85fc-305e4431a37d", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8520c4e3-0df9-429a-b4aa-202e06661a15", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "11544646a2343d64ef447bcf0958fa1fbfcab697c2f51590c1b7e53bdd900d83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25", "node_type": "1", "metadata": {}, "hash": "92369f0c0279a2a0088f73edc3f4ae0cc71216b4376b80d5185472befc2a7441", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25": {"__data__": {"id_": "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8520c4e3-0df9-429a-b4aa-202e06661a15", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "11544646a2343d64ef447bcf0958fa1fbfcab697c2f51590c1b7e53bdd900d83", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54cf1130-e1bc-46db-85fc-305e4431a37d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "69d810b10e92edc3c590bf24b78cc9f62c059ec585a75e2dcde4945467c59733", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f30924-ab67-4032-af15-9018fcbab6e7", "node_type": "1", "metadata": {}, "hash": "5e448f0ebd508ce3e0ee7a48ee4bd5cf90442fbb7cff76a28133badba1d6acf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.", "mimetype": "text/plain", "start_char_idx": 836, "end_char_idx": 2111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6f30924-ab67-4032-af15-9018fcbab6e7": {"__data__": {"id_": "c6f30924-ab67-4032-af15-9018fcbab6e7", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8520c4e3-0df9-429a-b4aa-202e06661a15", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "11544646a2343d64ef447bcf0958fa1fbfcab697c2f51590c1b7e53bdd900d83", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "695f43cb44c21ca8500f1697881717d3df5e09cd7688ad9b59e15f57cef42ea4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92880e25-c799-44c2-8141-4decf2bb4ec1", "node_type": "1", "metadata": {}, "hash": "650e4576b39a15c9adfa31f4e5a95933606f7e58d024ed36bf3c62f609d3d752", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning.", "mimetype": "text/plain", "start_char_idx": 1793, "end_char_idx": 3004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92880e25-c799-44c2-8141-4decf2bb4ec1": {"__data__": {"id_": "92880e25-c799-44c2-8141-4decf2bb4ec1", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8520c4e3-0df9-429a-b4aa-202e06661a15", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "11544646a2343d64ef447bcf0958fa1fbfcab697c2f51590c1b7e53bdd900d83", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6f30924-ab67-4032-af15-9018fcbab6e7", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "15f148c2e1bd71e29c16b831547f3c332a4f58385598e25c11e0c96b6359427c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11", "mimetype": "text/plain", "start_char_idx": 2790, "end_char_idx": 3215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8afe343c-7a2d-4db7-a179-886423a7dfb4": {"__data__": {"id_": "8afe343c-7a2d-4db7-a179-886423a7dfb4", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5df396e7-e704-4f2c-87cd-26f26836bc55", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "97872dd698748260218b3018ad2c50f743585347c16edf515ce222dfc2d4c7a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3db08707-3250-4d28-ba7d-ccb5b5743e3b", "node_type": "1", "metadata": {}, "hash": "05adcdc15286b0db14f0e43e76bed165fa3360af9bf33dbe19978ed8b5c9d763", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3db08707-3250-4d28-ba7d-ccb5b5743e3b": {"__data__": {"id_": "3db08707-3250-4d28-ba7d-ccb5b5743e3b", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5df396e7-e704-4f2c-87cd-26f26836bc55", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "97872dd698748260218b3018ad2c50f743585347c16edf515ce222dfc2d4c7a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afe343c-7a2d-4db7-a179-886423a7dfb4", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "dbda66336bafd662c23762c6bd05d88fbd8422b8161b7fa4c3bf4dbfa6bfbea9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "691090d0-5c1e-4f05-80b8-30d5f150ba33", "node_type": "1", "metadata": {}, "hash": "a40417007fb53ae776240331f467c4b06db0ad2f63bfc7e3a7175e41e6938538", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.", "mimetype": "text/plain", "start_char_idx": 968, "end_char_idx": 2186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "691090d0-5c1e-4f05-80b8-30d5f150ba33": {"__data__": {"id_": "691090d0-5c1e-4f05-80b8-30d5f150ba33", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5df396e7-e704-4f2c-87cd-26f26836bc55", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "97872dd698748260218b3018ad2c50f743585347c16edf515ce222dfc2d4c7a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3db08707-3250-4d28-ba7d-ccb5b5743e3b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "3725a1a4e2902d5a7e7d65c96fbfae151922be423c3aeb37cde2e2a8ba90fd8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12", "mimetype": "text/plain", "start_char_idx": 1975, "end_char_idx": 3213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1161c14e-cfeb-49a9-b8c7-791cd978bac1": {"__data__": {"id_": "1161c14e-cfeb-49a9-b8c7-791cd978bac1", "embedding": null, "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d562e8c9-4a52-44d4-a202-22643c69e2c8", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "d1f7165940e8fd8b0434ac12f47bef0cfd7238856c955c8441a2e8077362c29b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87054fef-106c-4066-974c-7772c83383e6": {"__data__": {"id_": "87054fef-106c-4066-974c-7772c83383e6", "embedding": null, "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35f71d0-6f82-47da-b623-7dc45d888860", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "9b8fb49a13dfafa0ba15a3c7be6c7dbc61277418aeba0dfd312efcf6f1189fe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee4efd4c-4b1f-43cd-b853-9ba0b0b08ee4": {"__data__": {"id_": "ee4efd4c-4b1f-43cd-b853-9ba0b0b08ee4", "embedding": null, "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86552836-947b-40c4-855b-5105b35d0ee5", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}, "hash": "a24d54ba776f4bbb0de1be6f8df3d6bd5e3a0d50deda2c2828eaaf3923915dba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03d47bd3-bc2d-4290-988f-65337cc71d9b": {"__data__": {"id_": "03d47bd3-bc2d-4290-988f-65337cc71d9b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560c2293-8732-4956-98bf-c39c108e1d95", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "db00b9b5ab47d13ddb4947b7f3c711977de4922eda6f28944be4a9731e8eca95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79ef22a0-af76-4a0c-99c4-4b462168df45", "node_type": "1", "metadata": {}, "hash": "33242155a936b99a56e655b91952b16cbdafd107cf6ca50273651d513408a7f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AST: Audio Spectrogram Transformer\nYuan Gong, Yu-An Chung, James Glass\nMIT Computer Science and Arti\ufb01cial Intelligence Laboratory, Cambridge, MA 02139, USA\n{yuangong, andyyuan, glass}@mit.edu\nAbstract\nIn the past decade, convolutional neural networks (CNNs)\nhave been widely adopted as the main building block for end-\nto-end audio classi\ufb01cation models, which aim to learn a direct\nmapping from audio spectrograms to corresponding labels. To\nbetter capture long-range global context, a recent trend is to\nadd a self-attention mechanism on top of the CNN, forming a\nCNN-attention hybrid model. However, it is unclear whether\nthe reliance on a CNN is necessary, and if neural networks\npurely based on attention are suf\ufb01cient to obtain good perfor-\nmance in audio classi\ufb01cation. In this paper, we answer the ques-\ntion by introducing the Audio Spectrogram Transformer(AST),\nthe \ufb01rst convolution-free, purely attention-based model for au-\ndio classi\ufb01cation. We evaluate AST on various audio classi\ufb01-\ncation benchmarks, where it achieves new state-of-the-art re-\nsults of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,\nand 98.1% accuracy on Speech Commands V2.\nIndex Terms: audio classi\ufb01cation, self-attention, Transformer\n1. Introduction\nWith the advent of deep neural networks, over the last decade\naudio classi\ufb01cation research has moved from models based\non hand-crafted features [1, 2] to end-to-end models that di-\nrectly map audio spectrograms to corresponding labels [3, 4, 5].\nSpeci\ufb01cally, convolutional neural networks (CNNs) [6] have\nbeen widely used to learn representations from raw spectro-\ngrams for end-to-end modeling, as the inductive biases inherent\nto CNNs such as spatial locality and translation equivariance\nare believed to be helpful.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79ef22a0-af76-4a0c-99c4-4b462168df45": {"__data__": {"id_": "79ef22a0-af76-4a0c-99c4-4b462168df45", "embedding": null, "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560c2293-8732-4956-98bf-c39c108e1d95", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "db00b9b5ab47d13ddb4947b7f3c711977de4922eda6f28944be4a9731e8eca95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03d47bd3-bc2d-4290-988f-65337cc71d9b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "4f37342cb8f202877c68899f508329537cea945bbc3c6682c839656f899e8b2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf", "node_type": "1", "metadata": {}, "hash": "bc0ef328cff2ba12e072300e7b654bb773d1b304b19fec8f22d8a432106efe56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Speci\ufb01cally, convolutional neural networks (CNNs) [6] have\nbeen widely used to learn representations from raw spectro-\ngrams for end-to-end modeling, as the inductive biases inherent\nto CNNs such as spatial locality and translation equivariance\nare believed to be helpful. In order to better capture long-range\nglobal context, a recent trend is to add a self-attention mech-\nanism on top of the CNN. Such CNN-attention hybrid mod-\nels have achieved state-of-the-art (SOTA) results for many au-\ndio classi\ufb01cation tasks such as audio event classi\ufb01cation [7, 8],\nspeech command recognition [9], and emotion recognition [10].\nHowever, motivated by the success of purely attention-based\nmodels in the vision domain [11, 12, 13], it is reasonable to ask\nwhether a CNN is still essential for audio classi\ufb01cation.\nTo answer the question, we introduce the Audio Spectro-\ngram Transformer (AST), a convolution-free, purely attention-\nbased model that is directly applied to an audio spectrogram\nand can capture long-range global context even in the lowest\nlayers. Additionally, we propose an approach for transferring\nknowledge from the Vision Transformer (ViT) [12] pretrained\non ImageNet [14] to AST, which can signi\ufb01cantly improve the\nperformance. The advantages of AST are threefold. First, AST\nhas superior performance: we evaluate AST on a variety of au-\ndio classi\ufb01cation tasks and datasets including AudioSet [15],\nESC-50 [16] and Speech Commands [17]. AST outperforms\nstate-of-the-art systems on all these datasets. Second, AST nat-\nurally supports variable-length inputs and can be applied to dif-\nferent tasks without any change of architecture. Speci\ufb01cally, the\nCode at https://github.com/YuanGongND/ast.", "mimetype": "text/plain", "start_char_idx": 1482, "end_char_idx": 3188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf": {"__data__": {"id_": "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf", "embedding": null, "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560c2293-8732-4956-98bf-c39c108e1d95", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "db00b9b5ab47d13ddb4947b7f3c711977de4922eda6f28944be4a9731e8eca95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79ef22a0-af76-4a0c-99c4-4b462168df45", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "951fe69d639b7aa9d7a0acec56d050b830cc5f8d31a767a38c87e79114e09fb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14982d34-ffc2-422b-8652-ad1d9ae7fec8", "node_type": "1", "metadata": {}, "hash": "e032f8d0d05d865d03fa26bf09b09007f0d361c1cc2065c364515a582fb13d62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AST outperforms\nstate-of-the-art systems on all these datasets. Second, AST nat-\nurally supports variable-length inputs and can be applied to dif-\nferent tasks without any change of architecture. Speci\ufb01cally, the\nCode at https://github.com/YuanGongND/ast.\nTransformer Encoder\nLinear ProjectionE[CLS]\nP[0]\nE[1]E[2]E[3]E[4]E[5]E[6]E[7]E[8]\nP[1]P[2]P[3]P[4]P[5]P[6]P[7]P[8]\nLinearOutput\nPatch Split with Overlap\nPatchEmbedding\nPositional Embedding\nPatch Embedding\nInput Spectrogram\n1 2 3 456 78\n1 2 3 4 5 6 7 8\nFigure 1: The proposed audio spectrogram transformer (AST)\narchitecture. The 2D audio spectrogram is split into a sequence\nof 16\u00d716 patches with overlap, and then linearly projected to\na sequence of 1-D patch embeddings. Each patch embedding\nis added with a learnable positional embedding. An additional\nclassi\ufb01cation token is prepended to the sequence. The output\nembedding is input to a Transformer, and the output of the clas-\nsi\ufb01cation token is used for classi\ufb01cation with a linear layer.\nmodels we use for all aforementioned tasks have the same archi-\ntecture while the input lengths vary from 1 sec. (Speech Com-\nmands) to 10 sec. (AudioSet). In contrast, CNN-based models\ntypically require architecture tuning to obtain optimal perfor-\nmance for different tasks. Third, comparing with SOTA CNN-\nattention hybrid models, AST features a simpler architecture\nwith fewer parameters, and converges faster during training. To\nthe best of our knowledge, AST is the \ufb01rst purely attention-\nbased audio classi\ufb01cation model.", "mimetype": "text/plain", "start_char_idx": 2933, "end_char_idx": 4461, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14982d34-ffc2-422b-8652-ad1d9ae7fec8": {"__data__": {"id_": "14982d34-ffc2-422b-8652-ad1d9ae7fec8", "embedding": null, "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560c2293-8732-4956-98bf-c39c108e1d95", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "db00b9b5ab47d13ddb4947b7f3c711977de4922eda6f28944be4a9731e8eca95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "301f491b315eccd66e625625ecc44c0aeb806f257e5bbc8744117c5e1db554f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(Speech Com-\nmands) to 10 sec. (AudioSet). In contrast, CNN-based models\ntypically require architecture tuning to obtain optimal perfor-\nmance for different tasks. Third, comparing with SOTA CNN-\nattention hybrid models, AST features a simpler architecture\nwith fewer parameters, and converges faster during training. To\nthe best of our knowledge, AST is the \ufb01rst purely attention-\nbased audio classi\ufb01cation model.\nRelated Work The proposed Audio Spectrogram Trans-\nformer, as the name suggests, is based on the Transformer ar-\nchitecture [18], which was originally proposed for natural lan-\nguage processing tasks. Recently, the Transformer has also\nbeen adapted for audio processing, but is typically used in\nconjunction with a CNN [19, 20, 21]. In [19, 20], the au-\nthors stack a Transformer on top of a CNN, while in [21],\nthe authors combine a Transformer and a CNN in each model\nblock. Other efforts combine CNNs with simpler attention\nmodules [8, 7, 9]. The proposed AST differs from these stud-\nies in that it is convolution-free and purely based on attention\nmechanisms. The closest work to ours is the Vision Trans-\nformer (ViT) [11, 12, 13], which is a Transformer architecture\nfor vision tasks. AST and ViT have similar architectures but\nViT has only been applied to \ufb01xed-dimensional inputs (images)\nwhile AST can process variable-length audio inputs. In addi-\ntion, we propose an approach to transfer knowledge from Ima-\ngeNet pretrained ViT to AST. We also conduct extensive exper-\niments to show the design choice of AST on audio tasks.\narXiv:2104.01778v3  [cs.SD]  8 Jul 2021", "mimetype": "text/plain", "start_char_idx": 4047, "end_char_idx": 5638, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "488dad6f-7414-45a9-b3fc-a8e9c9f796dd": {"__data__": {"id_": "488dad6f-7414-45a9-b3fc-a8e9c9f796dd", "embedding": null, "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "d07ab6d7933a9ee3cbd5d2b97aa770d1b998250923535526c0ff43baa57c6abd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72f12176-914c-4620-861c-678d50d5bd8b", "node_type": "1", "metadata": {}, "hash": "74a4b23444240df477fadacd157c2637d2610bc82a18a7c4bb212f4918ffbdaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Audio Spectrogram Transformer\n2.1. Model Architecture\nFigure 1 illustrates the proposed Audio Spectrogram Trans-\nformer (AST) architecture. First, the input audio waveform of t\nseconds is converted into a sequence of 128-dimensional log\nMel \ufb01lterbank (fbank) features computed with a 25ms Ham-\nming window every 10ms. This results in a128 \u00d7100t spectro-\ngram as input to the AST. We then split the spectrogram into a\nsequence of N 16\u00d716 patches with an overlap of 6 in both time\nand frequency dimension, where N = 12\u2308(100t \u221216)/10\u2309is\nthe number of patches and the effective input sequence length\nfor the Transformer. We \ufb02atten each16\u00d716 patch to a 1D patch\nembedding of size 768 using a linear projection layer. We re-\nfer to this linear projection layer as the patch embedding layer.\nSince the Transformer architecture does not capture the input\norder information and the patch sequence is also not in tem-\nporal order, we add a trainable positional embedding (also of\nsize 768) to each patch embedding to allow the model to cap-\nture the spatial structure of the 2D audio spectrogram.\nSimilar to [22], we append a [CLS] token at the beginning\nof the sequence. The resulting sequence is then input to the\nTransformer. A Transformer consists of several encoder and\ndecoder layers. Since AST is designed for classi\ufb01cation tasks,\nwe only use the encoder of the Transformer. Intentionally, we\nuse the original Transformer encoder [18] architecture without\nmodi\ufb01cation. The advantages of this simple setup are 1) the\nstandard Transformer architecture is easy to implement and re-\nproduce as it is off-the-shelf in TensorFlow and PyTorch, and\n2) we intend to apply transfer learning for AST, and a stan-\ndard architecture makes transfer learning easier.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72f12176-914c-4620-861c-678d50d5bd8b": {"__data__": {"id_": "72f12176-914c-4620-861c-678d50d5bd8b", "embedding": null, "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "d07ab6d7933a9ee3cbd5d2b97aa770d1b998250923535526c0ff43baa57c6abd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "488dad6f-7414-45a9-b3fc-a8e9c9f796dd", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "dc67e0db9cd82bbc8a4d24177619fce9b674c959f0a96d0a46fc1039b42e3c53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7944edc1-9968-4708-9e7f-1ff39f28f721", "node_type": "1", "metadata": {}, "hash": "d76765264ff2f48d2f3b989048cd6c7a0c07b6dedf5a22659e86e60d63f19a3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Intentionally, we\nuse the original Transformer encoder [18] architecture without\nmodi\ufb01cation. The advantages of this simple setup are 1) the\nstandard Transformer architecture is easy to implement and re-\nproduce as it is off-the-shelf in TensorFlow and PyTorch, and\n2) we intend to apply transfer learning for AST, and a stan-\ndard architecture makes transfer learning easier. Speci\ufb01cally,\nthe Transformer encoder we use has an embedding dimension\nof 768, 12 layers, and 12 heads, which are the same as those\nin [12, 11]. The Transformer encoder\u2019s output of the [CLS]\ntoken serves as the audio spectrogram representation. A linear\nlayer with sigmoid activation maps the audio spectrogram rep-\nresentation to labels for classi\ufb01cation.\nStrictly speaking, the patch embedding layer can be viewed\nas a single convolution layer with a large kernel and stride size,\nand the projection layer in each Transformer block is equivalent\nto 1\u00d71 convolution. However, the design is different from con-\nventional CNNs that have multiple layers and small kernel and\nstride sizes. These Transformer models are usually referred to\nas convolution-free to distinguish them from CNNs [11, 12].\n2.2. ImageNet Pretraining\nOne disadvantage of the Transformer compared with CNNs is\nthat the Transformer needs more data to train [11]. In [11],\nthe authors point out that the Transformer only starts to out-\nperform CNNs when the amount of data is over 14 million for\nimage classi\ufb01cation tasks. However, audio datasets typically\ndo not have such large amounts of data, which motivates us\nto apply cross-modality transfer learning to AST since images\nand audio spectrograms have similar formats. Transfer learn-\ning from vision tasks to audio tasks has been previously stud-\nied in [23, 24, 25, 8], but only for CNN-based models, where\nImageNet-pretrained CNN weights are used as initial CNN\nweights for audio classi\ufb01cation training.", "mimetype": "text/plain", "start_char_idx": 1375, "end_char_idx": 3280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7944edc1-9968-4708-9e7f-1ff39f28f721": {"__data__": {"id_": "7944edc1-9968-4708-9e7f-1ff39f28f721", "embedding": null, "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "d07ab6d7933a9ee3cbd5d2b97aa770d1b998250923535526c0ff43baa57c6abd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72f12176-914c-4620-861c-678d50d5bd8b", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "a0595907545508d9cb23c06dc864985be9a11deff274c267dc6c00f2b8994826", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d1a925d-c4f5-41cb-8bbd-aacd58b25818", "node_type": "1", "metadata": {}, "hash": "73f1eec0115ed89400a9f30974dc3c4549a10b82179946b42d385b9cadd28b21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Transfer learn-\ning from vision tasks to audio tasks has been previously stud-\nied in [23, 24, 25, 8], but only for CNN-based models, where\nImageNet-pretrained CNN weights are used as initial CNN\nweights for audio classi\ufb01cation training. In practice, it is com-\nputationally expensive to train a state-of-the-art vision model,\nbut many commonly used architectures (e.g., ResNet [26], Ef-\n\ufb01cientNet [27]) have off-the-shelf ImageNet-pretrained mod-\nels for both TensorFlow and PyTorch, making transfer learning\nmuch easier. We also follow this regime by adapting an off-the-\nshelf pretrained Vision Transformer (ViT) to AST.\nWhile ViT and AST have similar architectures (e.g., both\nuse a standard Transformer, same patch size, same embedding\nsize), they are not same. Therefore, a few modi\ufb01cations need to\nmake for the adaptation. First, the input of ViT is a 3-channel\nimage while the input to the AST is a single-channel spectro-\ngram, we average the weights corresponding to each of the\nthree input channels of the ViT patch embedding layer and use\nthem as the weights of the AST patch embedding layer. This\nis equivalent to expanding a single-channel spectrogram to 3-\nchannels with the same content, but is computationally more\nef\ufb01cient. We also normalize the input audio spectrogram so that\nthe dataset mean and standard deviation are 0 and 0.5, respec-\ntively. Second, the input shape of ViT is \ufb01xed (either224 \u00d7224\nor 384 \u00d7384), which is different from a typical audio spectro-\ngram. In addition, the length of an audio spectrogram can be\nvariable. While the Transformer naturally supports variable in-\nput length and can be directly transferred from ViT to AST, the\npositional embedding needs to be carefully processed because\nit learns to encode the spatial information during the ImageNet\ntraining.", "mimetype": "text/plain", "start_char_idx": 3043, "end_char_idx": 4851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d1a925d-c4f5-41cb-8bbd-aacd58b25818": {"__data__": {"id_": "1d1a925d-c4f5-41cb-8bbd-aacd58b25818", "embedding": null, "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "d07ab6d7933a9ee3cbd5d2b97aa770d1b998250923535526c0ff43baa57c6abd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7944edc1-9968-4708-9e7f-1ff39f28f721", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "5aafe1c1f8058c7e6a6646fe0b39dc6901ac88b1154ec4764401a4186b8d6ab9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "779702f8-33b3-4388-86db-e6877a5317b1", "node_type": "1", "metadata": {}, "hash": "c0f57cf2ca23ece380fa14c1c1bed2f3fc048d6282568fca9b2d77b5fb4f9b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, the input shape of ViT is \ufb01xed (either224 \u00d7224\nor 384 \u00d7384), which is different from a typical audio spectro-\ngram. In addition, the length of an audio spectrogram can be\nvariable. While the Transformer naturally supports variable in-\nput length and can be directly transferred from ViT to AST, the\npositional embedding needs to be carefully processed because\nit learns to encode the spatial information during the ImageNet\ntraining. We propose a cut and bi-linear interpolate method for\npositional embedding adaptation. For example, for a ViT that\ntakes 384 \u00d7384 image input and uses a patch size of 16 \u00d716,\nthe number of patches and corresponding positional embedding\nis 24 \u00d724 = 576 (ViT splits patches without overlap). An\nAST that takes 10-second audio input has 12 \u00d7100 patches,\neach patch needs a positional embedding. We therefore cut\nthe \ufb01rst dimension and interpolate the second dimension of the\n24 \u00d724 ViT positional embedding to 12 \u00d7100 and use it as\nthe positional embedding for the AST. We directly reuse the\npositional embedding for the [CLS] token. By doing this we\nare able to transfer the 2D spatial knowledge from a pretrained\nViT to the AST even when the input shapes are different. Fi-\nnally, since the classi\ufb01cation task is essentially different, we\nabandon the last classi\ufb01cation layer of the ViT and reinitialize\na new one for AST. With this adaptation framework, the AST\ncan use various pretrained ViT weights for initialization. In this\nwork, we use pretrained weights of a data-ef\ufb01cient image Trans-\nformer (DeiT) [12], which is trained with CNN knowledge dis-\ntillation, 384 \u00d7384 images, has 87M parameters, and achieves\n85.2% top-1 accuracy on ImageNet 2012. During ImageNet\ntraining, DeiT has two [CLS] tokens; we average them as a\nsingle [CLS] token for audio training.", "mimetype": "text/plain", "start_char_idx": 4410, "end_char_idx": 6218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "779702f8-33b3-4388-86db-e6877a5317b1": {"__data__": {"id_": "779702f8-33b3-4388-86db-e6877a5317b1", "embedding": null, "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "d07ab6d7933a9ee3cbd5d2b97aa770d1b998250923535526c0ff43baa57c6abd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d1a925d-c4f5-41cb-8bbd-aacd58b25818", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "753d703ee590d6ed96c946185fd36fce6591d1a55cdef722b9d8dee3f867d61d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this\nwork, we use pretrained weights of a data-ef\ufb01cient image Trans-\nformer (DeiT) [12], which is trained with CNN knowledge dis-\ntillation, 384 \u00d7384 images, has 87M parameters, and achieves\n85.2% top-1 accuracy on ImageNet 2012. During ImageNet\ntraining, DeiT has two [CLS] tokens; we average them as a\nsingle [CLS] token for audio training.\n3. Experiments\nIn this section, we focus on evaluating the AST on AudioSet\n(Section 3.1) as weakly-labeled audio event classi\ufb01cation is one\nof the most challenging audio classi\ufb01cation tasks. We present\nour primary AudioSet results and ablation study in Section 3.1.2\nand Section 3.1.3, respectively. We then present our experi-\nments on ESC-50 and Speech Commands V2 in Section 3.2.\n3.1. AudioSet Experiments\n3.1.1. Dataset and Training Details\nAudioSet [15] is a collection of over 2 million 10-second au-\ndio clips excised from YouTube videos and labeled with the\nsounds that the clip contains from a set of 527 labels. The bal-\nanced training, full training, and evaluation set contains 22k,\n2M, and 20k samples, respectively. For AudioSet experiments,\nwe use the exact same training pipeline with [8]. Speci\ufb01cally,\nwe use ImageNet pretraining (as described in Section 2.2), bal-\nanced sampling (for full set experiments only), data augmenta-", "mimetype": "text/plain", "start_char_idx": 5873, "end_char_idx": 7164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03e7f862-198e-4a96-9756-b97865f55d6c": {"__data__": {"id_": "03e7f862-198e-4a96-9756-b97865f55d6c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "197846d7-4db7-44b6-9e92-1cf00a44b087", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "3ac36b9f43b76ed93af6ba21d9b836e64544f86177d47fda066754eb48ea596a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4", "node_type": "1", "metadata": {}, "hash": "72de9585d0ec2601f9486762cf5d71dddcbbd47af3af1aef90e34da76b8ae047", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Performance comparison of AST and previous methods\non AudioSet.\nModel\nArchitecture\nBalanced\nmAP\nFull\nmAP\nBaseline [15] CNN+MLP - 0.314\nPANNs [7] CNN+Attention 0.278 0.439\nPSLA [8] (Single) CNN+Attention 0.319 0.444\nPSLA (Ensemble-S) CNN+Attention 0.345 0.464\nPSLA (Ensemble-M) CNN+Attention 0.362 0.474\nAST (Single) Pure Attention 0.347\n\u00b1 0.001\n0.459\n\u00b1 0.000\nAST (Ensemble-S) Pure Attention 0.363 0.475\nAST (Ensemble-M) Pure Attention 0.378 0.485\ntion (including mixup [28] with mixup ratio =0.5 and spectro-\ngram masking [29] with max time mask length of 192 frames\nand max frequency mask length of 48 bins), and model aggrega-\ntion (including weight averaging [30] and ensemble [31]). We\ntrain the model with a batch size of 12, the Adam optimizer [32],\nand use binary cross-entropy loss. We conduct experiments on\nthe of\ufb01cial balanced and full training set and evaluate on the Au-\ndioSet evaluation set. For balanced set experiments, we use an\ninitial learning rate of 5e-5 and train the model for 25 epochs,\nthe learning rate is cut into half every 5 epoch after the 10th\nepoch. For full set experiments, we use an initial learning rate\nof 1e-5 and train the model for 5 epochs, the learning rate is\ncut into half every epoch after the 2nd epoch. We use the mean\naverage precision (mAP) as our main evaluation metric.\n3.1.2. AudioSet Results\nWe repeat each experiment three times with the same setup but\ndifferent random seeds and report the mean and standard devi-\nation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4": {"__data__": {"id_": "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4", "embedding": null, "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "197846d7-4db7-44b6-9e92-1cf00a44b087", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "3ac36b9f43b76ed93af6ba21d9b836e64544f86177d47fda066754eb48ea596a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03e7f862-198e-4a96-9756-b97865f55d6c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "2c28f17bc07f54faac92016a7a9c946a74d645343eaeb16238d7dc791d78ea68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dd01c88-c64f-444a-b5dc-12d186420111", "node_type": "1", "metadata": {}, "hash": "c8fbcdd037ffb2b221cb6584d3bbbc289f817f9db2304dc3c1e56be88d73ca48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For full set experiments, we use an initial learning rate\nof 1e-5 and train the model for 5 epochs, the learning rate is\ncut into half every epoch after the 2nd epoch. We use the mean\naverage precision (mAP) as our main evaluation metric.\n3.1.2. AudioSet Results\nWe repeat each experiment three times with the same setup but\ndifferent random seeds and report the mean and standard devi-\nation. When AST is trained with the full AudioSet, the mAP at\nthe last epoch is 0.448 \u00b10.001. As in [8], we also use weight\naveraging [30] and ensemble [31] strategies to further improve\nthe performance of AST. Speci\ufb01cally, for weight averaging, we\naverage all weights of the model checkpoints from the \ufb01rst to\nthe last epoch. The weight-averaged model achieves an mAP of\n0.459\u00b10.000, which is our best single model (weight averag-\ning does not increase the model size). For ensemble, we eval-\nuate two settings: 1) Ensemble-S: we run the experiment three\ntimes with the exact same setting, but with a different random\nseed. We then average the output of the last checkpoint model of\neach run. In this setting, the ensemble model achieves an mAP\nof 0.475; 2) Ensemble-M: we ensemble models trained with\ndifferent settings, speci\ufb01cally, we ensemble the three models\nin Ensemble-S together with another three models trained with\ndifferent patch split strategies (described in Section 3.1.3 and\nshown in Table 5). In this setting, the ensemble model achieves\nan mAP of 0.485, this is our best full model on AudioSet. As\nshown in Table 1, the proposed AST outperforms the previous\nbest system in [8] in all settings.", "mimetype": "text/plain", "start_char_idx": 1092, "end_char_idx": 2691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dd01c88-c64f-444a-b5dc-12d186420111": {"__data__": {"id_": "3dd01c88-c64f-444a-b5dc-12d186420111", "embedding": null, "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "197846d7-4db7-44b6-9e92-1cf00a44b087", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "3ac36b9f43b76ed93af6ba21d9b836e64544f86177d47fda066754eb48ea596a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "956766894dbe33650d842b2f44b877ad3e923c298a7da97683545c216f1b883c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42c7e353-4d6a-4a6d-a182-3b52b822da15", "node_type": "1", "metadata": {}, "hash": "62eafaaf2b9e76976f8aff7681795f19f16b5825c770bd5aff31d5ceaf5ac2d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this setting, the ensemble model achieves\nan mAP of 0.485, this is our best full model on AudioSet. As\nshown in Table 1, the proposed AST outperforms the previous\nbest system in [8] in all settings. Note that we use the same\ntraining pipeline with [8] and [8] also use ImageNet pretrain-\ning, so it is a fair comparison. In addition, we use fewer models\n(6) for our best ensemble models than [8] (10). Finally, it is\nworth mentioning that AST training converges quickly; AST\nonly needs 5 training epochs, while in [8], the CNN-attention\nhybrid model is trained for 30 epochs.\nWe also conduct experiments with the balanced AudioSet\n(about 1% of the full set) to evaluate the performance of AST\nwhen the training data volume is smaller. For weight averag-\ning, we average all weights of the model checkpoints of the\nTable 2: Performance impact due to ImageNet pretraining.\n\u201cUsed\u201d denotes the setting used by our optimal AST model.\nBalanced Set Full Set\nNo Pretrain 0.148 0.366\nImageNet Pretrain (Used) 0.347 0.459\nTable 3: Performance of AST models initialized with different\nViT weights on balanced AudioSet and corresponding ViT mod-\nels\u2019 top-1 accuracy on ImageNet 2012. (* Model is trained\nwithout patch split overlap due to memory limitation.)\n# Params ImageNet AudioSet\nViT Base [11] 86M 0.846 0.320\nViT Large [11]* 307M 0.851 0.330\nDeiT w/o Distill [12] 86M 0.829 0.330\nDeiT w/ Distill (Used) 87M 0.852 0.347\nlast 20 epochs.", "mimetype": "text/plain", "start_char_idx": 2490, "end_char_idx": 3922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42c7e353-4d6a-4a6d-a182-3b52b822da15": {"__data__": {"id_": "42c7e353-4d6a-4a6d-a182-3b52b822da15", "embedding": null, "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "197846d7-4db7-44b6-9e92-1cf00a44b087", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "3ac36b9f43b76ed93af6ba21d9b836e64544f86177d47fda066754eb48ea596a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dd01c88-c64f-444a-b5dc-12d186420111", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "6081bedee2d035ab97326b8165a3fd17e750da8ec1fa847c27b2b2452f7e4157", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f1618d-e6a6-4b55-8de9-8b66eff2f39d", "node_type": "1", "metadata": {}, "hash": "834b12779ec23859a64fa4bb19d9eea01a4e5272cca3c981f0d942618098e94c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Params ImageNet AudioSet\nViT Base [11] 86M 0.846 0.320\nViT Large [11]* 307M 0.851 0.330\nDeiT w/o Distill [12] 86M 0.829 0.330\nDeiT w/ Distill (Used) 87M 0.852 0.347\nlast 20 epochs. For Ensemble-S, we follow the same setting\nused for the full AudioSet experiment; for Ensemble-M, we in-\nclude 11 models trained with different random seeds (Table 1),\ndifferent pretrained weights (Table 3), different positional em-\nbedding interpolation (Table 4), and different patch split strate-\ngies (Table 5). The single, Ensemble-S, and Ensemble-M model\nachieve 0.347\u00b10.001, 0.363, and 0.378, respectively, all outper-\nform the previous best system. This demonstrates that AST can\nwork better than CNN-attention hybrid models even when the\ntraining set is relatively small.\n3.1.3. Ablation Study\nWe conduct a series of ablation studies to illustrate the design\nchoices for the AST. To save compute, we mainly conduct ab-\nlation studies with the balanced AudioSet. For all experiments,\nwe use weight averaging but do not use ensembles.\nImpact of ImageNet Pretraining.We compare ImageNet pre-\ntrained AST and randomly initialized AST. As shown in Ta-\nble 2, ImageNet pretrained AST noticeably outperforms ran-\ndomly initialized AST for both balanced and full AudioSet ex-\nperiments. The performance improvement of ImageNet pre-\ntraining is more signi\ufb01cant when the training data volume is\nsmaller, demonstrating that ImageNet pretraining can greatly\nreduce the demand for in-domain audio data for AST. We fur-\nther study the impact of pretrained weights used.", "mimetype": "text/plain", "start_char_idx": 3740, "end_char_idx": 5287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6f1618d-e6a6-4b55-8de9-8b66eff2f39d": {"__data__": {"id_": "c6f1618d-e6a6-4b55-8de9-8b66eff2f39d", "embedding": null, "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "197846d7-4db7-44b6-9e92-1cf00a44b087", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "3ac36b9f43b76ed93af6ba21d9b836e64544f86177d47fda066754eb48ea596a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c7e353-4d6a-4a6d-a182-3b52b822da15", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "90e1c7f0a1689550440385b825bf423865177ebdb7ae7a9d5664b075dc373784", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Ta-\nble 2, ImageNet pretrained AST noticeably outperforms ran-\ndomly initialized AST for both balanced and full AudioSet ex-\nperiments. The performance improvement of ImageNet pre-\ntraining is more signi\ufb01cant when the training data volume is\nsmaller, demonstrating that ImageNet pretraining can greatly\nreduce the demand for in-domain audio data for AST. We fur-\nther study the impact of pretrained weights used. As shown in\nTable 3, we compare the performance of AST models initialized\nwith pretrained weights of ViT-Base, ViT-Large, and DeiT mod-\nels. These models have similar architectures but are trained with\ndifferent settings. We made the necessary architecture modi\ufb01-\ncations for AST to reuse the weights. We \ufb01nd that AST using\nthe weights of the DeiT model with distillation that performs\nbest on ImageNet2012 also performs best on AudioSet.\nImpact of Positional Embedding Adaptation.As mentioned\nin Section 2.2, we use a cut and bi-linear interpolation approach\nfor positional embedding adaptation when transferring knowl-\nedge from the Vision Transformer to the AST. We compare it\nwith a pretrained AST model with a randomly initialized posi-\ntional embedding. As shown in Table 4, we \ufb01nd reinitializing\nthe positional embedding does not completely break the pre-\ntrained model as the model still performs better than a fully\nrandomly reinitialized model, but it does lead to a noticeable\nperformance drop compared with the proposed adaptation ap-\nproach. This demonstrates the importance of transferring spatial", "mimetype": "text/plain", "start_char_idx": 4863, "end_char_idx": 6399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f15dc0a-8f9a-49be-855c-7bbf52cc3223": {"__data__": {"id_": "7f15dc0a-8f9a-49be-855c-7bbf52cc3223", "embedding": null, "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "276dbcdef2db119ee0f22f28e2ee986b53fc61beac444a21239c30fde5f074fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2108054-4a74-44fc-b9fd-03f8fa286714", "node_type": "1", "metadata": {}, "hash": "feaadbae860267e43e2df1fad8f206575158edcaa622771a061c8247cc4c328b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4: Performance impact due to various positional embed-\nding adaptation settings.\nBalanced Set\nReinitialize 0.305\nNearest Neighbor Interpolation 0.346\nBilinear Interpolation (Used) 0.347\nTable 5: Performance impact due to various patch overlap size.\n# Patches Balanced Set Full Set\nNo Overlap 512 0.336 0.451\nOverlap-2 657 0.342 0.456\nOverlap-4 850 0.344 0.455\nOverlap-6 (Used) 1212 0.347 0.459\nTable 6: Performance impact due to various patch shape and\nsize. All models are trained with no patch split overlap.\n# Patches w/o Pretrain w/ Pretrain\n128\u00d72 512 0.154 -\n16\u00d716 (Used) 512 0.143 0.336\n32\u00d732 128 0.139 -\nknowledge. Bi-linear interpolation and nearest-neighbor inter-\npolation do not result in a big difference.\nImpact of Patch Split Overlap.We compare the performance\nof models trained with different patch split overlap [13]. As\nshown in Table 5, the performance improves with the overlap\nsize for both balanced and full set experiments. However, in-\ncreasing the overlap also leads to longer patch sequence inputs\nto the Transformer, which will quadratically increase the com-\nputational overhead. Even with no patch split overlap, AST can\nstill outperform the previous best system in [8].\nImpact of Patch Shape and Size. As mentioned in Sec-\ntion 2.1, we split the audio spectrogram into 16 \u00d716 square\npatches, so the input sequence to the Transformer cannot be in\ntemporal order. We hope the positional embedding can learn\nto encode the 2D spatial information. An alternative way to\nsplit the patch is slicing the audio spectrogram into rectangu-\nlar patches in the temporal order.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2108054-4a74-44fc-b9fd-03f8fa286714": {"__data__": {"id_": "d2108054-4a74-44fc-b9fd-03f8fa286714", "embedding": null, "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "276dbcdef2db119ee0f22f28e2ee986b53fc61beac444a21239c30fde5f074fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f15dc0a-8f9a-49be-855c-7bbf52cc3223", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "53d9b3fd03f8502a49b4eb0504bffcfa024c13ffd9818503399ddc98a0a2fc50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c984be96-b580-4e49-8207-c0667df3776d", "node_type": "1", "metadata": {}, "hash": "baa961e97ba4c19413f46e65ed7a590fa1dc6e4dbcaffdc55fcdb805d4131fcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Impact of Patch Shape and Size. As mentioned in Sec-\ntion 2.1, we split the audio spectrogram into 16 \u00d716 square\npatches, so the input sequence to the Transformer cannot be in\ntemporal order. We hope the positional embedding can learn\nto encode the 2D spatial information. An alternative way to\nsplit the patch is slicing the audio spectrogram into rectangu-\nlar patches in the temporal order. We compare both methods\nin Table 6, when the area of the patch is the same (256), using\n128 \u00d72 rectangle patches leads to better performance than us-\ning 16 \u00d716 square patches when both models are trained from\nscratch. However, considering there is no 128 \u00d72 patch based\nImageNet pretrained models, using 16 \u00d716 patches is still the\ncurrent optimal solution. We also compare using patches with\ndifferent sizes, smaller size patches lead to better performance.\n3.2. Results on ESC-50 and Speech Commands\nThe ESC-50 [16] dataset consists of 2,000 5-second environ-\nmental audio recordings organized into 50 classes. The cur-\nrent best results on ESC-50 are 86.5% accuracy (trained from\nscratch, SOTA-S) [33] and 94.7% accuracy (with AudioSet pre-\ntraining, SOTA-P) [7]. We compare AST with the SOTA mod-\nels in these two settings, speci\ufb01cally, we train an AST model\nwith only ImageNet pretraining (AST-S) and an AST model\nwith ImageNet and AudioSet pretraining (AST-P). We train\nboth models with frequency/time masking [29] data augmen-\ntation, a batch size of 48, and the Adam optimizer [32] for 20\nTable 7: Comparing AST and SOTA models on ESC-50 and\nSpeech Commands. \u201c-S\u201d and \u201c-P\u201d denotes model trained with-\nout and with additional audio data, respectively.", "mimetype": "text/plain", "start_char_idx": 1205, "end_char_idx": 2858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c984be96-b580-4e49-8207-c0667df3776d": {"__data__": {"id_": "c984be96-b580-4e49-8207-c0667df3776d", "embedding": null, "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "276dbcdef2db119ee0f22f28e2ee986b53fc61beac444a21239c30fde5f074fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2108054-4a74-44fc-b9fd-03f8fa286714", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "9a8d095d391f732e1b03134c7ac58cff9a7afab5d64d3fe902839cd3dbc5844b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbd307c5-db4f-48f8-99a3-47f6d2f352ce", "node_type": "1", "metadata": {}, "hash": "f32f5fa76921a8983fc99ee2eff4d5791c7d827434f0a2aaee563dc18df465f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We train\nboth models with frequency/time masking [29] data augmen-\ntation, a batch size of 48, and the Adam optimizer [32] for 20\nTable 7: Comparing AST and SOTA models on ESC-50 and\nSpeech Commands. \u201c-S\u201d and \u201c-P\u201d denotes model trained with-\nout and with additional audio data, respectively.\nESC-50 Speech Commands V2 (35 classes)\nSOTA-S 86.5 [33] 97.4 [34]\nSOTA-P 94.7 [7] 97.7 [35]\nAST-S 88.7 \u00b10.7 98.11\u00b10.05\nAST-P 95.6\u00b10.4 97.88\u00b10.03\nepochs. We use an initial learning rate of 1e-4 and 1e-5 for AST-\nS and AST-P, respectively, and decrease the learning rate with\na factor of 0.85 every epoch after the 5th epoch. We follow the\nstandard 5-fold cross-validation to evaluate our model, repeat\neach experiment three times, and report the mean and standard\ndeviation. As shown in Table 7, AST-S achieves 88.7\u00b10.7 and\nAST-P achieves 95.6 \u00b10.4, both outperform SOTA models in\nthe same setting. Of note, although ESC-50 has 1,600 training\nsamples for each fold, AST still works well with such a small\namount of data even without AudioSet pretraining.\nSpeech Commands V2 [17] is a dataset consists of 105,829\n1-second recordings of 35 common speech commands. The\ntraining, validation, and test set contains 84,843, 9,981, and\n11,005 samples, respectively.", "mimetype": "text/plain", "start_char_idx": 2567, "end_char_idx": 3816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbd307c5-db4f-48f8-99a3-47f6d2f352ce": {"__data__": {"id_": "dbd307c5-db4f-48f8-99a3-47f6d2f352ce", "embedding": null, "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "276dbcdef2db119ee0f22f28e2ee986b53fc61beac444a21239c30fde5f074fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c984be96-b580-4e49-8207-c0667df3776d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "b24b6b0485039d1a1410e50ad334222640f24908d96d6aec25175b55b0dba38d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2bd2716-baf2-452a-8b2e-e532bd37be75", "node_type": "1", "metadata": {}, "hash": "21a34f95f001d0ef7ea8625cc69afe1be7e9cd494cc5a52fa19010584624a619", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Of note, although ESC-50 has 1,600 training\nsamples for each fold, AST still works well with such a small\namount of data even without AudioSet pretraining.\nSpeech Commands V2 [17] is a dataset consists of 105,829\n1-second recordings of 35 common speech commands. The\ntraining, validation, and test set contains 84,843, 9,981, and\n11,005 samples, respectively. We focus on the 35-class clas-\nsi\ufb01cation task, the SOTA model on Speech Commands V2 (35-\nclass classi\ufb01cation) without additional audio data pretraining is\nthe time-channel separable convolutional neural network [34],\nwhich achieves 97.4% on the test set. In [35], a CNN model\npretrained with additional 200 million YouTube audio achieves\n97.7% on the test set. We also evaluate AST in these two set-\ntings. Speci\ufb01cally, we train an AST model with only ImageNet\npretraining (AST-S) and an AST model with ImageNet and\nAudioSet pretraining (AST-P). We train both models with fre-\nquency and time masking [29], random noise, and mixup [28]\naugmentation, a batch size of 128, and the Adam optimizer [32].\nWe use an initial learning rate of 2.5e-4 and decrease the learn-\ning rate with a factor of 0.85 every epoch after the 5th epoch.\nWe train the model for up to 20 epochs, and select the best\nmodel using the validation set, and report the accuracy on\nthe test set. We repeat each experiment three times and re-\nport the mean and standard deviation. AST-S model achieves\n98.11\u00b10.05, outperforms the SOTA model in [9]. In addition,\nwe \ufb01nd AudioSet pretraining unnecessary for the speech com-\nmand classi\ufb01cation task as AST-S outperforms AST-P. To sum-\nmarize, while the input audio length varies from 1 sec.", "mimetype": "text/plain", "start_char_idx": 3457, "end_char_idx": 5120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2bd2716-baf2-452a-8b2e-e532bd37be75": {"__data__": {"id_": "d2bd2716-baf2-452a-8b2e-e532bd37be75", "embedding": null, "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "276dbcdef2db119ee0f22f28e2ee986b53fc61beac444a21239c30fde5f074fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbd307c5-db4f-48f8-99a3-47f6d2f352ce", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "6cb93418db1047cf631c2fe8142425725f6fcf205513815b0cbb7d6fbfc942a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We repeat each experiment three times and re-\nport the mean and standard deviation. AST-S model achieves\n98.11\u00b10.05, outperforms the SOTA model in [9]. In addition,\nwe \ufb01nd AudioSet pretraining unnecessary for the speech com-\nmand classi\ufb01cation task as AST-S outperforms AST-P. To sum-\nmarize, while the input audio length varies from 1 sec. (Speech\nCommands), 5 sec. (ESC-50) to 10 sec. (AudioSet) and content\nvaries from speech (Speech Commands) to non-speech (Au-\ndioSet and ESC-50), we use a \ufb01xed AST architecture for all\nthree benchmarks and achieve SOTA results on all of them. This\nindicates the potential for AST use as a generic audio classi\ufb01er.\n4. Conclusions\nOver the last decade, CNNs have become a common model\ncomponent for audio classi\ufb01cation. In this work, we \ufb01nd CNNs\nare not indispensable, and introduce the Audio Spectrogram\nTransformer (AST), a convolution-free, purely attention-based\nmodel for audio classi\ufb01cation which features a simple architec-\nture and superior performance.\n5. Acknowledgements\nThis work is partly supported by Signify.", "mimetype": "text/plain", "start_char_idx": 4780, "end_char_idx": 5841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5345077-fc58-4dac-84ba-1db24974d231": {"__data__": {"id_": "e5345077-fc58-4dac-84ba-1db24974d231", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "421e01ee-f83e-4c19-baa5-d6536a975fa4", "node_type": "1", "metadata": {}, "hash": "a2c2cad020d4a1668b289cd88d18364d4f25ebf014eb1aa6f2ecaf1f5c80d8a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. References\n[1] F. Eyben, F. Weninger, F. Gross, and B. Schuller, \u201cRecent de-\nvelopments in openSMILE, the Munich open-source multimedia\nfeature extractor,\u201d inMultimedia, 2013.\n[2] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,\nF. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi,\nM. Mortillaro, H. Salamin, A. Polychroniou, F. Valente, and S. K.\nKim, \u201cThe Interspeech 2013 computational paralinguistics chal-\nlenge: Social signals, con\ufb02ict, emotion, autism,\u201d in Interspeech,\n2013.\n[3] N. Jaitly and G. Hinton, \u201cLearning a better representation of\nspeech soundwaves using restricted boltzmann machines,\u201d in\nICASSP, 2011.\n[4] S. Dieleman and B. Schrauwen, \u201cEnd-to-end learning for music\naudio,\u201d in ICASSP, 2014.\n[5] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-\nlaou, B. Schuller, and S. Zafeiriou, \u201cAdieu features? end-to-end\nspeech emotion recognition using a deep convolutional recurrent\nnetwork,\u201d inICASSP, 2016.\n[6] Y . LeCun and Y . Bengio, \u201cConvolutional networks for images,\nspeech, and time series,\u201dThe Handbook of Brain Theory and Neu-\nral Networks, vol. 3361, no. 10, p. 1995, 1995.\n[7] Q. Kong, Y . Cao, T. Iqbal, Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "421e01ee-f83e-4c19-baa5-d6536a975fa4": {"__data__": {"id_": "421e01ee-f83e-4c19-baa5-d6536a975fa4", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5345077-fc58-4dac-84ba-1db24974d231", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "4df503368f497f357481bd8cac1ee76802c63f74c3cc9a68e328c646b0c02faf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3622b161-5bff-4dbe-9aa1-e654e49c6d68", "node_type": "1", "metadata": {}, "hash": "68cc60d4f7ac32fae9d438878e0c27e8e80c55828def0af7680ff15b60970856", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] Y . LeCun and Y . Bengio, \u201cConvolutional networks for images,\nspeech, and time series,\u201dThe Handbook of Brain Theory and Neu-\nral Networks, vol. 3361, no. 10, p. 1995, 1995.\n[7] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumb-\nley, \u201cPANNs: Large-scale pretrained audio neural networks for\naudio pattern recognition,\u201dIEEE/ACM TASLP, vol. 28, pp. 2880\u2013\n2894, 2020.\n[8] Y . Gong, Y .-A. Chung, and J. Glass, \u201cPSLA: Improving audio\nevent classi\ufb01cation with pretraining, sampling, labeling, and ag-\ngregation,\u201darXiv preprint arXiv:2102.01243, 2021.\n[9] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and\nS. Laurenzo, \u201cStreaming keyword spotting on mobile devices,\u201d in\nInterspeech, 2020.\n[10] P. Li, Y . Song, I. V . McLoughlin, W. Guo, and L.-R. Dai, \u201cAn\nattention pooling based representation learning method for speech\nemotion recognition,\u201d in Interspeech, 2018.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16\nwords: Transformers for image recognition at scale,\u201d in ICLR,\n2021.", "mimetype": "text/plain", "start_char_idx": 963, "end_char_idx": 2106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3622b161-5bff-4dbe-9aa1-e654e49c6d68": {"__data__": {"id_": "3622b161-5bff-4dbe-9aa1-e654e49c6d68", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "421e01ee-f83e-4c19-baa5-d6536a975fa4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "e303494a00bb9a59c0a50c9e9e3fc1170dd048b340d1c0ccc0183a24aa75fcfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa7b669-b5e7-4f4f-9824-9950ae479e35", "node_type": "1", "metadata": {}, "hash": "995ad489da224c01f4ee6ba75920ad21225fed661deeafa675303af203865006", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J\u00b4egou, \u201cTraining data-ef\ufb01cient image transformers & distilla-\ntion through attention,\u201d arXiv preprint arXiv:2012.12877, 2020.\n[13] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, \u201cTokens-to-token ViT: Training vision transformers from\nscratch on ImageNet,\u201d arXiv preprint arXiv:2101.11986, 2021.\n[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n\u201cImageNet: A large-scale hierarchical image database,\u201d inCVPR,\n2009.\n[15] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, \u201cAudio Set: An ontology\nand human-labeled dataset for audio events,\u201d inICASSP, 2017.\n[16] K. J. Piczak, \u201cESC: Dataset for environmental sound classi\ufb01ca-\ntion,\u201d in Multimedia, 2015.\n[17] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary\nspeech recognition,\u201d arXiv preprint arXiv:1804.03209, 2018.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nin NIPS, 2017.", "mimetype": "text/plain", "start_char_idx": 2107, "end_char_idx": 3205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fa7b669-b5e7-4f4f-9824-9950ae479e35": {"__data__": {"id_": "5fa7b669-b5e7-4f4f-9824-9950ae479e35", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3622b161-5bff-4dbe-9aa1-e654e49c6d68", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "cffadaa83ff0b4f1687f56b8be855c509680d967442a1508ddfd764906f4cb02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab875183-f1f9-48c8-a7ac-6efc7e4784cd", "node_type": "1", "metadata": {}, "hash": "4c9536318cdfdabb03bd665b426462986b915e8f7df56496a2a9c1c8a7b94994", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nin NIPS, 2017.\n[19] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda,\nand K. Takeda, \u201cConvolution augmented transformer for semi-\nsupervised sound event detection,\u201d inDCASE, 2020.\n[20] Q. Kong, Y . Xu, W. Wang, and M. D. Plumbley, \u201cSound event\ndetection of weakly labelled data with CNN-transformer and au-\ntomatic threshold optimization,\u201d IEEE/ACM TASLP, vol. 28, pp.\n2450\u20132460, 2020.\n[21] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu,\nW. Han, S. Wang, Z. Zhang, Y . Wu, and R. Pang, \u201cConformer:\nConvolution-augmented transformer for speech recognition,\u201d in\nInterspeech, 2020.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,\u201d in NAACL-HLT, 2019.\n[23] G. Gwardys and D. M. Grzywczak, \u201cDeep image features in mu-\nsic information retrieval,\u201dIJET, vol. 60, no. 4, pp. 321\u2013326, 2014.\n[24] A. Guzhov, F. Raue, J. Hees, and A. Dengel, \u201cESResNet: Envi-\nronmental sound classi\ufb01cation based on visual domain models,\u201d\nin ICPR, 2020.", "mimetype": "text/plain", "start_char_idx": 3055, "end_char_idx": 4229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab875183-f1f9-48c8-a7ac-6efc7e4784cd": {"__data__": {"id_": "ab875183-f1f9-48c8-a7ac-6efc7e4784cd", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa7b669-b5e7-4f4f-9824-9950ae479e35", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "c98a7ab3b58e40401c93e47a3a7169be4c4a412caa7cfacb197176c4d4f8a301", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93a2b0eb-cb7c-4e50-b90b-824f2078aca0", "node_type": "1", "metadata": {}, "hash": "cd2c16b3017ed4ea022bf483bbe4b26c390f7a5cbf6d642c4b3607c118be086f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "60, no. 4, pp. 321\u2013326, 2014.\n[24] A. Guzhov, F. Raue, J. Hees, and A. Dengel, \u201cESResNet: Envi-\nronmental sound classi\ufb01cation based on visual domain models,\u201d\nin ICPR, 2020.\n[25] K. Palanisamy, D. Singhania, and A. Yao, \u201cRethinking CNN mod-\nels for audio classi\ufb01cation,\u201d arXiv preprint arXiv:2007.11154 ,\n2020.\n[26] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for\nimage recognition,\u201d in CVPR, 2016.\n[27] M. Tan and Q. V . Le, \u201cEf\ufb01cientNet: Rethinking model scaling for\nconvolutional neural networks,\u201d inICML, 2019.\n[28] Y . Tokozume, Y . Ushiku, and T. Harada, \u201cLearning from between-\nclass examples for deep sound recognition,\u201d inICLR, 2018.\n[29] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, \u201cSpecAugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d in Interspeech,\n2019.\n[30] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G.\nWilson, \u201cAveraging weights leads to wider optima and better gen-\neralization,\u201d in UAI, 2018.\n[31] L. Breiman, \u201cBagging predictors,\u201d Machine Learning , vol. 24,\nno. 2, pp. 123\u2013140, 1996.", "mimetype": "text/plain", "start_char_idx": 4057, "end_char_idx": 5161, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93a2b0eb-cb7c-4e50-b90b-824f2078aca0": {"__data__": {"id_": "93a2b0eb-cb7c-4e50-b90b-824f2078aca0", "embedding": null, "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7801eaca-c0be-4680-aa81-1dbe51de1953", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "7c9ccf7e77873fbacfd98a36fd8e1ff718df2ab51852d7444d6f004998707251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab875183-f1f9-48c8-a7ac-6efc7e4784cd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}, "hash": "dd07431c450a07e3c020dcd8385458b8c92b7a2f06a1f6225949d12d96675ac8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G.\nWilson, \u201cAveraging weights leads to wider optima and better gen-\neralization,\u201d in UAI, 2018.\n[31] L. Breiman, \u201cBagging predictors,\u201d Machine Learning , vol. 24,\nno. 2, pp. 123\u2013140, 1996.\n[32] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d in ICLR, 2015.\n[33] H. B. Sailor, D. M. Agrawal, and H. A. Patil, \u201cUnsupervised \ufb01lter-\nbank learning using convolutional restricted boltzmann machine\nfor environmental sound classi\ufb01cation.\u201d in Interspeech, 2017.\n[34] S. Majumdar and B. Ginsburg, \u201cMatchboxnet\u20131d time-channel\nseparable convolutional neural network architecture for speech\ncommands recognition,\u201d arXiv preprint arXiv:2004.08531, 2020.\n[35] J. Lin, K. Kilgour, D. Roblek, and M. Shari\ufb01, \u201cTraining keyword\nspotters with limited and synthesized speech data,\u201d in ICASSP,\n2020.", "mimetype": "text/plain", "start_char_idx": 4909, "end_char_idx": 5776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d1f3c3e-a6e0-4591-b81f-81c4814757d5": {"__data__": {"id_": "8d1f3c3e-a6e0-4591-b81f-81c4814757d5", "embedding": null, "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "3a2ad81d0ad5afd34810f62a6ecde7a925dc5dcb4228ae401ad6b4ced7c6396d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "559a9f55-92bc-4642-ba38-e817686679c4", "node_type": "1", "metadata": {}, "hash": "6bb97118a2d8b625c2d34e72e13db94db9c7339a1477dc9b0a1a7d07aa0047c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Detection and Classi\ufb01cation of Acoustic Scenes and Events 2020 Challenge\nCONVOLUTION-AUGMENTED TRANSFORMER\nFOR SEMI-SUPERVISED SOUND EVENT DETECTION\nTechnical Report\nKoichi Miyazaki1, Tatsuya Komatsu2, Tomoki Hayashi1,3,\nShinji Watanabe4, Tomoki Toda1, Kazuya Takeda1\n1Nagoya University, Japan, 2LINE Corporation, Japan,\n3Human Dataware Lab. Co., Ltd., Japan, 4Johns Hopkins University, USA\nmiyazaki.koichi@g.sp.m.is.nagoya-u.ac.jp,\nkomatsu.tatsuya@linecorp.com, hayashi@hdwlab.co.jp\nshinjiw@ieee.org, tomoki@icts.nagoya-u.ac.jp, kazuya.takeda@nagoya-u.jp\nABSTRACT\nIn this technical report, we describe our submission system for\nDCASE2020 Task4: sound event detection and separation in do-\nmestic environments. Our model employs conformer blocks, which\ncombine the self-attention and depth-wise convolution networks,\nto ef\ufb01ciently capture the global and local context information of\nan audio feature sequence. In addition to this novel architecture,\nwe further improve the performance by utilizing a mean teacher\nsemi-supervised learning technique, data augmentation, and post-\nprocessing optimized for each sound event class. We demonstrate\nthat the proposed method achieves the event-based macro F1 score\nof 50.7% on the validation set, signi\ufb01cantly outperforming that of\nthe baseline score (34.8%).\nIndex Terms\u2014 Sound event detection, Conformer, Trans-\nformer, semi-supervised learning\n1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "559a9f55-92bc-4642-ba38-e817686679c4": {"__data__": {"id_": "559a9f55-92bc-4642-ba38-e817686679c4", "embedding": null, "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "3a2ad81d0ad5afd34810f62a6ecde7a925dc5dcb4228ae401ad6b4ced7c6396d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d1f3c3e-a6e0-4591-b81f-81c4814757d5", "node_type": "1", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "4eb147bd11af25e050f3786e7e339749bd9e32f6332ed90b7660ea6858aa0129", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c3685b0-088c-4656-893a-ff055d16a65a", "node_type": "1", "metadata": {}, "hash": "186873683373156784be22037504c4d215e2eede83205ef643915f610e18715b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We demonstrate\nthat the proposed method achieves the event-based macro F1 score\nof 50.7% on the validation set, signi\ufb01cantly outperforming that of\nthe baseline score (34.8%).\nIndex Terms\u2014 Sound event detection, Conformer, Trans-\nformer, semi-supervised learning\n1. INTRODUCTION\nThis technical report describes our submission system for\nDCASE2020 Challenge Task4: sound event detection (SED) and\nseparation in domestic environments [1]. The goal of this task is to\nbuild a system for the detection of sound events using real data ei-\nther weakly labeled or unlabeled and simulated data that is strongly\nlabeled (with timestamps). To address this task, we propose two\nneural network models that utilize the self-attention mechanism;\n\u2022 Transformer-based model [2], [3],\n\u2022 Conformer-based model [4].\nThese models can ef\ufb01ciently capture both local and global context\ninformation of an audio feature sequence through the stack of CNN\nand self-attention layers. Besides, to further improve the perfor-\nmance, we implement\n\u2022 semi-supervised learning based on mean teacher [5],\n\u2022 data augmentation techniques, such as time-shifting [6] and\nmixup [7],\n\u2022 post-processing re\ufb01nement,\n\u2022 posterior-level score fusion.\nWe conduct experimental evaluations on the DCASE2020 Task4\nvalidation set to investigate the effectiveness of the proposed net-\nwork architecture and each of the implemented techniques. The\nexperimental results show that the proposed models signi\ufb01cantly\noutperform the baseline system, achieving the event-based macro\nF1 score of 46.0% with the best single system and that of 50.7%\nwith the score fusion.\n2. PROPOSED METHOD\n2.1.", "mimetype": "text/plain", "start_char_idx": 1127, "end_char_idx": 2758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c3685b0-088c-4656-893a-ff055d16a65a": {"__data__": {"id_": "9c3685b0-088c-4656-893a-ff055d16a65a", "embedding": null, "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "3a2ad81d0ad5afd34810f62a6ecde7a925dc5dcb4228ae401ad6b4ced7c6396d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "559a9f55-92bc-4642-ba38-e817686679c4", "node_type": "1", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "80c1b0eaf76216ce6c7e98410b883ea47ba356a9cc28ee92f256292680ec1f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "468c9229-1745-4f6d-8751-3ebae9f8e381", "node_type": "1", "metadata": {}, "hash": "0202f5d9836da3b495c47e1325fc75445434e37d24a61b1968fad5ffbc46f001", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The\nexperimental results show that the proposed models signi\ufb01cantly\noutperform the baseline system, achieving the event-based macro\nF1 score of 46.0% with the best single system and that of 50.7%\nwith the score fusion.\n2. PROPOSED METHOD\n2.1. Feature extraction\nWe extract 64-dimensional log-Mel \ufb01lterbanks from the input au-\ndio. The window size and the hop size are 1,024 points and 323\npoints, respectively, in 16 kHz sampling. We \ufb01x the length of the\nfeature sequence to 496 frames (corresponding to around 10 sec-\nonds). To make the length of feature sequences the same, we per-\nform zero-padding for shorter sequences and truncation for longer\nsequences from their last frames. Then, we perform the normaliza-\ntion to make the feature sequences have zero means and unit vari-\nances over the training data.\n2.2. Network architecture\nInspired by the great success of the self-attention architectures in\nvarious \ufb01elds [2]\u2013[4], [8], [9], we propose two neural network\nmodels for SED; Transformer-based model and Conformer-based\nmodel. The Transformer-based model consists of three modules; a\nCNN-based feature extractor, Transformer blocks, and a position-\nwise classi\ufb01er [3]. The architecture of the CNN-based feature\nextractor follows the baseline system of DCASE2020 Task4 [1],\nwhich consists of three or seven convolution layers. To match with\nthe input size, we slightly modify the third and seventh average\npooling layers\u2019 kernel size from (2, 2) to (1, 2) and from (1, 2)\nto (1, 1), respectively.", "mimetype": "text/plain", "start_char_idx": 2516, "end_char_idx": 4021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "468c9229-1745-4f6d-8751-3ebae9f8e381": {"__data__": {"id_": "468c9229-1745-4f6d-8751-3ebae9f8e381", "embedding": null, "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "3a2ad81d0ad5afd34810f62a6ecde7a925dc5dcb4228ae401ad6b4ced7c6396d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c3685b0-088c-4656-893a-ff055d16a65a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a24271abc689b6bee6e5f260f905ddd6f706ba6c26e2936fbfe2000065a0c1f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The architecture of the CNN-based feature\nextractor follows the baseline system of DCASE2020 Task4 [1],\nwhich consists of three or seven convolution layers. To match with\nthe input size, we slightly modify the third and seventh average\npooling layers\u2019 kernel size from (2, 2) to (1, 2) and from (1, 2)\nto (1, 1), respectively. The Transformer block follows the archi-\ntecture in [2], which consists of a multi-head self-attention layer, a\nlayer-normalization layer, and a linear layer with a recti\ufb01ed linear\nunit (ReLU) activation function followed by another layer normal-\nization. The \ufb01nal position-wise classi\ufb01er is a simple linear layer to\ncalculate the \ufb01nal outputs that correspond to the sound event types.\nThe Conformer-based model is the same architecture as the\nTransformer model, except that the above Transformer block is re-", "mimetype": "text/plain", "start_char_idx": 3695, "end_char_idx": 4531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f18fd33-ee73-4341-a086-5b65bb46f184": {"__data__": {"id_": "3f18fd33-ee73-4341-a086-5b65bb46f184", "embedding": null, "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a57e9998fe385b35486d557734df758792e182835b37e9ed5d9462ccff44f319", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aefdcf9f-9336-4ce3-8551-80018fcb9a85", "node_type": "1", "metadata": {}, "hash": "fd770a2910a47813b997a10e3c3cbdaa2b05caecc7560360e316bffd8a882533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Detection and Classi\ufb01cation of Acoustic Scenes and Events 2020 Challenge\nplaced with the Conformer block. The Conformer block consists of\ntwo feed-forward modules that sandwich a multi-head self-attention\nmodule and a convolution module. The feed-forward module con-\nsists of a layer-normalization layer and a linear layer with a Swish\nactivation function [10] followed by another linear layer. The \ufb01rst\nlinear layer expands the dimension four times, and the second one\nprojects back to the original input dimension. After both the lin-\near layers, we multiply 0.5 with the outputs by following the orig-\ninal Conformer\u2019s setting [4]. The convolution module consists of\na layer-normalization layer, a point-wise convolution layer with a\ngated linear unit (GLU) activation function [11], and a 1D depth-\nwise convolution layer. The depth-wise convolution layer is fol-\nlowed by a batch normalization layer, a Swish activation, and a\npoint-wise convolution layer. We set a kernel size of the depth-wise\nconvolution to seven instead of following the original setting [4] due\nto the shorter input feature sequence.\nAdditionally, we introduce a special tag token for the weakly\nlabeled training [3], making it possible to explicitly summarize the\nsequence-level information through the self-attention layers, simi-\nlar to the special classi\ufb01cation token used in BERT [12]. We attach\nthe tag token as the \ufb01rst frame of the latent feature sequence trans-\nformed through the CNN-based feature extractor. Therefore, the\nnetwork output corresponding to the \ufb01rst frame is used for the weak\nlabel prediction, while those of the other frames are used for the\nstrong label prediction. The tag token is initialized with a uniform\ndistribution and it is updated through the training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aefdcf9f-9336-4ce3-8551-80018fcb9a85": {"__data__": {"id_": "aefdcf9f-9336-4ce3-8551-80018fcb9a85", "embedding": null, "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a57e9998fe385b35486d557734df758792e182835b37e9ed5d9462ccff44f319", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f18fd33-ee73-4341-a086-5b65bb46f184", "node_type": "1", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "0231c0d1b7c9a6fc9571569912f7d33ac451f84f3fbaad982814d038714e3c8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "144e4d07-b1bc-401c-a262-a0e032f8466d", "node_type": "1", "metadata": {}, "hash": "e9ffc116e4645a3d600cad8f1924f924f667566abe5dfd92f0d150d2d9369a3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We attach\nthe tag token as the \ufb01rst frame of the latent feature sequence trans-\nformed through the CNN-based feature extractor. Therefore, the\nnetwork output corresponding to the \ufb01rst frame is used for the weak\nlabel prediction, while those of the other frames are used for the\nstrong label prediction. The tag token is initialized with a uniform\ndistribution and it is updated through the training.\n2.3. Semi-supervised learning\nTo further improve the performance, we employ the mean teacher\ntechnique [5] as one of the typical semi-supervised training meth-\nods capable of using unlabeled data in training. We use a mean\nsquare error function as the consistency criterion, and set the expo-\nnential ramp-up steps [13] and the consistency cost to 10,000 and\n2.0, respectively.\n2.4. Data augmentation\nFor data augmentation, we employ time-shifting [6] and mixup [7].\nThe time-shifting shifts a feature sequence on the time axis, and\noverrun frames are concatenated with the opposite side of the se-\nquence. We randomly choose the shift size by sampling from a\nnormal distribution with a zero mean and a standard deviation of\n90. The use of the time-shifting is helpful for preventing the net-\nwork from inappropriately learning the location information over\nthe sequence.\nThe mixup smoothes out the decision boundary by adding\npseudo data generated by mixing different data points (x1,x2) and\nthe corresponding labels (y1,y2). The mixup is formulated as fol-\nlows;\n\u00afx = \u03bbx1 + (1 \u2212\u03bb)x2, (1)\n\u00afy = \u03bby1 + (1 \u2212\u03bb)y2, (2)\nwhere \u03bb \u2208[0,1] is the mixing ratio.", "mimetype": "text/plain", "start_char_idx": 1368, "end_char_idx": 2918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "144e4d07-b1bc-401c-a262-a0e032f8466d": {"__data__": {"id_": "144e4d07-b1bc-401c-a262-a0e032f8466d", "embedding": null, "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a57e9998fe385b35486d557734df758792e182835b37e9ed5d9462ccff44f319", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aefdcf9f-9336-4ce3-8551-80018fcb9a85", "node_type": "1", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "0bb1f16d9782a5c429a0e914b773051b9ba40be8f72697cf825585ee2c1edb6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1204fafc-0e56-4ddd-b01c-38b5ca1552e7", "node_type": "1", "metadata": {}, "hash": "fe4054fcbbcb5148c9229b23860348800cf8a753b26335a9b08311fdda5a0f64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mixup smoothes out the decision boundary by adding\npseudo data generated by mixing different data points (x1,x2) and\nthe corresponding labels (y1,y2). The mixup is formulated as fol-\nlows;\n\u00afx = \u03bbx1 + (1 \u2212\u03bb)x2, (1)\n\u00afy = \u03bby1 + (1 \u2212\u03bb)y2, (2)\nwhere \u03bb \u2208[0,1] is the mixing ratio. In this study, we randomly\nchoose this value by sampling from a beta distribution with \u03b1 =\n0.2.\n2.5. Post-processing\nTo determine the sound event activation, we perform thresholding\nfor the network output posterior. Then, we perform median \ufb01lter-\ning as post-processing to smooth the detected activation sequence.\nSince each sound event has different characteristics, such as tem-\nporal structures, the optimal post-processing parameters depend on\nthe individual sound events. Hence, we determine the optimal post-\nprocessing parameters for each sound event using the validation set.\nWe search the optimal threshold and median \ufb01lter size from 0.1 to\n0.9 in increments of 0.1, and from 1 to 31 in increments of 2, re-\nspectively.\n2.6. Score fusion\nTo improve generalization performance, we perform score fusion as\na model ensemble technique. We \ufb01rst average the raw posterior out-\nputs of the multiple models with different training settings. Then,\nwe perform thresholding and apply post-processing, as mentioned\nin Section 2.5.\n3. EXPERIMENTAL EV ALUATION\n3.1. Experimental conditions\nWe conducted experimental evaluations using DCASE2020 Task4\ndataset [1].", "mimetype": "text/plain", "start_char_idx": 2640, "end_char_idx": 4076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1204fafc-0e56-4ddd-b01c-38b5ca1552e7": {"__data__": {"id_": "1204fafc-0e56-4ddd-b01c-38b5ca1552e7", "embedding": null, "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a57e9998fe385b35486d557734df758792e182835b37e9ed5d9462ccff44f319", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "144e4d07-b1bc-401c-a262-a0e032f8466d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "79d4ee186f4aa07bf980b7790bcdc3e8cdcd1eff4e2de49316cca57e853d7f43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faaf2b13-e4ca-40d2-bcfd-2a20b5b8a7c9", "node_type": "1", "metadata": {}, "hash": "05a2856a5ecfcb41ee71ca86263ba4d175ba5b517fa34abb670576a0f232f87e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Score fusion\nTo improve generalization performance, we perform score fusion as\na model ensemble technique. We \ufb01rst average the raw posterior out-\nputs of the multiple models with different training settings. Then,\nwe perform thresholding and apply post-processing, as mentioned\nin Section 2.5.\n3. EXPERIMENTAL EV ALUATION\n3.1. Experimental conditions\nWe conducted experimental evaluations using DCASE2020 Task4\ndataset [1]. The dataset included 2,584 audio clips with a strong\nlabel, 1578 audio clips with a weak label, and 14,412 unlabeled\naudio clips. Since the audio clips were collected from YouTube, the\ndataset included various audio clips with different recording settings\n(e.g., 16 kHz sampling rate vs. 44.1 kHz sampling rate). To address\nthis issue, we \ufb01rst converted all of the audio clips to be 1 ch, 16 bit,\nand 16 kHz sampling rate using sox [14]. We then normalized each\naudio clip to be -3 dBFS and removed a direct current component\nby applying a 10 Hz high-pass \ufb01lter.\nTo verify the performance, we compared the following models:\nBaseline: The DCASE2020 Task4 of\ufb01cial baseline system [15].\nThe architecture was a convolutional recurrent neural net-\nwork (CRNN), and it was trained with the mean-teacher\nsemi-supervised learning technique [5]. We used the num-\nbers provided in the of\ufb01cial HP.\nTransformer (Ours): The proposed Transformer-based model.\nThe number of attention units and that of the attention heads\nwere 512 and 16, respectively. The dropout rate was set to\n0.1.\nConformer (Ours): The proposed Conformer-based model.", "mimetype": "text/plain", "start_char_idx": 3653, "end_char_idx": 5201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faaf2b13-e4ca-40d2-bcfd-2a20b5b8a7c9": {"__data__": {"id_": "faaf2b13-e4ca-40d2-bcfd-2a20b5b8a7c9", "embedding": null, "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "a57e9998fe385b35486d557734df758792e182835b37e9ed5d9462ccff44f319", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1204fafc-0e56-4ddd-b01c-38b5ca1552e7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "958d50eb213b93dbd607c638c3de6f96f7012353b39c184ce8a6427b199c968b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We used the num-\nbers provided in the of\ufb01cial HP.\nTransformer (Ours): The proposed Transformer-based model.\nThe number of attention units and that of the attention heads\nwere 512 and 16, respectively. The dropout rate was set to\n0.1.\nConformer (Ours): The proposed Conformer-based model. The\nnumber of attention units and that of the attention heads were\n144 and 4, respectively. The kernel size of the depth-wise\nconvolution was 7. The dropout rate was set to 0.1. The\nnumber of Conformer blocks was varied from 2 to 10.\nWe used RAdam [16] optimizer with a batch size of 128 and a learn-\ning rate of 0.001. We multiplied the learning rate by 0.1 every 10000\niteration and continued to train until 30,000 iterations. We used a\nGPU (NVIDIA Titan pascal) to train the models. It took around 12\nhours to \ufb01nish the training. The detailed training condition is shown\nin Table 1.\nThe evaluation metrics were the event-based macro F1 score\n(EB-F1), the segment-based macro F1 score (SB-F1), and the poly-\nphonic sound event detection score (PSDS) [17]. These metrics\nwere calculated using sed eval toolkit [18]. The segment length in\nthe segment-based evaluation was set to 1 second. The event-based\nmetrics were calculated using both the onset and offset of the detec-\ntion. The allowable length of detection errors was set to 200 ms for\nthe onsets and 200 ms / 20 % of the event length for the offsets. We\ncomputed PSDS using 50 thresholds from 0.01 to 0.99.", "mimetype": "text/plain", "start_char_idx": 4914, "end_char_idx": 6367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "289b1b8d-bb5c-4939-b900-f23057836e1b": {"__data__": {"id_": "289b1b8d-bb5c-4939-b900-f23057836e1b", "embedding": null, "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "9e27e65be545e495ce8fbd3da793d921402ff202378fc6605556c2301c58d4c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1", "node_type": "1", "metadata": {}, "hash": "29bbc98cbbb47875617b261495acd8d9dc44635ed44aa969977c8a423de3c1c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Detection and Classi\ufb01cation of Acoustic Scenes and Events 2020 Challenge\nTable 1: Network training con\ufb01guration.\n# of strong = 2,584\n# of training samples # of weak = 1,578\n# of unlabeled = 14,412\nBatch size 128\n# of iterations 30,000\nOptimizer RAdam [16]\nLearning rate 0.001\nConsistency cost 2.0\nRampup steps 10,000\nTable 2: Effects of model architectures.\nMethod EB-F1[%] SB-F1 [%]\nBaseline 35.6 -\nTransformer 41.0 69.3\nConformer 41.7 67.2\n3.2. Experimental Results\n3.2.1. Effects of model architecture\nFirst, we investigated the effects of the model architecture. In a\ncomparison of the model architectures, we used the post-processing\nand the mean teacher learning but we did not use the data augmenta-\ntion for the proposed method. From the result shown in Table 2, we\ncan observe that both the proposed models outperform the baseline\neven if we do not use data augmentation, revealing the effectiveness\nof the self-attention architecture for SED.\nNext, we investigated the effects of the number of Conformer\nblocks. The result in Table 3 shows that the number of blocks affects\nthe performance and 4 was the best. We used this con\ufb01guration in\nthe following experiments.\n3.2.2. Effects of post-processing\nNext, we investigated the effects of the post-processing. For the\nmodel without post-processing, we \ufb01xed the threshold to 0.5 for all\nsound event classes and did not apply the median \ufb01lter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1": {"__data__": {"id_": "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1", "embedding": null, "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "9e27e65be545e495ce8fbd3da793d921402ff202378fc6605556c2301c58d4c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "289b1b8d-bb5c-4939-b900-f23057836e1b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "5429fd5cf9d0b3c23713934813fdf55741fdb81ffbd5c96159b519d13f6d5ef0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cf6a737-620f-4c2b-9f6a-54606f2e766c", "node_type": "1", "metadata": {}, "hash": "04b94ec411729928c41fd9051bab2153c4a3741fd169ff81f1ec48f62bb97aeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The result in Table 3 shows that the number of blocks affects\nthe performance and 4 was the best. We used this con\ufb01guration in\nthe following experiments.\n3.2.2. Effects of post-processing\nNext, we investigated the effects of the post-processing. For the\nmodel without post-processing, we \ufb01xed the threshold to 0.5 for all\nsound event classes and did not apply the median \ufb01lter. Table 4\nshows the result with or without post-processing. From the result,\nwe can con\ufb01rm that the post-processing improves the performance\nespecially on the event-based macro F1 score. This is because the\nevent-based metric is more sensitive to the deletion or insertion er-\nrors than the segment-based metric.\n3.2.3. Effects of data augmentation\nNext, we investigated the effects of the data augmentation. Table 5\nshows the result of the Conformer-based model with or without each\ndata augmentation method. The result shows that both time-shifting\nand mixup improve the performance and that a combination of these\ntwo methods yields further performance improvement.\nTable 3: Effects of the number of Conformer blocks.\n# of blocks EB-F1[%] SB-F1 [%]\n2 39.39 65.91\n3 40.29 67.03\n4 41.30 67.17\n5 38.47 66.18\n6 39.44 67.14\n7 39.44 66.29\n8 38.18 64.96\n9 37.82 65.90\n10 36.49 65.10\nTable 4: Effects of post-processing (p.p.).\nMethod EB-F1[%] SB-F1 [%]\nTransformer w/o p.p.", "mimetype": "text/plain", "start_char_idx": 1022, "end_char_idx": 2367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cf6a737-620f-4c2b-9f6a-54606f2e766c": {"__data__": {"id_": "4cf6a737-620f-4c2b-9f6a-54606f2e766c", "embedding": null, "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "9e27e65be545e495ce8fbd3da793d921402ff202378fc6605556c2301c58d4c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1", "node_type": "1", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "57df12828b9d203cfc3f823649690326cb2b8e3ebd41796d8f1ce6f9b4ca9882", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f877bc0-942a-4367-8f6d-1d17c9e5cfb5", "node_type": "1", "metadata": {}, "hash": "2176ae473062e0841f98aa3d9506eca797dd3915b9613dc0fda07f404bfc4720", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Method EB-F1[%] SB-F1 [%]\nTransformer w/o p.p. 28.6 64.4\nTransformer w/ p.p. 41.0 69.3\nConformer w/o p.p. 34.4 64.5\nConformer w/ p.p. 41.7 67.2\nTable 5: Effects of data augmentation.\nMethod EB-F1 [%] PSDS\nw/o data augmentation 41.7 0.593\nw/ time-shifting 41.4 0.616\nw/ mixup 42.5 0.620\nw/ time-shifting & mixup 46.0 0.643\n3.2.4. Effects of score fusion\nFinally, we investigated the effects of the score fusion. We com-\npared the following three models;\nConformer fusion: Score fusion with the top 8 Conformer mod-\nels, where each model was trained with different hyperpa-\nrameter settings.\nTransformer fusion: Score fusion with the top 7 Transformer\nmodels, where each model was trained with of different hy-\nperparameter settings.\nConformer & Transformer fusion: Score fusion with the top 8\nConformer models and the top 7 Transformer models.\nThe score-fusion result is shown in Table 6 From the result, the\nscore fusion yields further performance improvements and signi\ufb01-\ncantly outperforms the baseline. The best model achieves the event-\nbased F1 score of 50.6 and the PSPD of 0.700.\n4. CONCLUSION\nIn this technical report, we have described our submission system\nfor DCASE2020 Task4.", "mimetype": "text/plain", "start_char_idx": 2321, "end_char_idx": 3508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f877bc0-942a-4367-8f6d-1d17c9e5cfb5": {"__data__": {"id_": "8f877bc0-942a-4367-8f6d-1d17c9e5cfb5", "embedding": null, "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "9e27e65be545e495ce8fbd3da793d921402ff202378fc6605556c2301c58d4c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cf6a737-620f-4c2b-9f6a-54606f2e766c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "da8ca41e66e8bad6dca9d9419b543f7e63ecb21960f26a97f96e9dd06b0c55aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score-fusion result is shown in Table 6 From the result, the\nscore fusion yields further performance improvements and signi\ufb01-\ncantly outperforms the baseline. The best model achieves the event-\nbased F1 score of 50.6 and the PSPD of 0.700.\n4. CONCLUSION\nIn this technical report, we have described our submission system\nfor DCASE2020 Task4. Our system has been developed by us-\ning the self-attention architecture including the Transformer and\nthe Conformer blocks, the data augmentation techniques, the class-\ndependent post-processing, and the score fusion. The experimental", "mimetype": "text/plain", "start_char_idx": 3164, "end_char_idx": 3744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "336fae26-fc57-4f09-9c89-1dd00909e1bc": {"__data__": {"id_": "336fae26-fc57-4f09-9c89-1dd00909e1bc", "embedding": null, "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "03c231fe47febe044c8aa7bf46a693811aa07f5eda34f6dd829c944064d1460f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2808721c-1d34-40d9-b8bc-9c4d98524c7c", "node_type": "1", "metadata": {}, "hash": "a2737c41c5f70c4af7aeffb82e4e96d03c05efb65ce481764be3512e51118e16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Detection and Classi\ufb01cation of Acoustic Scenes and Events 2020 Challenge\nTable 6: Effects of score fusion.\nMethod EB-F1 [%] PSDS\nBaseline 35.6 0.626\nConformer fusion 50.6 0.700\nTransformer fusion 47.3 0.643\nConformer & Transformer fusion 49.8 0.626\nresults using the validation set have demonstrated that these tech-\nniques are helpful for improving the sound event detection perfor-\nmance, and our system signi\ufb01cantly outperforms the baseline. In\nfuture work, we will investigate the class-wise performance more\ncarefully to develop more effective model ensemble technique, and\nfurther integrate source separation techniques to sound event detec-\ntion.\n5. REFERENCES\n[1] http://dcase.community/challenge2020/.\n[2] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all\nyou need,\u201d inNIPS, 2017, pp. 5998\u20136008.\n[3] M. Koichi, K. Tatsuya, H. Tomoki, et al. , \u201cWeakly-\nsupervised sound event detection with self-attention,\u201d in\nICASSP, 2020, pp. 66\u201370.\n[4] A. Gulati, J. Qin, C.-C. Chiu, et al. , \u201cConformer:\nConvolution-augmented transformer for speech recogni-\ntion,\u201darXiv preprint arXiv:2005.08100, 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2808721c-1d34-40d9-b8bc-9c4d98524c7c": {"__data__": {"id_": "2808721c-1d34-40d9-b8bc-9c4d98524c7c", "embedding": null, "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "03c231fe47febe044c8aa7bf46a693811aa07f5eda34f6dd829c944064d1460f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "336fae26-fc57-4f09-9c89-1dd00909e1bc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "ff016407e4350fee2ea1cc03ce93cde954e6825b1dd1026e7a4466dbbd4243b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc0bddbe-995a-44cf-a692-7544bf625727", "node_type": "1", "metadata": {}, "hash": "3d9a9417de5099a6555a7cf58865b197217a38b36eca417e3f502078803dd7bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", \u201cWeakly-\nsupervised sound event detection with self-attention,\u201d in\nICASSP, 2020, pp. 66\u201370.\n[4] A. Gulati, J. Qin, C.-C. Chiu, et al. , \u201cConformer:\nConvolution-augmented transformer for speech recogni-\ntion,\u201darXiv preprint arXiv:2005.08100, 2020.\n[5] A. Tarvainen and H. Valpola, \u201cMean teachers are better role\nmodels: Weight-averaged consistency targets improve semi-\nsupervised deep learning results,\u201d in NIPS, 2017, pp. 1195\u2013\n1204.\n[6] L. Delphin-Poulat and C. Plapous, \u201cMean teacher with data\naugmentation for dcase 2019 task 4,\u201d Orange Labs Lannion,\nFrance, Tech. Rep., 2019.\n[7] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-\nPaz, \u201cMixup: Beyond empirical risk minimization,\u201d arXiv\npreprint arXiv:1710.09412, 2017.\n[8] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study\non transformer vs RNN in speech applications,\u201d in Proc.\nASRU, 2019, pp. 449\u2013456.\n[9] Y . Fujita, N. Kanda, S. Horiguchi, et al., \u201cEnd-to-end neu-\nral speaker diarization with self-attention,\u201d in Proc. ASRU,\n2019, pp. 296\u2013303.\n[10] P. Ramachandran, B. Zoph, and Q. V .", "mimetype": "text/plain", "start_char_idx": 861, "end_char_idx": 1919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc0bddbe-995a-44cf-a692-7544bf625727": {"__data__": {"id_": "cc0bddbe-995a-44cf-a692-7544bf625727", "embedding": null, "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "03c231fe47febe044c8aa7bf46a693811aa07f5eda34f6dd829c944064d1460f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2808721c-1d34-40d9-b8bc-9c4d98524c7c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "3ecfc0c02ae839313217c2e420d1c257ed1844d0c2545a33e703a97968fc1403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94769752-8058-40db-a40a-ce16cc007f73", "node_type": "1", "metadata": {}, "hash": "08b115fd5e3f5e32be54dcca843ea3e31632d2acf31e37564ad4a63aafdcc959", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ASRU, 2019, pp. 449\u2013456.\n[9] Y . Fujita, N. Kanda, S. Horiguchi, et al., \u201cEnd-to-end neu-\nral speaker diarization with self-attention,\u201d in Proc. ASRU,\n2019, pp. 296\u2013303.\n[10] P. Ramachandran, B. Zoph, and Q. V . Le, \u201cSearching for ac-\ntivation functions,\u201darXiv preprint arXiv:1710.05941, 2017.\n[11] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage\nmodeling with gated convolutional networks,\u201d in ICML,\n2017, pp. 933\u2013941.\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,\u201d inNAACL, 2019, pp. 4171\u20134186.\n[13] S. Laine and T. Aila, \u201cTemporal ensembling for semi-\nsupervised learning,\u201d arXiv preprint arXiv:1610.02242 ,\n2016.\n[14] http://sox.sourceforge.net/.\n[15] https : / / github . com / turpaultn / dcase20 _\ntask4/tree/public_branch/baseline.\n[16] L. Liu, H. Jiang, P. He, et al., \u201cOn the variance of the adaptive\nlearning rate and beyond,\u201d inICLR, 2020.", "mimetype": "text/plain", "start_char_idx": 1708, "end_char_idx": 2663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94769752-8058-40db-a40a-ce16cc007f73": {"__data__": {"id_": "94769752-8058-40db-a40a-ce16cc007f73", "embedding": null, "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "03c231fe47febe044c8aa7bf46a693811aa07f5eda34f6dd829c944064d1460f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc0bddbe-995a-44cf-a692-7544bf625727", "node_type": "1", "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}, "hash": "6ff3a2b36403fd927d8e4d3caa9ec261ca7a9271523f74ff09657a6d9dc4767e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] http://sox.sourceforge.net/.\n[15] https : / / github . com / turpaultn / dcase20 _\ntask4/tree/public_branch/baseline.\n[16] L. Liu, H. Jiang, P. He, et al., \u201cOn the variance of the adaptive\nlearning rate and beyond,\u201d inICLR, 2020.\n[17] C. Bilen, G. Ferroni, F. Tuveri, J. Azcarreta, and S.\nKrstulovic, \u201cA framework for the robust evaluation of sound\nevent detection,\u201darXiv preprint arXiv:1910.08440, 2019.\n[18] A. Mesaros, T. Heittola, and T. Virtanen, \u201cMetrics for poly-\nphonic sound event detection,\u201d Applied Sciences , vol. 6,\nno. 6, p. 162, 2016.", "mimetype": "text/plain", "start_char_idx": 2429, "end_char_idx": 2983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecc2d542-15ad-42b7-a487-900c176a3fa6": {"__data__": {"id_": "ecc2d542-15ad-42b7-a487-900c176a3fa6", "embedding": null, "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83c18eab-5164-4adf-aca1-61f6095f0c62", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "5ffa046c57c1eea67f2be57547b1b13809b1deaa7dd7abe2c2986707667a6187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27c3b9d3-fc72-4590-aee2-949877aa7fd6", "node_type": "1", "metadata": {}, "hash": "ac51aeaaa261d9365a4feaec6070c688ec72c87e03ed22d644c6bb5047c9a967", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs). Transformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother. We also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27c3b9d3-fc72-4590-aee2-949877aa7fd6": {"__data__": {"id_": "27c3b9d3-fc72-4590-aee2-949877aa7fd6", "embedding": null, "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83c18eab-5164-4adf-aca1-61f6095f0c62", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "5ffa046c57c1eea67f2be57547b1b13809b1deaa7dd7abe2c2986707667a6187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecc2d542-15ad-42b7-a487-900c176a3fa6", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b8a3069c789ec8c8f2add14a5499f1fda0b041600452d06d6c26f0d92894b32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2253a3e4-3550-4eb3-835a-3d6fdde8c56c", "node_type": "1", "metadata": {}, "hash": "7f5ee2703dbfaa843e888c68c89bbea9152d73da479767c5e6a3af246600e3fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns. Convolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation.", "mimetype": "text/plain", "start_char_idx": 1276, "end_char_idx": 2805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2253a3e4-3550-4eb3-835a-3d6fdde8c56c": {"__data__": {"id_": "2253a3e4-3550-4eb3-835a-3d6fdde8c56c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83c18eab-5164-4adf-aca1-61f6095f0c62", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "5ffa046c57c1eea67f2be57547b1b13809b1deaa7dd7abe2c2986707667a6187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27c3b9d3-fc72-4590-aee2-949877aa7fd6", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "ae9fe0843291e750e611cc168e839f83ceebb72b8a7ace11b3a5607b6e02f59a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35b7f3e9-ec82-4ebc-940f-cf924406685b", "node_type": "1", "metadata": {}, "hash": "bcf92ec472c00acbc6febcd3cf344dcd392802a80120e243dd04e2ee2d323ab2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+ \n+ \n+ \n+ \nLayernorm\nFigure 1: Conformer encoder model architecture.Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.", "mimetype": "text/plain", "start_char_idx": 2384, "end_char_idx": 3963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35b7f3e9-ec82-4ebc-940f-cf924406685b": {"__data__": {"id_": "35b7f3e9-ec82-4ebc-940f-cf924406685b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83c18eab-5164-4adf-aca1-61f6095f0c62", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "5ffa046c57c1eea67f2be57547b1b13809b1deaa7dd7abe2c2986707667a6187", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2253a3e4-3550-4eb3-835a-3d6fdde8c56c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "533156965f4251d0aa9f7a718d440e7b869b4620275118b520332b78b0666282", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020", "mimetype": "text/plain", "start_char_idx": 3607, "end_char_idx": 5072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4655479-c6ba-4dec-b851-071409dc19b1": {"__data__": {"id_": "d4655479-c6ba-4dec-b851-071409dc19b1", "embedding": null, "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a53155-9852-451a-bb15-24cf4011bc7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "0a85c12f4d7af6066ba838a7ac5bb0f02d954907b3017b256136a282e089923d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa59ac24-3d7a-4655-93c8-c8572df02294", "node_type": "1", "metadata": {}, "hash": "75b5ed55448fd8526f512378893f4b7a2c186f11487de21148ed65d466b4dbf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Layernorm Glu \nActivation \nPointwise \nConv BatchNorm Swish \nActivation \n1D \nDepthwise \nConv \nPointwise \nConv Dropout + \nFigure 2: Convolution module.The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa59ac24-3d7a-4655-93c8-c8572df02294": {"__data__": {"id_": "aa59ac24-3d7a-4655-93c8-c8572df02294", "embedding": null, "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a53155-9852-451a-bb15-24cf4011bc7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "0a85c12f4d7af6066ba838a7ac5bb0f02d954907b3017b256136a282e089923d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4655479-c6ba-4dec-b851-071409dc19b1", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "16f6baa0730654ee9a6780ad026ff6b3728d15dff9a734e743bf4b44f4957e28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efd60397-3e10-40e5-aa1f-8742400f0663", "node_type": "1", "metadata": {}, "hash": "a35940ccc442bb2e859c8b902d87e9a993372a828b43b940658e0e610bac95d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout + \nFigure 3: Multi-Headed self-attention module.We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2.", "mimetype": "text/plain", "start_char_idx": 1305, "end_char_idx": 2752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efd60397-3e10-40e5-aa1f-8742400f0663": {"__data__": {"id_": "efd60397-3e10-40e5-aa1f-8742400f0663", "embedding": null, "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a53155-9852-451a-bb15-24cf4011bc7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "0a85c12f4d7af6066ba838a7ac5bb0f02d954907b3017b256136a282e089923d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa59ac24-3d7a-4655-93c8-c8572df02294", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "da384740e606e88c418bd7d715151fa770acea4f00a613829f1734654b701a76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4519b263-4996-4fc9-ad1d-f08c81f36040", "node_type": "1", "metadata": {}, "hash": "f765117fb61fe7acb1d70f6eb37c81efae82080f77d21889adb5ebc972b1db1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout + \nFigure 3: Multi-Headed self-attention module.We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.", "mimetype": "text/plain", "start_char_idx": 2353, "end_char_idx": 3931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4519b263-4996-4fc9-ad1d-f08c81f36040": {"__data__": {"id_": "4519b263-4996-4fc9-ad1d-f08c81f36040", "embedding": null, "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a53155-9852-451a-bb15-24cf4011bc7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "0a85c12f4d7af6066ba838a7ac5bb0f02d954907b3017b256136a282e089923d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efd60397-3e10-40e5-aa1f-8742400f0663", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "9375d6bb5a805e2471d279b823ea61ae10a29eeb08aacd2c36da5c65a6940f9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d6e3e63-f4a8-43bc-9ede-509ba6c18c5b", "node_type": "1", "metadata": {}, "hash": "5af8a92a689a72639c998de72e4d919a342963c3f7c56a12732bf08b861dfb9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works.", "mimetype": "text/plain", "start_char_idx": 3606, "end_char_idx": 4839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d6e3e63-f4a8-43bc-9ede-509ba6c18c5b": {"__data__": {"id_": "2d6e3e63-f4a8-43bc-9ede-509ba6c18c5b", "embedding": null, "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6a53155-9852-451a-bb15-24cf4011bc7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "0a85c12f4d7af6066ba838a7ac5bb0f02d954907b3017b256136a282e089923d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4519b263-4996-4fc9-ad1d-f08c81f36040", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "7fb7d2cfed8dd5a8d3fee1ce4bede9583398cf27968280ddc4473f66e630bfd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve", "mimetype": "text/plain", "start_char_idx": 4708, "end_char_idx": 5223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16d83b02-71da-4ed6-9dc9-74a9ec2c40cc": {"__data__": {"id_": "16d83b02-71da-4ed6-9dc9-74a9ec2c40cc", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65d018d0-fa5d-4753-84a5-780cb3c538d2", "node_type": "1", "metadata": {}, "hash": "8030e69a25384c451cd6cfe825ee0c259ec564680b5d428c3f58151dbf47df73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Layernorm Linear \nLayer Dropout Linear \nLayer \nSwish \nActivation Dropout + \nFigure 4: Feed forward module.The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set topS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65d018d0-fa5d-4753-84a5-780cb3c538d2": {"__data__": {"id_": "65d018d0-fa5d-4753-84a5-780cb3c538d2", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16d83b02-71da-4ed6-9dc9-74a9ec2c40cc", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "94197c4bd199710abe209fa6b77c58f5ee84fcde8380c820347fe25823af3bf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6c6b166-d86e-43ba-b91d-6a9fe8532091", "node_type": "1", "metadata": {}, "hash": "c958ec46d2da47609c80e00e26679d4a5f7ee224ff4fbf2324d1c3049a871c1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e\u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].", "mimetype": "text/plain", "start_char_idx": 1027, "end_char_idx": 2435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6c6b166-d86e-43ba-b91d-6a9fe8532091": {"__data__": {"id_": "f6c6b166-d86e-43ba-b91d-6a9fe8532091", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65d018d0-fa5d-4753-84a5-780cb3c538d2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "bbd995051c5503451c8e942ee94593d75ec9d982a7b5cd39f46f18104a977c7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a80d112-7b87-4483-9783-b6f9ccb97aa9", "node_type": "1", "metadata": {}, "hash": "2f77d4f2a1fcdb1c1952f2104a9e0aeaa700c874de5f0cc934af61a952a1e418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model, the performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel Conformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M) 10.3 30.7 118.8\nEncoder Layers 16 16 17\nEncoder Dim 144 256 512\nAttention Heads 4 4 8\nConv Kernel Size 32 32 32\nDecoder Layers 1 1 1\nDecoder Dim 320 640 640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints.", "mimetype": "text/plain", "start_char_idx": 2241, "end_char_idx": 3571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a80d112-7b87-4483-9783-b6f9ccb97aa9": {"__data__": {"id_": "3a80d112-7b87-4483-9783-b6f9ccb97aa9", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6c6b166-d86e-43ba-b91d-6a9fe8532091", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1be7bf7e1eea37347364d189368390772664169d9383ac42f5ea70b832c602b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "694162d9-2657-4d4d-b452-537cee00910b", "node_type": "1", "metadata": {}, "hash": "40bc3f6c52cccd772060ed53dec9dfd21672f654cc05f0a7d90d201dd4c5aade", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.", "mimetype": "text/plain", "start_char_idx": 3483, "end_char_idx": 3857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "694162d9-2657-4d4d-b452-537cee00910b": {"__data__": {"id_": "694162d9-2657-4d4d-b452-537cee00910b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a80d112-7b87-4483-9783-b6f9ccb97aa9", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "ebe3c6b3ee805010365932d39388622de764b9f5e5050afbc4cbcbcfa267f5fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28a85989-adde-41dc-a6f6-f56b5cec086b", "node_type": "1", "metadata": {}, "hash": "39b9747621a2cafcd3d8c136ff1370a1e7e779764377fc18dafb834930b9b6cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.\nMethod #Params (M) WER Without LM WER With LM\ntestclean testother testclean testother\nHybrid\nTransformer [33] - - - 2.26 4.85\nCTC\nQuartzNet [9] 19 3.90 11.28 2.69 7.25\nLAS\nTransformer [34] 270 2.89 6.98 2.33 5.17\nTransformer [19] - 2.2 5.6 2.6 5.7\nLSTM 360 2.6 6.0 2.2 5.2\nTransducer\nTransformer [7] 139 2.4 5.6 2.0 4.6\nContextNet(S) [10] 10.8 2.9 7.0 2.3 5.5\nContextNet(M) [10] 31.4 2.4 5.4 2.0 4.5\nContextNet(L) [10] 112.7 2.1 4.6 1.9 4.1\nConformer (Ours)\nConformer(S) 10.3 2.7 6.3 2.1 5.0\nConformer(M) 30.7 2.3 5.0 2.0 4.3\nConformer(L) 118.8 2.1 4.3 1.9 3.9\nerror rate among all the existing models.", "mimetype": "text/plain", "start_char_idx": 3483, "end_char_idx": 4460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28a85989-adde-41dc-a6f6-f56b5cec086b": {"__data__": {"id_": "28a85989-adde-41dc-a6f6-f56b5cec086b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89551c77-5741-478a-9a15-735f30a541fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "96b3182ffca8d20dc47cfaebcd9a117b727c29fce0663df52488ed55ceb0849f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "694162d9-2657-4d4d-b452-537cee00910b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "a303ae65498e21f0cb484b2643568f19a6737be026643dd1c62a88bde1f9dace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most", "mimetype": "text/plain", "start_char_idx": 4461, "end_char_idx": 5126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c75251b-8254-4b05-9c45-e0be7f499e3f": {"__data__": {"id_": "7c75251b-8254-4b05-9c45-e0be7f499e3f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b2d488d8ca3a08361451657ff7e90bee8311fd2dd67237cc672031b9e23a42e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c", "node_type": "1", "metadata": {}, "hash": "712f9cb9e5ec09ae66d045144b4d4e71c6f13c1d1469d564a2ac10cbcff3cfe2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "important feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer.Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study results are evaluated without the\nexternal LM.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer Model 1.9 4.4 2.1 4.3\n\u2013 SWISH + ReLU 1.9 4.4 2.0 4.5\n\u2013 Convolution Block 2.1 4.8 2.1 4.9\n\u2013 Macaron FFN 2.1 5.1 2.1 5.0\n\u2013 Relative Pos. Emb. 2.3 5.8 2.4 5.6\n3.4.2. Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signi\ufb01cant drop in the performance especially on the dev-\nother dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c": {"__data__": {"id_": "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c", "embedding": null, "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b2d488d8ca3a08361451657ff7e90bee8311fd2dd67237cc672031b9e23a42e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c75251b-8254-4b05-9c45-e0be7f499e3f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "21c93d8bedf887f9be16e3bc04739b64cf06e45df0bb5da57d2b09814dff9301", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46a8e9be-2a77-47fd-8bea-66b1e482ac65", "node_type": "1", "metadata": {}, "hash": "88e556af963f43802eb707d5b7ae6eaf82e109dc48a886fc993f5e1d3ffb6b72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signi\ufb01cant drop in the performance especially on the dev-\nother dataset. Second, we study placing the convolution mod-\nule before the MHSA module in our Conformer model and \ufb01nd\nthat it degrades the results by 0.1 on dev-other. Another pos-\nsible way of the architecture is to split the input into parallel\nbranches of multi-headed self attention module and a convolu-\ntion module with their output concatenated as suggested in [17].\nWe found that this worsens the performance when compared to\nour proposed architecture.\nThese results in Table 4 suggest the advantage of placing\nthe convolution module after the self-attention module in the\nConformer block.\nTable 4: Ablation study of Conformer Attention Convolution\nBlocks. Varying the combination of the convolution block with\nthe multi-headed self attention: (1) Conformer architecture; (2)\nUsing Lightweight convolutions instead of depthwise convolu-\ntion in the convolution block in Conformer; (3) Convolution be-\nfore multi-headed self attention; (4) Convolution and MHSA in\nparallel with their output concatenated [17].\nModel Architecture dev\nclean\ndev\nother\nConformer 1.9 4.4\n\u2013 Depthwise conv + Lightweight convolution 2.0 4.8\nConvolution block before MHSA 1.9 4.5\nParallel MHSA and Convolution 2.0 4.9\n3.4.3.", "mimetype": "text/plain", "start_char_idx": 888, "end_char_idx": 2462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46a8e9be-2a77-47fd-8bea-66b1e482ac65": {"__data__": {"id_": "46a8e9be-2a77-47fd-8bea-66b1e482ac65", "embedding": null, "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b2d488d8ca3a08361451657ff7e90bee8311fd2dd67237cc672031b9e23a42e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "7fec5c5335d0243a0284ce87b73df4ba17ee5ccb96c8352b3ca0733fbb885344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3159b1cb-85a0-4df1-8173-f631dc6c973b", "node_type": "1", "metadata": {}, "hash": "8471cdb4ef8f6905a48cc1161c81a275c9dd2261835f03fefc31d98af9bddfc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Architecture dev\nclean\ndev\nother\nConformer 1.9 4.4\n\u2013 Depthwise conv + Lightweight convolution 2.0 4.8\nConvolution block before MHSA 1.9 4.5\nParallel MHSA and Convolution 2.0 4.9\n3.4.3. Macaron Feed Forward Modules\nInstead of a single feed-forward module (FFN) post the atten-\ntion blocks as in the Transformer models, the Conformer block\nhas a pair of macaron-like Feed forward modules sandwiching\nthe self-attention and convolution modules. Further, the Con-\nformer feed forward modules are used with half-step residuals.\nTable 5 shows the impact of changing the Conformer block to\nuse a single FFN or full-step residuals.\nTable 5: Ablation study of Macaron-net Feed Forward mod-\nules. Ablating the differences between the Conformer feed for-\nward module with that of a single FFN used in Transformer\nmodels: (1) Conformer; (2) Conformer with full-step residuals\nin Feed forward modules; (3) replacing the Macaron-style FFN\npair with a single FFN.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer 1.9 4.4 2.1 4.3\nSingle FFN 1.9 4.5 2.1 4.5\nFull step residuals 1.9 4.5 2.1 4.5\n3.4.4. Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average.", "mimetype": "text/plain", "start_char_idx": 2272, "end_char_idx": 3575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3159b1cb-85a0-4df1-8173-f631dc6c973b": {"__data__": {"id_": "3159b1cb-85a0-4df1-8173-f631dc6c973b", "embedding": null, "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b2d488d8ca3a08361451657ff7e90bee8311fd2dd67237cc672031b9e23a42e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46a8e9be-2a77-47fd-8bea-66b1e482ac65", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "134d6a0acdd5b474b1961ffdb81c40eda1f3a00061a8910d696e6000112854c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff40bcdf-f8f0-401c-a40e-b747052e70a4", "node_type": "1", "metadata": {}, "hash": "c757b76a85207889204f1344c1d618f4b86c5634d8a909ef71eede9ae44a294d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average. We perform experiments\nto study the effect of varying the number of attention heads from\n4 to 32 in our large model, using the same number of heads\nin all layers. We \ufb01nd that increasing attention heads up to 16\nimproves the accuracy, especially over the devother datasets, as\nshown in Table 6.\nTable 6: Ablation study on the attention heads in multi-headed\nself attention.\nAttention\nHeads\nDim per\nHead\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n4 128 1.9 4.6 2.0 4.5\n8 64 1.9 4.4 2.1 4.3\n16 32 2.0 4.3 2.2 4.4\n32 16 1.9 4.4 2.1 4.5\n3.4.5. Convolution Kernel Sizes\nTo study the effect of kernel sizes in the depthwise convolu-\ntion, we sweep the kernel size in {3,7,17,32,65}of the large\nmodel, using the same kernel size for all layers. We \ufb01nd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\n\ufb01nd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.", "mimetype": "text/plain", "start_char_idx": 3384, "end_char_idx": 4622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff40bcdf-f8f0-401c-a40e-b747052e70a4": {"__data__": {"id_": "ff40bcdf-f8f0-401c-a40e-b747052e70a4", "embedding": null, "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "1b2d488d8ca3a08361451657ff7e90bee8311fd2dd67237cc672031b9e23a42e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3159b1cb-85a0-4df1-8173-f631dc6c973b", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "81711a8de8d9fd038fbd8b622f0742292f7760fc38750518a8eeaa613e4fbce2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \ufb01nd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\n\ufb01nd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.\nKernel\nsize\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n3 1.88 4.41 1.99 4.39\n7 1.88 4.30 2.02 4.44\n17 1.87 4.31 2.04 4.38\n32 1.83 4.30 2.03 4.29\n65 1.89 4.47 1.98 4.46\n4. Conclusion\nIn this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother.", "mimetype": "text/plain", "start_char_idx": 4312, "end_char_idx": 5292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8e9092e-4033-4719-ab14-c692f2b058da": {"__data__": {"id_": "e8e9092e-4033-4719-ab14-c692f2b058da", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48988c3c-e1a3-4f36-88a8-148c7a38b1ee", "node_type": "1", "metadata": {}, "hash": "1f2dfd97e752992183b366bf58880b60f30a58aaa609f458dcdc201879808ea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. References\n[1] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Goninaet al., \u201cState-\nof-the-art speech recognition with sequence-to-sequence models,\u201d\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 4774\u20134778.\n[2] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures,\ndata and units for streaming end-to-end speech recognition with\nrnn-transducer,\u201d in2017 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199.\n[3] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y . Wu, R. Pang, Q. Liang,\nD. Bhatia, Y . Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,\nS.-Y . Chang, K. Rao, and A. Gruenstein, \u201cStreaming End-to-end\nSpeech Recognition For Mobile Devices,\u201d inProc. ICASSP, 2019.\n[4] T. N. Sainath, Y . He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48988c3c-e1a3-4f36-88a8-148c7a38b1ee": {"__data__": {"id_": "48988c3c-e1a3-4f36-88a8-148c7a38b1ee", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8e9092e-4033-4719-ab14-c692f2b058da", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "f4781ded021095b8de34c232dc116e4fd98c1ded5e7d0f115c657c470a0d4eea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b9422a1-b351-4f1e-a0c1-efbc92b95863", "node_type": "1", "metadata": {}, "hash": "105fc0a02a683d63080f010faa57ff848b19e911fafcda79ea7412f0e0aa8462", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chang, K. Rao, and A. Gruenstein, \u201cStreaming End-to-end\nSpeech Recognition For Mobile Devices,\u201d inProc. ICASSP, 2019.\n[4] T. N. Sainath, Y . He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d inICASSP, 2020.\n[5] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201darXiv preprint arXiv:1211.3711, 2012.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\n2017.\n[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,\u201d inICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7829\u20137833.\n[8] J. Li, V .", "mimetype": "text/plain", "start_char_idx": 776, "end_char_idx": 1717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b9422a1-b351-4f1e-a0c1-efbc92b95863": {"__data__": {"id_": "2b9422a1-b351-4f1e-a0c1-efbc92b95863", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48988c3c-e1a3-4f36-88a8-148c7a38b1ee", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "2596c60b357d3676d216d4c39fec23dc2905e92f0e8e3f85150448c6c4a28e3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95c9a21b-cd4a-45be-8bbb-74a1bff406fd", "node_type": "1", "metadata": {}, "hash": "274eaa757969ed10bcbb874d51ae378914926d3102644130b5a311fa25f9d68d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE, 2020, pp. 7829\u20137833.\n[8] J. Li, V . Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co-\nhen, H. Nguyen, and R. T. Gadde, \u201cJasper: An end-to-end convo-\nlutional neural acoustic model,\u201darXiv preprint arXiv:1904.03288,\n2019.\n[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,\nV . Lavrukhin, R. Leary, J. Li, and Y . Zhang, \u201cQuartznet: Deep\nautomatic speech recognition with 1d time-channel separable con-\nvolutions,\u201darXiv preprint arXiv:1910.10261, 2019.\n[10] W. Han, Z. Zhang, Y . Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y . Wu, \u201cContextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,\u201d\narXiv preprint arXiv:2005.03191, 2020.\n[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, \u201cDeep convolutional neural networks for lvcsr,\u201d in2013 IEEE\ninternational conference on acoustics, speech and signal process-\ning. IEEE, 2013, pp. 8614\u20138618.\n[12] O. Abdel-Hamid, A.-r.", "mimetype": "text/plain", "start_char_idx": 1676, "end_char_idx": 2646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95c9a21b-cd4a-45be-8bbb-74a1bff406fd": {"__data__": {"id_": "95c9a21b-cd4a-45be-8bbb-74a1bff406fd", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b9422a1-b351-4f1e-a0c1-efbc92b95863", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "271763fb0c83c3d354564bda13a2aa69bd19515d746a2fee4ecf389cbc44c1ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4", "node_type": "1", "metadata": {}, "hash": "584fcb4835ee456d42789ee803311d666a0a70d98f31fb60a42fa6a3e7cf6501", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, \u201cDeep convolutional neural networks for lvcsr,\u201d in2013 IEEE\ninternational conference on acoustics, speech and signal process-\ning. IEEE, 2013, pp. 8614\u20138618.\n[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,\nand D. Yu, \u201cConvolutional neural networks for speech recogni-\ntion,\u201d IEEE/ACM Transactions on audio, speech, and language\nprocessing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[13] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7132\u20137141.\n[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, \u201cAttention\naugmented convolutional networks,\u201d in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2019, pp. 3286\u2013\n3295.\n[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \u201cConvolu-\ntional self-attention networks,\u201d arXiv preprint arXiv:1904.03107,\n2019.\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V .", "mimetype": "text/plain", "start_char_idx": 2391, "end_char_idx": 3445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4": {"__data__": {"id_": "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95c9a21b-cd4a-45be-8bbb-74a1bff406fd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "c6c22bd187680d8e81968d9a48a788831983a7476f8ad738eda6a7d3fddf290e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebd80897-b5dc-40c1-ab95-02f98c9c38ba", "node_type": "1", "metadata": {}, "hash": "37e6181a24b5db7fb827bd517863158799daed2583272bb42965cf6d5e1b7592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \u201cConvolu-\ntional self-attention networks,\u201d arXiv preprint arXiv:1904.03107,\n2019.\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V . Le, \u201cQanet: Combining local convolution with\nglobal self-attention for reading comprehension,\u201d arXiv preprint\narXiv:1804.09541, 2018.\n[17] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, \u201cLite transformer with\nlong-short range attention,\u201d arXiv preprint arXiv:2004.11886 ,\n2020.\n[18] Y . Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and\nT.-Y . Liu, \u201cUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view,\u201d arXiv preprint\narXiv:1906.02762, 2019.\n[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y .", "mimetype": "text/plain", "start_char_idx": 3230, "end_char_idx": 4189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebd80897-b5dc-40c1-ab95-02f98c9c38ba": {"__data__": {"id_": "ebd80897-b5dc-40c1-ab95-02f98c9c38ba", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "7591e222ff37d4743da958f314b8aef8d2cb3d9a3b8624e7780201d51b16a260", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6ccb94b-8de7-428a-9c60-89c0e5750933", "node_type": "1", "metadata": {}, "hash": "85c6cf0d45568106fcf44d1b3a3477f2295f5c04e8cccd4de86da0065d7f2c4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhut-\ndinov, \u201cTransformer-xl: Attentive language models beyond a\n\ufb01xed-length context,\u201d 2019.\n[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n\u201cLearning deep transformer models for machine translation,\u201d in\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Lin-\nguistics, Jul. 2019, pp. 1810\u20131822.\n[22] T. Q. Nguyen and J. Salazar, \u201cTransformers without tears:\nImproving the normalization of self-attention,\u201d arXiv preprint\narXiv:1910.05895, 2019.\n[23] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage\nmodeling with gated convolutional networks,\u201d in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70. JMLR. org, 2017, pp. 933\u2013941.\n[24] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).", "mimetype": "text/plain", "start_char_idx": 4020, "end_char_idx": 5205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6ccb94b-8de7-428a-9c60-89c0e5750933": {"__data__": {"id_": "c6ccb94b-8de7-428a-9c60-89c0e5750933", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebd80897-b5dc-40c1-ab95-02f98c9c38ba", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "6cfdfbfc95236afca9cf335b7fc46a60b3c94d413ae343ad03cbb27d5b789ebf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8c9d156-629a-4a51-8509-77a6bea84425", "node_type": "1", "metadata": {}, "hash": "6175d1ffdf2378d5f1629bc79793afdeeb90c79b1a59d1e052e4926b9302017f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "JMLR. org, 2017, pp. 933\u2013941.\n[24] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 5884\u20135888.\n[25] P. Ramachandran, B. Zoph, and Q. V . Le, \u201cSearching for activa-\ntion functions,\u201darXiv preprint arXiv:1710.05941, 2017.\n[26] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\n[27] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, \u201cSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y . Zhang, C.-C. Chiu, Y . Chen, B. Li, W. Chan, Q. V .\nLe, and Y .", "mimetype": "text/plain", "start_char_idx": 4964, "end_char_idx": 5893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8c9d156-629a-4a51-8509-77a6bea84425": {"__data__": {"id_": "a8c9d156-629a-4a51-8509-77a6bea84425", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ccb94b-8de7-428a-9c60-89c0e5750933", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "c82e9561b831ce793c0f6bf871c3ef9091bbdcbe562f16b203d4d1f2977c2058", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f802c401-9854-445c-850c-66fe04a100d9", "node_type": "1", "metadata": {}, "hash": "c208ab094b1744c2ed2395094294895909da185233aae7ad7fc3005face6da50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Le, \u201cSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y . Zhang, C.-C. Chiu, Y . Chen, B. Li, W. Chan, Q. V .\nLe, and Y . Wu, \u201cSpecaugment on large scale datasets,\u201d arXiv\npreprint arXiv:1912.05533, 2019.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, \u201cDropout: A simple way to prevent neural net-\nworks from over\ufb01tting,\u201d Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929\u20131958, 2014.\n[30] K.-C. Jim, C. L. Giles, and B. G. Horne, \u201cAn analysis of noise\nin recurrent neural networks: convergence and generalization,\u201d\nIEEE Transactions on neural networks , vol. 7, no. 6, pp. 1424\u2013\n1438, 1996.\n[31] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201darXiv preprint arXiv:1412.6980, 2014.\n[32] J. Shen, P. Nguyen, Y . Wu, Z. Chen, and et al., \u201cLingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,\u201d\n2019.\n[33] Y .", "mimetype": "text/plain", "start_char_idx": 5681, "end_char_idx": 6673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f802c401-9854-445c-850c-66fe04a100d9": {"__data__": {"id_": "f802c401-9854-445c-850c-66fe04a100d9", "embedding": null, "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "b9f3dcdccfdf272cdbb996ae0442a3d37d145af3c5acefed9e6f1f20b15cec05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8c9d156-629a-4a51-8509-77a6bea84425", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}, "hash": "eabd919073766ac695da11df976ca59014d211e4437eafd697a047f2b1710da5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[32] J. Shen, P. Nguyen, Y . Wu, Z. Chen, and et al., \u201cLingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,\u201d\n2019.\n[33] Y . Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \u201cTransformer-\nbased acoustic modeling for hybrid speech recognition,\u201d arXiv\npreprint arXiv:1910.09799, 2019.\n[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,\nV . Pratap, A. Sriram, V . Liptchinsky, and R. Collobert, \u201cEnd-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,\u201d 2019.\n[35] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli, \u201cPay\nless attention with lightweight and dynamic convolutions,\u201d arXiv\npreprint arXiv:1901.10430, 2019.", "mimetype": "text/plain", "start_char_idx": 6525, "end_char_idx": 7267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f4eb994-a455-4c67-a3f1-5c51af04d89d": {"__data__": {"id_": "9f4eb994-a455-4c67-a3f1-5c51af04d89d", "embedding": null, "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98bd6505-291a-43f8-849d-54681f25a6e6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "838ad244c2b6eb0210ac34aa477608d107753bde5fa21d1a92999894785c3283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9eef1426-56c8-4d4e-9fba-6d5074a0e25c", "node_type": "1", "metadata": {}, "hash": "7ee3ce4f12aa9dc860e9731f03a6ac1c25cc3550cf0692622b76febcdd964648", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\nPANNs: Large-Scale Pretrained Audio Neural Networks for\nAudio Pattern Recognition\nQiuqiang Kong, Student Member, IEEE, Yin Cao, Member, IEEE, Turab Iqbal,\nYuxuan Wang, Wenwu Wang, Senior Member, IEEE and Mark D. Plumbley, Fellow, IEEE\nAbstract\u2014Audio pattern recognition is an important research\ntopic in the machine learning area, and includes several tasks\nsuch as audio tagging, acoustic scene classi\ufb01cation, music classi-\n\ufb01cation, speech emotion classi\ufb01cation and sound event detection.\nRecently, neural networks have been applied to tackle audio\npattern recognition problems. However, previous systems are\nbuilt on speci\ufb01c datasets with limited durations. Recently, in com-\nputer vision and natural language processing, systems pretrained\non large-scale datasets have generalized well to several tasks.\nHowever, there is limited research on pretraining systems on\nlarge-scale datasets for audio pattern recognition. In this paper,\nwe propose pretrained audio neural networks (PANNs) trained\non the large-scale AudioSet dataset. These PANNs are transferred\nto other audio related tasks. We investigate the performance\nand computational complexity of PANNs modeled by a variety\nof convolutional neural networks. We propose an architecture\ncalled Wavegram-Logmel-CNN using both log-mel spectrogram\nand waveform as input feature. Our best PANN system achieves\na state-of-the-art mean average precision (mAP) of 0.439 on\nAudioSet tagging, outperforming the best previous system of\n0.392. We transfer PANNs to six audio pattern recognition tasks,\nand demonstrate state-of-the-art performance in several of those\ntasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9eef1426-56c8-4d4e-9fba-6d5074a0e25c": {"__data__": {"id_": "9eef1426-56c8-4d4e-9fba-6d5074a0e25c", "embedding": null, "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98bd6505-291a-43f8-849d-54681f25a6e6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "838ad244c2b6eb0210ac34aa477608d107753bde5fa21d1a92999894785c3283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f4eb994-a455-4c67-a3f1-5c51af04d89d", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "53cea49dca0ce08cf6ee14a29ecf1e3d29b5065dc86c575ece2e1d28fae54135", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef357faf-228a-4caf-8007-a9c13493ca66", "node_type": "1", "metadata": {}, "hash": "e08ebb47f8ab62f06aab3b6b31dadc2c0d60658f7f37014108725cd504afed42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We propose an architecture\ncalled Wavegram-Logmel-CNN using both log-mel spectrogram\nand waveform as input feature. Our best PANN system achieves\na state-of-the-art mean average precision (mAP) of 0.439 on\nAudioSet tagging, outperforming the best previous system of\n0.392. We transfer PANNs to six audio pattern recognition tasks,\nand demonstrate state-of-the-art performance in several of those\ntasks. We have released the source code and pretrained models of\nPANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.\nIndex Terms\u2014Audio tagging, pretrained audio neural net-\nworks, transfer learning.\nI. I NTRODUCTION\nAudio pattern recognition is an important research topic\nin the machine learning area, and plays an important role\nin our life. We are surrounded by sounds that contain rich\ninformation of where we are, and what events are happening\naround us. Audio pattern recognition contains several tasks\nsuch as audio tagging [1], acoustic scene classi\ufb01cation [2],\nmusic classi\ufb01cation [3], speech emotion classi\ufb01cation and\nsound event detection [4].\nAudio pattern recognition has attracted increasing research\ninterest in recent years. Early audio pattern recognition work\nQ. Kong, Y . Cao, T. Iqbal, and M. D. Plumbley are with the Centre\nfor Vision, Speech and Signal Processing, University of Surrey, Guild-\nford GU2 7XH, U.K. (e-mail: q.kong@surrey.ac.uk; yin.cao@surrey.ac.uk;\nt.iqbal@surrey.ac.uk; m.plumbley@surrey.ac.uk).", "mimetype": "text/plain", "start_char_idx": 1216, "end_char_idx": 2655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef357faf-228a-4caf-8007-a9c13493ca66": {"__data__": {"id_": "ef357faf-228a-4caf-8007-a9c13493ca66", "embedding": null, "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98bd6505-291a-43f8-849d-54681f25a6e6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "838ad244c2b6eb0210ac34aa477608d107753bde5fa21d1a92999894785c3283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9eef1426-56c8-4d4e-9fba-6d5074a0e25c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cf9c2cde5dc64619f8aeb9d69c0989bb833e0680a20f127c948905c41836bdee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa6268c5-875a-4fca-8150-1c428685b75c", "node_type": "1", "metadata": {}, "hash": "7b62430c18354d5436082f38958641182e179d6519dcca29ff52dfeb737e6ed2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cao, T. Iqbal, and M. D. Plumbley are with the Centre\nfor Vision, Speech and Signal Processing, University of Surrey, Guild-\nford GU2 7XH, U.K. (e-mail: q.kong@surrey.ac.uk; yin.cao@surrey.ac.uk;\nt.iqbal@surrey.ac.uk; m.plumbley@surrey.ac.uk).\nThis work was supported in part by the EPSRC Grant EP/N014111/1\n\u201cMaking Sense of Sounds\u201d, in part by the Research Scholarship from the\nChina Scholarship Council 201406150082, and in part by a studentship\n(Reference: 1976218) from the EPSRC Doctoral Training Partnership under\nGrant EP/N509772/1. This work was supported by National Natural Science\nFoundation of China (Grant No. 11804365). (Qiuqiang Kong is \ufb01rst author.)\n(Yin Cao is corresponding author.)\nY . Wang is with the ByteDance AI Lab, Mountain View, CA, USA (e-mail:\nwangyuxuan.11@bytedance.com).\nW. Wang is with the Centre for Vision, Speech and Signal Processing,\nUniversity of Surrey, Guildford GU2 7XH, U.K., and also with Qingdao\nUniversity of Science and Technology, Qingdao 266071, China (e-mail:\nw.wang@surrey.ac.uk).\nfocused on private datasets collected by individual researchers\n[5][6]. For example, Woodard [5] applied a hidden Markov\nmodel (HMM) to classify three types of sounds: wooden door\nopen and shut, dropped metal and poured water.", "mimetype": "text/plain", "start_char_idx": 2412, "end_char_idx": 3669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa6268c5-875a-4fca-8150-1c428685b75c": {"__data__": {"id_": "aa6268c5-875a-4fca-8150-1c428685b75c", "embedding": null, "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98bd6505-291a-43f8-849d-54681f25a6e6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "838ad244c2b6eb0210ac34aa477608d107753bde5fa21d1a92999894785c3283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef357faf-228a-4caf-8007-a9c13493ca66", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "735855347642e87040e145e06df63646d5e89b6f4d0689182921262e725eda3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e653e99-88c9-4408-b338-299410edd28b", "node_type": "1", "metadata": {}, "hash": "f3247aaa9f36f94e5a41fc2a22054043aea2a21dd8f40116d7f25d77bc0bc175", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "focused on private datasets collected by individual researchers\n[5][6]. For example, Woodard [5] applied a hidden Markov\nmodel (HMM) to classify three types of sounds: wooden door\nopen and shut, dropped metal and poured water. Recently, the\nDetection and Classi\ufb01cation of Acoustic Scenes and Events\n(DCASE) challenge series [7][8][9][2] have provided publicly\navailable datasets, such as acoustic scene classi\ufb01cation and\nsound event detection datasets. The DCASE challenges have\nattracted increasing research interest in audio pattern recogni-\ntion. For example, the recent DCASE 2019 challenge received\n311 entries across \ufb01ve subtasks [10].\nHowever, it is still an open question how well an audio\npattern recognition system can perform when trained on large-\nscale datasets. In computer vision, several image classi\ufb01cation\nsystems have been built with the large-scale ImageNet dataset\n[11]. In natural language processing, several language models\nhave been built with the large-scale text datasets such as\nWikipedia [12]. However, systems trained on large-scale audio\ndatasets have been more limited [1][13][14][15].\nA milestone for audio pattern recognition was the release of\nAudioSet [1], a dataset containing over 5,000 hours of audio\nrecordings with 527 sound classes. Instead of releasing the raw\naudio recordings, AudioSet released embedding features of au-\ndio clips extracted from a pretrained convolutional neural net-\nwork [13]. Several researchers have investigated building sys-\ntems with those embedding features [13][16][17][18][19][20].\nHowever, the embedding features may not be an optimal\nrepresentation for audio recordings, which may limit the\nperformance of those systems.", "mimetype": "text/plain", "start_char_idx": 3443, "end_char_idx": 5137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e653e99-88c9-4408-b338-299410edd28b": {"__data__": {"id_": "2e653e99-88c9-4408-b338-299410edd28b", "embedding": null, "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98bd6505-291a-43f8-849d-54681f25a6e6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "838ad244c2b6eb0210ac34aa477608d107753bde5fa21d1a92999894785c3283", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa6268c5-875a-4fca-8150-1c428685b75c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "631269b33500f0045e7a2a6408830832296fddad182cbb944eac9706764de723", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of releasing the raw\naudio recordings, AudioSet released embedding features of au-\ndio clips extracted from a pretrained convolutional neural net-\nwork [13]. Several researchers have investigated building sys-\ntems with those embedding features [13][16][17][18][19][20].\nHowever, the embedding features may not be an optimal\nrepresentation for audio recordings, which may limit the\nperformance of those systems. In this article, we propose\npretrained audio neural networks (PANNs) trained on raw\nAudioSet recordings with a wide range of neural networks. We\nshow that several PANN systems outperform previous state-\nof-the-art audio tagging systems. We also investigate the audio\ntagging performance and computation complexities of PANNs.\nWe propose that PANNs can be transferred to other audio\npattern recognition tasks. Previous researchers have previously\ninvestigated transfer learning for audio tagging. For example,\naudio tagging systems were pretrained on the Million Song\nDataset were proposed in [21], with embedding features ex-\ntracted from pretrained convolutional neural networks (CNNs)\nare used as inputs to second-stage classi\ufb01ers such as neural\nnetworks or support vector machines (SVMs) [14][22]. Sys-\ntems pretrained on MagnaTagATune [23] and acoustic scene\n[24] datasets were \ufb01ne-tuned on other audio tagging tasks\n[25][26]. These transfer learning systems were mainly trained\nwith music datasets, and were limited to smaller datasets than\nAudioSet.\nThe contribution of this work includes: (1) We introduce\narXiv:1912.10211v5  [cs.SD]  23 Aug 2020", "mimetype": "text/plain", "start_char_idx": 4718, "end_char_idx": 6291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d775d94-255e-433e-abc4-464d8961a836": {"__data__": {"id_": "1d775d94-255e-433e-abc4-464d8961a836", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423b3ae5-ac1f-4768-b51a-631a1c7a3998", "node_type": "1", "metadata": {}, "hash": "2bc6a7d6d123ba8a4bc03d362b8eade1f26cb6ec23fc6d01f199ba1e95e67bfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2\nPANNs trained on AudioSet with 1.9 million audio clips with\nan ontology of 527 sound classes; (2) We investigate the\ntrade-off between audio tagging performance and computation\ncomplexity of a wide range of PANNs; (3) We propose a\nsystem that we call Wavegram-Logmel-CNN that achieves a\nmean average precision (mAP) of 0.439 on AudioSet tagging,\noutperforming previous state-of-the-art system with an mAP\n0.392 and Google\u2019s system with an mAP 0.314; (4) We show\nthat PANNs can be transferred to other audio pattern recog-\nnition tasks, outperforming several state-of-the-art systems;\n(5) We have released the source code and pretrained PANN\nmodels.\nThis paper is organized as follows: Section II introduces\naudio tagging with various convolutional neural networks;\nSection III introduces our proposed Wavegram-CNN systems;\nSection IV introduces our data processing techniques, in-\ncluding data balancing and data augmentation for AudioSet\ntagging; Section VI shows experimental results, and Section\nVII concludes this work.\nII. A UDIO TAGGING SYSTEMS\nAudio tagging is an essential task of audio pattern recog-\nnition, with the goal of predicting the presence or absence\nof audio tags in an audio clip. Early work on audio tag-\nging included using manually-designed features as input,\nsuch as audio energy, zero-crossing rate, and mel-frequency\ncepstrum coef\ufb01cients (MFCCs) [27]. Generative models, in-\ncluding Gaussian mixture models (GMMs) [28][29], hidden\nMarkov models (HMMs), and discriminative support vector\nmachines (SVMs) [30] have been used as classi\ufb01ers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "423b3ae5-ac1f-4768-b51a-631a1c7a3998": {"__data__": {"id_": "423b3ae5-ac1f-4768-b51a-631a1c7a3998", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d775d94-255e-433e-abc4-464d8961a836", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "3406eea01e8f6d07726de9274283a46df78313178f4887dcb107b56f7617e363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1", "node_type": "1", "metadata": {}, "hash": "dd437658157e02488d3532143dd13e88d375126a6dc3b5f3bb607f04f517744e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generative models, in-\ncluding Gaussian mixture models (GMMs) [28][29], hidden\nMarkov models (HMMs), and discriminative support vector\nmachines (SVMs) [30] have been used as classi\ufb01ers. Recently,\nneural network based methods such as convolutional neural\nnetworks (CNNs) have been used [3] to predict the tags of\naudio recordings. CNN-based systems have achieved state-\nof-the-art performance in several DCASE challenge tasks\nincluding acoustic scene classi\ufb01cation [2] and sound event\ndetection [4]. However, many of those works focused on\nparticular tasks with a limited number of sound classes, and\nwere not designed to recognize a wide range of sound classes.\nIn this article, we focus on training large-scale PANNs on\nAudioSet [1] to tackle the general audio tagging problem.\nA. CNNs\n1) Conventional CNNs: CNNs have been successfully ap-\nplied to computer vision tasks such as image classi\ufb01cation\n[31][32]. A CNN consists of several convolutional layers.\nEach convolutional layer contains several kernels that are\nconvolved with the input feature maps to capture their local\npatterns. CNNs adopted for audio tagging [3][20] often use\nlog mel spectrograms as input [3][20]. Short time Fourier\ntransforms (STFTs) are applied to time-domain waveforms\nto calculate spectrograms. Then, mel \ufb01lter banks are applied\nto the spectrograms, followed by a logarithmic operation to\nextract log mel spectrograms [3][20].\n2) Adapting CNNs for AudioSet tagging: The PANNs we\nuse are based on our previously-proposed cross-task CNN\nsystems for the DCASE 2019 challenge [33],", "mimetype": "text/plain", "start_char_idx": 1381, "end_char_idx": 2941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1": {"__data__": {"id_": "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423b3ae5-ac1f-4768-b51a-631a1c7a3998", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "733bc8b55bd41b8f9544fd5eeec82931cd409ee7a467eda1a0b12e8303818270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6", "node_type": "1", "metadata": {}, "hash": "8111e7a1ae0f1e79d03ade3c92118762b7bc3ced69d2cb2d59a7fd804724c32d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Short time Fourier\ntransforms (STFTs) are applied to time-domain waveforms\nto calculate spectrograms. Then, mel \ufb01lter banks are applied\nto the spectrograms, followed by a logarithmic operation to\nextract log mel spectrograms [3][20].\n2) Adapting CNNs for AudioSet tagging: The PANNs we\nuse are based on our previously-proposed cross-task CNN\nsystems for the DCASE 2019 challenge [33], with an extra\nfully-connected layer added to the penultimate layer of CNNs\nTABLE I\nCNN S FOR AUDIO SET TAGGING\nVGGish [1] CNN6 CNN10 CNN14\nLog-mel spectrogram\n96frames\u00d764mel bins\nLog-mel spectrogram\n1000frames\u00d764mel bins\n3\u00d73 @ 64\nReLU\n5\u00d75 @ 64\nBN, ReLU\n(3\u00d73 @ 64\nBN, ReLU\n)\n\u00d72\n(3\u00d73 @ 64\nBN, ReLU\n)\n\u00d72\nMP2\u00d72 Pooling2\u00d72\n3\u00d73 @ 128\nReLU\n5\u00d75 @ 128\nBN, ReLU\n(3\u00d73 @ 128\nBN, ReLU\n)\n\u00d72\n(3\u00d73 @ 128\nBN, ReLU\n)\n\u00d72\nMP2\u00d72 Pooling2\u00d72(3\u00d73 @ 256\nReLU\n)\n\u00d72 5\u00d75 @ 256\nBN, ReLU\n(3\u00d73 @ 256\nBN, ReLU\n)\n\u00d72\n(3\u00d73 @ 256\nBN, ReLU\n)\n\u00d72\nMP2\u00d72 Pooling2\u00d72(3\u00d73 @ 512\nReLU\n)\n\u00d72 5\u00d75 @ 512\nBN, ReLU\n(3\u00d73 @ 512\nBN,", "mimetype": "text/plain", "start_char_idx": 2557, "end_char_idx": 3520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6": {"__data__": {"id_": "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e15868a784f6edec0ad52d9c1042cd36c24570347d9b3270d42142fd8b97016e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f47292db-3a95-41f0-9573-b92ee8f6ab9d", "node_type": "1", "metadata": {}, "hash": "4dbb3e925c78cb6aba33b00f665091a7911d31436cccf657047526f034ba9f9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ReLU\n(3\u00d73 @ 256\nBN, ReLU\n)\n\u00d72\n(3\u00d73 @ 256\nBN, ReLU\n)\n\u00d72\nMP2\u00d72 Pooling2\u00d72(3\u00d73 @ 512\nReLU\n)\n\u00d72 5\u00d75 @ 512\nBN, ReLU\n(3\u00d73 @ 512\nBN, ReLU\n)\n\u00d72\n(3\u00d73 @ 512\nBN, ReLU\n)\n\u00d72\nMP2\u00d72\nFlatten Global pooling Pooling2\u00d72\nFC4096\nReLU \u00d72 FC 512, ReLU\n(3\u00d73 @ 1024\nBN, ReLU\n)\n\u00d72\nFC 527, Sigmoid FC 527, Sigmoid Pooling2\u00d72(3\u00d73 @ 2048\nBN, ReLU\n)\n\u00d72\nGlobal pooling\nFC 2048, ReLU\nFC 527, Sigmoid\nto further increase the representation ability. We investigate\n6-, 10- and 14-layer CNNs. The 6-layer CNN consists of\n4 convolutional layers with a kernel size of 5 \u00d75, based\non AlexNet [34]. The 10- and 14-layer CNNs consist of\n4 and 6 convolutional layers, respectively, inspired by the\nVGG-like CNNs [35]. Each convolutional block consists of\n2 convolutional layers with a kernel size of 3 \u00d73. Batch\nnormalization [36] is applied between each convolutional\nlayer, and the ReLU nonlinearity [37] is used to speed up\nand stabilize the training. We apply average pooling of size\nof 2 \u00d72 to each convolutional block for downsampling, as\n2 \u00d72 average pooling has been shown to outperform 2 \u00d72\nmax pooling [33].", "mimetype": "text/plain", "start_char_idx": 3395, "end_char_idx": 4471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f47292db-3a95-41f0-9573-b92ee8f6ab9d": {"__data__": {"id_": "f47292db-3a95-41f0-9573-b92ee8f6ab9d", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a6b5cdf47790f810da9cd48c2669eae780ccc8d345232401380667064c0f7d00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a05b865-dd80-4aa7-9408-a5e39688dea3", "node_type": "1", "metadata": {}, "hash": "6565dbbe62c662f17af67314b12a25aa22cd90f44c14fecfbf4cc0475cc58d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Batch\nnormalization [36] is applied between each convolutional\nlayer, and the ReLU nonlinearity [37] is used to speed up\nand stabilize the training. We apply average pooling of size\nof 2 \u00d72 to each convolutional block for downsampling, as\n2 \u00d72 average pooling has been shown to outperform 2 \u00d72\nmax pooling [33].\nGlobal pooling is applied after the last convolutional layer\nto summarize the feature maps into a \ufb01xed-length vector. In\n[15], maximum and average operation were used for global\npooling. To combine their advantages, we sum the averaged\nand maximized vectors. In our previous work [33], those \ufb01xed-\nlength vectors were used as embedding features for audio clips.\nIn this work, we add an extra fully-connected layer to the \ufb01xed\nlength vectors to extract embedding features which can further\nincrease their representation ability. For a particular audio\npattern recognition task, a linear classi\ufb01er is applied to the\nembedding features, followed by either a softmax nonlinearity\nfor classi\ufb01cation tasks or a sigmoid nonlinearity for tagging\ntasks. Dropout [38] is applied after each downsampling op-\neration and fully connected layers to prevent systems from\nover\ufb01tting. Table I summarizes our proposed CNN systems.\nThe number after the \u201c @\u201d symbol indicates the number of\nfeature maps. The \ufb01rst column shows the VGGish network\nproposed by [13]. MP is the abbreviation of max pooling. The\n\u201cPooling 2\u00d72\u201d in Table I is average pooling with size of 2\u00d72.\nIn [13], an audio clip was split into 1-second segments, [13]\nalso assumed each segment inherits the label of the audio clip,\nwhich may lead to incorrect labels.", "mimetype": "text/plain", "start_char_idx": 4160, "end_char_idx": 5781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a05b865-dd80-4aa7-9408-a5e39688dea3": {"__data__": {"id_": "7a05b865-dd80-4aa7-9408-a5e39688dea3", "embedding": null, "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4477d28c88dcf36a73a130b8e720f40e604fcc435e27eb2feb3db638094ce0f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f47292db-3a95-41f0-9573-b92ee8f6ab9d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "d3a8bcdafca77b4842d5d51b2496a170b111f41278e0c18cb8402c104cbe1c9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \ufb01rst column shows the VGGish network\nproposed by [13]. MP is the abbreviation of max pooling. The\n\u201cPooling 2\u00d72\u201d in Table I is average pooling with size of 2\u00d72.\nIn [13], an audio clip was split into 1-second segments, [13]\nalso assumed each segment inherits the label of the audio clip,\nwhich may lead to incorrect labels. In contrast, our systems\nfrom the second to the fourth columns in Table I applies an\nentire audio clip for training without cutting the audio clip\ninto segments.", "mimetype": "text/plain", "start_char_idx": 5456, "end_char_idx": 5943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0081f9f-b637-42fa-8290-b70cf952a28d": {"__data__": {"id_": "e0081f9f-b637-42fa-8290-b70cf952a28d", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7abea2c3-fd8f-4a10-88f6-879cc97ac304", "node_type": "1", "metadata": {}, "hash": "dd937d8ffdb19d23d6364eba4c5d280398ddec8261cd4eeba18ce94aa1301b33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3\nTABLE II\nRESNETS FOR AUDIO SET TAGGING\nResNet22 ResNet38 ResNet54\nLog mel spectrogram1000 frames \u00d764 mel bins(3 \u00d73 @ 512, BN, ReLU) \u00d72\nPooling 2 \u00d72(BasicB @ 64) \u00d72 (BasicB @ 64) \u00d73 (BottleneckB@ 64) \u00d73\nPooling 2 \u00d72(BasicB @ 128) \u00d72 (BasicB @ 128) \u00d74 (BottleneckB@ 128) \u00d74\nPooling 2 \u00d72(BasicB @ 256) \u00d72 (BasicB @ 256) \u00d76 (BottleneckB@ 256) \u00d76\nPooling 2 \u00d72(BasicB @ 512) \u00d72 (BasicB @ 512) \u00d73 (BottleneckB@ 512) \u00d73\nPooling 2 \u00d72(3 \u00d73 @ 2048, BN, ReLU) \u00d72\nGlobal pooling\nFC 2048, ReLU\nFC 527, Sigmoid\nWe denote the waveform of an audio clip as xn, where nis\nthe index of audio clips, and f(xn) \u2208[0,1]K is the output of\na PANN representing the presence probabilities of K sound\nclasses. The label of xn is denoted as yn \u2208{0,1}K. A binary\ncross-entropy loss function l is used to train a PANN:\nl= \u2212\nN\u2211\nn=1\n(yn \u00b7lnf(xn) + (1\u2212yn) \u00b7ln(1 \u2212f(xn)), (1)\nwhere N is the number of training clips in AudioSet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7abea2c3-fd8f-4a10-88f6-879cc97ac304": {"__data__": {"id_": "7abea2c3-fd8f-4a10-88f6-879cc97ac304", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0081f9f-b637-42fa-8290-b70cf952a28d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "b4ca4c3e5c79d44aefad8638451271ad4f4521abaa19781ec420954cb9a62859", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92ccba74-3c14-4b84-bc2c-750d9f3ce26f", "node_type": "1", "metadata": {}, "hash": "e42f9b4753690bdb1ef5eb86cd767e1868cdee8fd4c7ca815c5fa901704cce03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The label of xn is denoted as yn \u2208{0,1}K. A binary\ncross-entropy loss function l is used to train a PANN:\nl= \u2212\nN\u2211\nn=1\n(yn \u00b7lnf(xn) + (1\u2212yn) \u00b7ln(1 \u2212f(xn)), (1)\nwhere N is the number of training clips in AudioSet. In\ntraining, the parameters of f(\u00b7) are optimized by using gradient\ndescent methods to minimize the loss function l.\nB. ResNets\n1) Conventional residual networks (ResNets): Deeper\nCNNs have been shown to achieve better performance than\nshallower CNNs for audio classi\ufb01cation [31]. One challenge\nof very deep conventional CNNs is that the gradients do\nnot propagate properly from the top layers to the bottom\nlayers [32]. To address this problem, ResNets [32] introduced\nshortcut connections between convolutional layers. In this way,\nthe forward and backward signals can be propagated from\none layer to any other layer directly. The shortcut connections\nonly introduce a small number of extra parameters and a little\nadditional computational complexity. A ResNet consists of\nseveral blocks, where each block consists of two convolutional\nlayers with a kernel size of 3 \u00d73, and a shortcut connection\nbetween input and output. Each bottleneck block consists of\nthree convolutional layers with a network-in-network archi-\ntecture [39] that can be used instead of the basic blocks in a\nResNet [32].\n2) Adapting ResNets for AudioSet tagging: We adapt\nResNet [32] for AudioSet tagging as follows. To begin with,\ntwo convolutional layers and a downsampling layer are applied\non the log mel spectrogram to reduce the input log mel\nspectrogram size.", "mimetype": "text/plain", "start_char_idx": 683, "end_char_idx": 2235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92ccba74-3c14-4b84-bc2c-750d9f3ce26f": {"__data__": {"id_": "92ccba74-3c14-4b84-bc2c-750d9f3ce26f", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7abea2c3-fd8f-4a10-88f6-879cc97ac304", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "868190973b5295292e5a64bc3e062530fc92a4e00eb5fdcccc6b038693bc52b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e868248-f7e2-4738-a625-77cbee2f64ca", "node_type": "1", "metadata": {}, "hash": "db50c44d7c153853ec19f0e7f535702e42e0d0b176685126702515ef19bea72a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) Adapting ResNets for AudioSet tagging: We adapt\nResNet [32] for AudioSet tagging as follows. To begin with,\ntwo convolutional layers and a downsampling layer are applied\non the log mel spectrogram to reduce the input log mel\nspectrogram size. We implement three types of ResNets with\ndifferent depths: a 22-layer ResNet with 8 basic blocks; a 38-\nlayer ResNet with 16 basic blocks, and a 54-layer ResNet\nwith 16 residual blocks. Table II shows the architecture of the\nResNet systems adapted for AudioSet tagging. The BasicB and\nBottleneckB are abbreviations of basic block and bottleneck\nblock, respectively.", "mimetype": "text/plain", "start_char_idx": 1990, "end_char_idx": 2601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e868248-f7e2-4738-a625-77cbee2f64ca": {"__data__": {"id_": "5e868248-f7e2-4738-a625-77cbee2f64ca", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92ccba74-3c14-4b84-bc2c-750d9f3ce26f", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "42fa8a17121732488c30316e804d470e7a2fbce62027bf7bd4a5802042ac0b2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b", "node_type": "1", "metadata": {}, "hash": "922032aeb55be9feaa8452ba670ff668e18c70c128322568829d46d00e468c95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We implement three types of ResNets with\ndifferent depths: a 22-layer ResNet with 8 basic blocks; a 38-\nlayer ResNet with 16 basic blocks, and a 54-layer ResNet\nwith 16 residual blocks. Table II shows the architecture of the\nResNet systems adapted for AudioSet tagging. The BasicB and\nBottleneckB are abbreviations of basic block and bottleneck\nblock, respectively.\nTABLE III\nMOBILE NETS FOR AUDIO SET TAGGING\nMobileNetV1 MobileNetV2\n3 \u00d7 3 @ 32, BN, ReLU\nPooling 2 \u00d7 2\nV1Block @ 64\nV1Block @ 128\nV2Block, t=1 @ 16\n(V2Block, t=6 @ 24)\u00d7 2\nPooling 2 \u00d7 2\nV1Block @ 128\nV1Block @ 256 (V2Block, t=6 @ 32)\u00d7 3\nPooling 2 \u00d7 2\nV1Block @ 256\nV1Block @ 512 (V2Block, t=6 @ 64)\u00d7 4\nPooling 2 \u00d7 2\n(V1Block @ 512)\u00d7 5\nV1Block @ 1024 (V2Block, t=6 @ 96)\u00d7 3\nPooling 2 \u00d7 2\nV1Block @ 1024 (V2Block, t=6 @ 160)\u00d7 3\n(V2Block, t=6 @ 320)\u00d7 1\nGlobal pooling\nFC, 1024, ReLU\nFC, 527, Sigmoid\nC. MobileNets\n1) Conventional MobileNets: Computational complexity is\nan important issue when systems are implemented on portable\ndevices.", "mimetype": "text/plain", "start_char_idx": 2236, "end_char_idx": 3236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b": {"__data__": {"id_": "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e868248-f7e2-4738-a625-77cbee2f64ca", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "d008f99473dc1ba4cdbddb6f32033c424a9abc45b0a56a7f32e6024fa273659e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d40ca56c-518e-43c8-b72a-7ec5916015a2", "node_type": "1", "metadata": {}, "hash": "3a2b1764f91d8b98195e6ebee5b5295cd33a9275be3de21426a142cbd68c219a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to CNNs and ResNets, MobileNets were\nintended to reduce the number of parameters and multiply-add\noperations in a CNN. MobileNets were based on depthwise\nseparable convolutions [40] by factorizing a standard convo-\nlution into a depthwise convolution and a 1 \u00d71 pointwise\nconvolution [40].\n2) Adapting MobileNets for AudioSet tagging: We adapt\nMobileNetV1 [40] and MobileNetV2 [41] systems for Au-\ndioSet tagging shown in Table III. The V1Blocks and\nV2Blocks are MobileNet convolutional blocks [40][41], each\nconsisting of two and three convolutional layers, respectively.\nD. One-dimensional CNNs\nPrevious audio tagging systems were based on the log mel\nspectrogram, a hand-crafted feature. To improve performance,\nseveral researchers proposed to build one-dimensional CNNs\nwhich operate directly on the time-domain waveforms. For\nexample, Dai et al. [31] proposed a one-dimensional CNN for\nacoustic scene classi\ufb01cation, and Lee et al. [42] proposed a\none-dimensional CNN that was later adopted by Pons et al.\n[15] for music tagging.\n1) DaiNet: DaiNet [31] applied kernels of length 80 and\nstride 4 to the input waveform of audio recordings. The kernels\nare learnable during training. To begin with, a maximum\noperation is applied to the \ufb01rst convolutional layer, which\nis designed to make the system be robust to phase shift of\nthe input signals. Then, several one-dimensional convolutional\nblocks with kernel size 3 and stride 4 were applied to extract\nhigh level features.", "mimetype": "text/plain", "start_char_idx": 3237, "end_char_idx": 4721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d40ca56c-518e-43c8-b72a-7ec5916015a2": {"__data__": {"id_": "d40ca56c-518e-43c8-b72a-7ec5916015a2", "embedding": null, "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f851d517-262c-4867-8fb6-6ccc065013c4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f75108a0e53f485275c10d0807b8a213a1b3dc10dd1abb83f95b3a481d16d3a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "78689108ebad1840be7e76b63fb24ce87b3b195332910a50fe3ebb208027f231", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The kernels\nare learnable during training. To begin with, a maximum\noperation is applied to the \ufb01rst convolutional layer, which\nis designed to make the system be robust to phase shift of\nthe input signals. Then, several one-dimensional convolutional\nblocks with kernel size 3 and stride 4 were applied to extract\nhigh level features. An 18-layer DaiNet with four convolu-\ntional layers in each convolutional block achieved the best\nresult in UrbanSound8K [43] classi\ufb01cation [31].\n2) LeeNet: In contrast to DaiNet that applied large kernels\nin the \ufb01rst layer, LeeNet [42] applied small kernels with length\n3 on the waveforms, to replace the STFT for spectrogram", "mimetype": "text/plain", "start_char_idx": 4388, "end_char_idx": 5048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eff43ae2-06e9-446b-9f43-6da5134dda8d": {"__data__": {"id_": "eff43ae2-06e9-446b-9f43-6da5134dda8d", "embedding": null, "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8e9b03c-a557-4618-9aea-c8b50668052e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "5ec04d1cb8259df18fcf9fbd6d414f64d6ff464c5b005abcae269ccd39df7f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb52c700-f4f3-453d-b2a0-0d15b88e668c", "node_type": "1", "metadata": {}, "hash": "8f7004ff03791179e8e3b71be49bdbf6e5f47b9cdc130bd4cffe6bd39b2c84eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4\nextraction. LeeNet consists of several one dimensional con-\nvolutional layers, each followed by a downsampling layer of\nsize 2. The original LeeNet consists of 11 layers.\n3) Adapting one-dimensional CNNs for AudioSet tagging:\nWe modify LeeNet by extending it to a deeper architecture\nwith 24 layers, replacing each convolutional layer with a\nconvolutional block that consists of two convolutional lay-\ners. To further increase the number of layers of the one-\ndimensional CNNs, we propose a one-dimensional residual\nnetwork (Res1dNet) with a small kernel size of 3. We replace\nthe convolutional blocks in LeeNet with residual blocks, where\neach residual block consists of two convolutional layers with\nkernel size 3. The \ufb01rst and second convolutional layers of\nconvolutional block have dilations of 1 and 2, respectively,\nto increase the receptive \ufb01eld of the corresponding residual\nblock. Downsampling is applied after each residual block. By\nusing 14 and 24 residual blocks, we obtain a Res1dNet31 and\na Res1dNet51 with 31 and 51 layers, respectively.\nIII. W AVEGRAM -CNN SYSTEMS\nPrevious one-dimensional CNN systems [31][42][15] have\nnot outperformed the systems trained with log mel spectro-\ngrams as input. One characteristic of previous time-domain\nCNN systems [31][42] is that they were not designed to\ncapture frequency information, because there is no frequency\naxis in the one-dimensional CNN systems, so they can not\ncapture frequency patterns of a sound event with different pitch\nshifts.\nA. Wavegram-CNN systems\nIn this section, we propose architectures which we call\nWavegram-CNN and Wavegram-Logmel-CNN for AudioSet\ntagging.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb52c700-f4f3-453d-b2a0-0d15b88e668c": {"__data__": {"id_": "cb52c700-f4f3-453d-b2a0-0d15b88e668c", "embedding": null, "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8e9b03c-a557-4618-9aea-c8b50668052e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "5ec04d1cb8259df18fcf9fbd6d414f64d6ff464c5b005abcae269ccd39df7f31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eff43ae2-06e9-446b-9f43-6da5134dda8d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "3355b5abf6be5299408a132ad83240f804f95cce93ec1d99e3dd97758b7d817c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "005375bc-2857-416e-bc4d-3fb74f38ecfc", "node_type": "1", "metadata": {}, "hash": "0e34b9faf0798f839fdb8a772de5e395bfa6c0e5d16a5101f5e9b535868794b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One characteristic of previous time-domain\nCNN systems [31][42] is that they were not designed to\ncapture frequency information, because there is no frequency\naxis in the one-dimensional CNN systems, so they can not\ncapture frequency patterns of a sound event with different pitch\nshifts.\nA. Wavegram-CNN systems\nIn this section, we propose architectures which we call\nWavegram-CNN and Wavegram-Logmel-CNN for AudioSet\ntagging. The Wavegram-CNN we propose is a time-domain\naudio tagging system. Wavegram is a feature we propose\nthat is similar to log mel spectrogram, but is learnt using\na neural network. A Wavegram is designed to learn a time-\nfrequency representation that is a modi\ufb01cation of the Fourier\ntransform. A Wavegram has a time axis and a frequency axis.\nFrequency patterns are important for audio pattern recognition,\nfor example, sounds with different pitch shifts belong to\nthe same class. A Wavegram is designed to learn frequency\ninformation that may be lacking in one-dimensional CNN\nsystems. Wavegrams may also improve over hand-crafted log\nmel spectrograms by learning a new kind of time-frequency\ntransform from data. Wavegrams can then replace log mel\nspectrograms as input features resulting in our Wavegram-\nCNN system. We also combine the Wavegram and the log mel\nspectrogram as a new feature to build the Wavegram-Logmel-\nCNN system as shown in Fig. 1.\nTo build a Wavegram, we \ufb01rst apply a one-dimensional\nCNN to time-domain waveform. The one-dimensional CNN\nbegins with a convolutional layer with \ufb01lter length 11 and\nstride 5 to reduce the size of the input. This immediately\nreduces the input lengths by a factor of 5 times to reduce\nmemory usage.", "mimetype": "text/plain", "start_char_idx": 1214, "end_char_idx": 2890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "005375bc-2857-416e-bc4d-3fb74f38ecfc": {"__data__": {"id_": "005375bc-2857-416e-bc4d-3fb74f38ecfc", "embedding": null, "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8e9b03c-a557-4618-9aea-c8b50668052e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "5ec04d1cb8259df18fcf9fbd6d414f64d6ff464c5b005abcae269ccd39df7f31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb52c700-f4f3-453d-b2a0-0d15b88e668c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "0676d03d688ec4be1b515de2f30f36aaacd896757cf8685b382ab1ac16894fae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d989f715-6a45-4fbe-b76a-f9db2cae5edc", "node_type": "1", "metadata": {}, "hash": "d5ed04162714ec8b1dadebd35cb830ce4669abcaf6e480da1cc04a11ba2c1756", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.\nTo build a Wavegram, we \ufb01rst apply a one-dimensional\nCNN to time-domain waveform. The one-dimensional CNN\nbegins with a convolutional layer with \ufb01lter length 11 and\nstride 5 to reduce the size of the input. This immediately\nreduces the input lengths by a factor of 5 times to reduce\nmemory usage. This is followed by three convolutional blocks,\nwhere each convolutional block consists of two convolutional\nlayers with dilations of 1 and 2, respectively, which are\nWaveform\nConv1D, k=11, s=5Conv1D blockMaxPooling1D, s=4Conv1D blockMaxPooling1D, s=4Conv1D blockMaxPooling1D, s=4Reshape to (N, C, T, F)\nWavegram\nLogmelConv2D block\nFeature maps\n2D CNNlayers\nConcat\nPrediction\nFig. 1. Architecture of Wavegram-Logmel-CNN\ndesigned to increase the receptive \ufb01eld of the convolutional\nlayers. Each convolutional block is followed by a downsam-\npling layer with stride 4. By using the stride and downsampling\nthree times, we downsample a 32 kHz audio recording to\n32,000/5/4/4/4 = 100 frames of features per second. We\ndenote the output size of the one-dimensional CNN layers\nas T \u00d7C, where T is the number of frames and C is\nthe number of channels. We reshape this output to a tensor\nwith a size of T \u00d7F \u00d7C/F by splitting C channels into\nC/F groups, where each group has F frequency bins. We\ncall this tensor a Wavegram. The Wavegram learns frequency\ninformation by introducing F frequency bins in each of C/F\nchannels.", "mimetype": "text/plain", "start_char_idx": 2591, "end_char_idx": 4006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d989f715-6a45-4fbe-b76a-f9db2cae5edc": {"__data__": {"id_": "d989f715-6a45-4fbe-b76a-f9db2cae5edc", "embedding": null, "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8e9b03c-a557-4618-9aea-c8b50668052e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "5ec04d1cb8259df18fcf9fbd6d414f64d6ff464c5b005abcae269ccd39df7f31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "005375bc-2857-416e-bc4d-3fb74f38ecfc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e7d86ce657150805ceb5c83491d9542666f985ebf5591f9989a22a78e2684c55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We reshape this output to a tensor\nwith a size of T \u00d7F \u00d7C/F by splitting C channels into\nC/F groups, where each group has F frequency bins. We\ncall this tensor a Wavegram. The Wavegram learns frequency\ninformation by introducing F frequency bins in each of C/F\nchannels. We apply CNN14 described in Section II-A as a\nbackbone architecture on the extracted Wavegram, so that we\ncan fairly compare the Wavegram and log mel spectrogram\nbased systems. Two dimensional CNNs such as CNN14 can\ncapture time-frequency invariant patterns on the Wavegram,\nbecause kernels are convolved along both time and frequency\naxis in a Wavegram.\nB. Wavegram-Logmel-CNN\nFurthermore, we can combine the Wavegram and log mel\nspectrogram into a new representation. In this way, we can\nutilize the information from both time-domain waveforms and\nlog mel spectrograms. The combination is carried out along the\nchannel dimension. The Wavegram provides extra information\nfor audio tagging, complementing the log mel spectrogram.\nFig. 1 shows the architecture of the Wavegram-Logmel-CNN.", "mimetype": "text/plain", "start_char_idx": 3736, "end_char_idx": 4794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77ae92bc-aa0f-4d7f-b84b-7e8b857b8850": {"__data__": {"id_": "77ae92bc-aa0f-4d7f-b84b-7e8b857b8850", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2", "node_type": "1", "metadata": {}, "hash": "a42cb0bb15977e55fc2a562ef56d1fd9d42e85468faf5d320cb575cc09ab3f7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\nIV. D ATA PROCESSING\nIn this section, we introduce data processing for AudioSet\ntagging, including data balancing and data augmentation. Data\nbalancing is a technique used to train neural networks on a\nhighly unbalanced dataset. Data augmentation is a technique\nused to augment the dataset, to prevent systems from over\ufb01t-\nting during training.\nA. Data balancing\nThe number of audio clips available for training varies\nfrom sound class to sound class. For example, there are over\n900,000 audio clips belonging to the categories \u201cSpeech\u201d and\n\u201cMusic\u201d. On the other hand, there are only tens of audio clips\nbelonging to the category \u201cToothbrush\u201d. The number of audio\nclips of different sound classes has a long tailed distribution.\nTraining data are input to a PANN in mini-batches during\ntraining. Without a data balancing strategy, audio clips are\nuniformly sampled from AudioSet. Therefore, sound classes\nwith more training clips such as \u201cSpeech\u201d are more likely\nto be sampled during training. In an extreme case, all data\nin a mini-batch may belong to the same sound class. This\nwill cause the PANN to over\ufb01t to sound classes with more\ntraining clips, and under\ufb01t to sound classes with fewer training\nclips. To solve this problem, we design a balanced sampling\nstrategy to train PANNs. That is, audio clips are approximately\nequally sampled from all sound classes to constitute a mini-\nbatch. We use the term \u201capproximately\u201d because an audio clip\nmay contain more than one tag.\nB. Data augmentation\nData augmentation is a useful way to prevent a system from\nover\ufb01tting.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2": {"__data__": {"id_": "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77ae92bc-aa0f-4d7f-b84b-7e8b857b8850", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "971935d3834805a0e2c0291aed017f05f0c93cd12da6a5535849459295f6e3e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc893f8-6257-47c9-814c-742e3dd11bbd", "node_type": "1", "metadata": {}, "hash": "fc8e2b3335422f02bab35231f6f74ec9eb11d40555aac458c98e5f713c18e738", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To solve this problem, we design a balanced sampling\nstrategy to train PANNs. That is, audio clips are approximately\nequally sampled from all sound classes to constitute a mini-\nbatch. We use the term \u201capproximately\u201d because an audio clip\nmay contain more than one tag.\nB. Data augmentation\nData augmentation is a useful way to prevent a system from\nover\ufb01tting. Some sound classes in AudioSet contain only a\nsmall number (e.g., hundreds) of training clips which may\nlimit the performance of PANNs. We apply mixup [44] and\nSpecAugment [45] to augment data during training.\n1) Mixup: Mixup [44] is a way to augment a dataset by\ninterpolating both the input and target of two audio clips from\na dataset. For example, we denote the input of two audio clips\nas x1,x2, and their targets as y1,y2, respectively. Then, the\naugmented input and target can be obtained by x= \u03bbx1 +(1\u2212\n\u03bb)x2 and y= \u03bby1 +(1\u2212\u03bb)y2 respectively, where \u03bbis sampled\nfrom a Beta distribution [44]. By default, we apply mixup on\nthe log mel spectrogram. We will compare the performance of\nmixup augmentation on the log mel spectrogram and on the\ntime-domain waveform in Section VI-C4.\n2) SpecAugment: SpecAugment [45] was proposed for aug-\nmenting speech data for speech recognition. SpecAugment\noperates on the log mel spectrogram of an audio clip using\nfrequency masking and time masking.", "mimetype": "text/plain", "start_char_idx": 1211, "end_char_idx": 2563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbc893f8-6257-47c9-814c-742e3dd11bbd": {"__data__": {"id_": "cbc893f8-6257-47c9-814c-742e3dd11bbd", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "7350759c7785e05944f7a4dec1b441ebb3f9a7b1f13be9f2199a819a7b22835d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92f81b41-7398-47ab-b21c-9a2ba02950eb", "node_type": "1", "metadata": {}, "hash": "f9daa47238992ef63aca97268d620672b290455c9b19ac06cfb8cdcb37f4695e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By default, we apply mixup on\nthe log mel spectrogram. We will compare the performance of\nmixup augmentation on the log mel spectrogram and on the\ntime-domain waveform in Section VI-C4.\n2) SpecAugment: SpecAugment [45] was proposed for aug-\nmenting speech data for speech recognition. SpecAugment\noperates on the log mel spectrogram of an audio clip using\nfrequency masking and time masking. Frequency masking is\napplied such that f consecutive mel frequency bins [f0,f0 +f]\nare masked, where f is chosen from a uniform distribution\nfrom 0 to a frequency mask parameter f\u2032, and f0 is chosen\nfrom [0,F \u2212f], where F is the number of mel frequency bins\n[45]. There can be more than one frequency mask in each\nlog mel spectrogram. The frequency mask can improve the\nrobustness of PANNs to frequency distortion of audio clips\n[45]. Time masking is similar to frequency masking, but is\napplied in the time domain.", "mimetype": "text/plain", "start_char_idx": 2172, "end_char_idx": 3079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92f81b41-7398-47ab-b21c-9a2ba02950eb": {"__data__": {"id_": "92f81b41-7398-47ab-b21c-9a2ba02950eb", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc893f8-6257-47c9-814c-742e3dd11bbd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "840272b589b25661763d941f36e27c216427ec595dc5ed60e8c7672f4db84821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "600a60da-6553-4936-92c4-9d515c550c99", "node_type": "1", "metadata": {}, "hash": "7dcce5a1e0caabadcc13e8a279623eec88231ef18b16863462c89d35d494fcb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There can be more than one frequency mask in each\nlog mel spectrogram. The frequency mask can improve the\nrobustness of PANNs to frequency distortion of audio clips\n[45]. Time masking is similar to frequency masking, but is\napplied in the time domain.\n(\ud835\udc65\u0b34,\ud835\udc66\u0b34) \u2208\ud835\udc37\u0b45\u0b73\u0b62\u0b67\u0b6d\u0b57\u0b63\u0b72(\ud835\udc65\u0b35,\ud835\udc66\u0b35) \u2208\ud835\udc37\u0b52\u0b63\u0b75\u0b58\u0b5f\u0b71\u0b69(\ud835\udc65\u0b36,\ud835\udc66\u0b36) \u2208\ud835\udc37\u0b52\u0b63\u0b75\u0b58\u0b5f\u0b71\u0b69\ud835\udc65\u0b34NNEmbeddingFCAudioSet\ud835\udc66\u0b34\ud835\udc65\u0b35NNEmbeddingFCNewTask\ud835\udc66\u0b35Copy parameters\ud835\udc65\u0b36NNEmbeddingFCNewTask\ud835\udc66\u0b36Initialize parameters(a)(b)(c)\nFig. 2. (a) A PANN is pretrained with the AudioSet dataset. (b) For a new\ntask, the PANN is used as a feature extractor. A classi\ufb01er is built on the\nextracted embedding features. The shaded rectangle indicates the parameters\nare frozen and not trained. (c) For a new task, the parameters of a neural\nnetwork are initialized with a PANN. Then, all parameters are \ufb01ne-tuned on\nthe new task.\nV. T RANSFER TO OTHER TASKS\nTo investigate the generalization ability of PANNs, we\ntransfer PANNs to a range of audio pattern recognition tasks.", "mimetype": "text/plain", "start_char_idx": 2828, "end_char_idx": 3784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "600a60da-6553-4936-92c4-9d515c550c99": {"__data__": {"id_": "600a60da-6553-4936-92c4-9d515c550c99", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92f81b41-7398-47ab-b21c-9a2ba02950eb", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "02d3706b7aa18b206bd99b1d8f4853726e447942d9ea72df73268b98e62fb89e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "546b58de-8f1c-4e2c-997d-db783d9c972d", "node_type": "1", "metadata": {}, "hash": "446443130b8b62d6b53e382a96619c43e5cf313fa8877ff7d4132cb3e3cdd6d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The shaded rectangle indicates the parameters\nare frozen and not trained. (c) For a new task, the parameters of a neural\nnetwork are initialized with a PANN. Then, all parameters are \ufb01ne-tuned on\nthe new task.\nV. T RANSFER TO OTHER TASKS\nTo investigate the generalization ability of PANNs, we\ntransfer PANNs to a range of audio pattern recognition tasks.\nPrevious works on audio transfer learning [21][14][22][25][23]\nmainly focused on music tagging, and were limited to smaller\ndatasets than AudioSet. To begin with, we demonstrate the\ntraining of a PANN in Fig. 2(a). Here,DAudioSet is the AudioSet\ndataset, and x0, y0 are training input and target, respectively.\nFCAudioSet is the fully connected layer for AudioSet tagging.\nIn this article, we propose to compare the following transfer\nlearning strategies.\n1) Train a system from scratch. All parameters are randomly\ninitialized. Systems are similar to PANNs, except for the \ufb01nal\nfully-connected layer which depends on the task dependent\nnumber of outputs. This system is used as a baseline system\nto be compared with other transfer learning systems.\n2) Use a PANN as a feature extractor. For new tasks, the\nembedding features of audio clips are calculated by using the\nPANN. Then, the embedding features are used as input to\na classi\ufb01er, such as a fully-connected neural network. When\ntraining on new tasks, the parameters of the PANN are frozen\nand not trained. Only the parameters of the classi\ufb01er built\non the embedding features are trained. Fig.", "mimetype": "text/plain", "start_char_idx": 3430, "end_char_idx": 4934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "546b58de-8f1c-4e2c-997d-db783d9c972d": {"__data__": {"id_": "546b58de-8f1c-4e2c-997d-db783d9c972d", "embedding": null, "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ce6db409b819433fbeef8cea1cf71fe8c60e3dcee1b67bd50d9298b0f680f737", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "600a60da-6553-4936-92c4-9d515c550c99", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a3d7d3df5754ec3e4ecfefbe295c12846ed8c2333f8967ff677270f8c7042210", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For new tasks, the\nembedding features of audio clips are calculated by using the\nPANN. Then, the embedding features are used as input to\na classi\ufb01er, such as a fully-connected neural network. When\ntraining on new tasks, the parameters of the PANN are frozen\nand not trained. Only the parameters of the classi\ufb01er built\non the embedding features are trained. Fig. 2(b) shows this\nstrategy, where DNewTask is a new task dataset, and FC NewTask\nis the fully connected layer of a new task. The PANN is used as\na feature extractor. A classi\ufb01er is built on the extracted embed-\nding features. The shaded rectangle indicates the parameters\nwhich are frozen and not trained.\n(3) Fine-tune a PANN. A PANN is used for a new task,\nexcept the \ufb01nal fully-connected layer. All parameters are\ninitialized from the PANN, except the \ufb01nal fully-connected\nlayer which is randomly initialized. All parameters are \ufb01ne-\ntuned on DNewTask. Fig. 2(c) demonstrates the \ufb01ne-tuning of a\nPANN.\nVI. E XPERIMENTS\nFirst, we evaluate the performance of PANNs on AudioSet\ntagging. Then, the PANNs are transferred to several audio pat-\ntern recognition tasks, including acoustic scene classi\ufb01cation,\ngeneral audio tagging, music classi\ufb01cation and speech emotion\nclassi\ufb01cation.", "mimetype": "text/plain", "start_char_idx": 4573, "end_char_idx": 5814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5db35dee-7104-4d4d-916f-21047cc962f5": {"__data__": {"id_": "5db35dee-7104-4d4d-916f-21047cc962f5", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89915979-a4a9-43cb-b367-1a7e00521701", "node_type": "1", "metadata": {}, "hash": "ac076bef2d183957fca166961015a5de554a4b7b1e01b90d58a89deb041e07e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\n0 200k 400k 600k\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0AP\nMusic (937,780)\nSpeech (937,432)\nVehicle (122,231)\n Musical (112,337)\n Inside, small (70,159)\nGuitar (49,200)\n Plucked string (42,509)\nCar (39,503)\nSinging (39,469)\nAnimal (37,857)\n0 200k 400k 600k\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0AP\n Canidae, dogs, (1,592)\n Traditional (1,579)\nTabla (1,575)\nSad music (1,572)\nSka (1,563)\nChant (1,559)\nScary music (1,539)\nLiquid (1,533)\n Traffic noise, (1,523)\nCarnatic music (1,516)\n0 200k 400k 600k\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0AP\nWheeze (114)\nHoot (106)\nCrushing (101)\nFinger snapping (98)\nSquawk (96)\nPulleys (88)\nSplinter (88)\nCreak (87)\nGargling (69)\nToothbrush (61)\nFig. 3. Class-wise AP of sound events with the CNN14 system. The number inside parentheses indicates the number of training clips. The left, middle, right\ncolumns show the AP of sound classes with the number of training clips ranked the 1st to 10th, 250th to 260th and 517th to 527th in the training set of\nAudioSet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89915979-a4a9-43cb-b367-1a7e00521701": {"__data__": {"id_": "89915979-a4a9-43cb-b367-1a7e00521701", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5db35dee-7104-4d4d-916f-21047cc962f5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "2fff3b5696ca18765c1798e0573f4a20134570f4bc9dd05be75743c54f91a434", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0", "node_type": "1", "metadata": {}, "hash": "1257019e0be4eb12d870cb1240ea65403c71361b8ab5ee462bc431d6180cdb3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. Class-wise AP of sound events with the CNN14 system. The number inside parentheses indicates the number of training clips. The left, middle, right\ncolumns show the AP of sound classes with the number of training clips ranked the 1st to 10th, 250th to 260th and 517th to 527th in the training set of\nAudioSet.\nA. AudioSet dataset\nAudioSet is a large-scale audio dataset with an ontology\nof 527 sound classes [1]. The audio clips from AudioSet are\nextracted from YouTube videos. The training set consists of\n2,063,839 audio clips, including a \u201cbalanced subset\u201d of 22,160\naudio clips, where there are at least 50 audio clips for each\nsound class. The evaluation set consists of 20,371 audio clips.\nInstead of using the embedding features provided by [1], we\ndownloaded raw audio waveforms of AudioSet in Dec. 2018\nusing the links provided by [1], and ignored the audio clips\nthat are no longer downloadable. We successfully download\n1,934,187 (94%) of the audio clips of the full training set,\nincluding 20,550 (93%) of the audio clips of the balanced\ntraining set. We successfully download 18,887 audio clips of\nthe evaluation dataset. We pad the audio clips to 10 seconds\nwith silence if they are shorter than 10 seconds. Considering\nthe fact that a large number of audio clips from YouTube are\nmonophonic and have a low sampling rate, we convert all\naudio clips to monophonic and resample them to 32 kHz.\nFor the CNN systems based on log mel spectrograms, STFT\nis applied on the waveforms with a Hamming window of size\n1024 [33] and a hop size of 320 samples.", "mimetype": "text/plain", "start_char_idx": 675, "end_char_idx": 2237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0": {"__data__": {"id_": "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89915979-a4a9-43cb-b367-1a7e00521701", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9a3d3d30ba893aa9e983acb2c1c298c7fecff7251dcadcd6df390fcccaf46497", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb33725a-8637-49b0-8f18-d9bd8abce73b", "node_type": "1", "metadata": {}, "hash": "819ea2d212c12c28077506b656e9499b9783a16ec52af56c640ea513066a11b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Considering\nthe fact that a large number of audio clips from YouTube are\nmonophonic and have a low sampling rate, we convert all\naudio clips to monophonic and resample them to 32 kHz.\nFor the CNN systems based on log mel spectrograms, STFT\nis applied on the waveforms with a Hamming window of size\n1024 [33] and a hop size of 320 samples. This con\ufb01guration\nleads to 100 frames per second. Following [33], we apply 64\nmel \ufb01lter banks to calculate the log mel spectrogram. The\nlower and upper cut-off frequencies of the mel banks are\nset to 50 Hz and 14 kHz to remove low frequency noise\nand the aliasing effects. We use the torchlibrosa1, a PyTorch\nimplementation of functions of librosa [46] to build log mel\nspectrogram extraction into PANNs. The log mel spectrogram\nof a 10-second audio clip has a shape of 1001 \u00d764. The\nextra one frame is caused by applying the \u201ccentre\u201d argument\nwhen calculating STFT. A batch size of 32, and an Adam [47]\noptimizer with a learning rate of 0.001 is used for training.\nSystems are trained on a single card Tesla-V100-PCIE-32GB.\nEach system takes around 3 days to train from scratch for 600\nk iterations.\nB. Evaluation metrics\nMean average precision (mAP), mean area under the curve\n(mAUC) and d-prime are used as of\ufb01cial evaluation metrics\nfor AudioSet tagging [20][1].", "mimetype": "text/plain", "start_char_idx": 1899, "end_char_idx": 3204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb33725a-8637-49b0-8f18-d9bd8abce73b": {"__data__": {"id_": "bb33725a-8637-49b0-8f18-d9bd8abce73b", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cd067d550fa4f0ae98e656a6dc5d3450c019549e978b24ffa8ba046e4cba3de1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc", "node_type": "1", "metadata": {}, "hash": "1ab345c24b80c531626473b9e7fe83feb3e5acfc019ccb6788e931bacc65fea1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Systems are trained on a single card Tesla-V100-PCIE-32GB.\nEach system takes around 3 days to train from scratch for 600\nk iterations.\nB. Evaluation metrics\nMean average precision (mAP), mean area under the curve\n(mAUC) and d-prime are used as of\ufb01cial evaluation metrics\nfor AudioSet tagging [20][1]. Average precision (AP) is the\n1https://github.com/qiuqiangkong/torchlibrosa\nTABLE IV\nCOMPARISON WITH PREVIOUS METHODS\nmAP AUC d-prime\nRandom guess 0.005 0.500 0.000\nGoogle CNN [1] 0.314 0.959 2.452\nSingle-level attention [16] 0.337 0.968 2.612\nMulti-level attention [17] 0.360 0.970 2.660\nLarge feature-level attention [20] 0.369 0.969 2.640\nTAL Net [19] 0.362 0.965 2.554\nDeepRes [48] 0.392 0.971 2.682\nOur proposed CNN14 0.431 0.973 2.732\narea under the recall and precision curve. AP does not depend\non the number of true negatives, because neither precision nor\nrecall depends on the number of true negatives. On the other\nhand, AUC is the area under the false positive rate and true\npositive rate (recall) which re\ufb02ects the in\ufb02uence of the true\nnegatives. The d-prime [1] is also used as an metric and be\ncalculated directly from AUC [1]. All metrics are calculated\non individual classes, and then averaged across all classes.\nThose metrics are also called macro metrics.", "mimetype": "text/plain", "start_char_idx": 2904, "end_char_idx": 4181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc": {"__data__": {"id_": "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb33725a-8637-49b0-8f18-d9bd8abce73b", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "1a64dcf19cd9037d7931f907968fb00ae5ab1e1bd20785f028ef24564f62c6e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bf48078-f2ea-493c-af47-45e803f76b5b", "node_type": "1", "metadata": {}, "hash": "b2ffdc6785cf46bacd5996782a21f7ae10aed84eea8b8f1bef0bdf42e0fb4e4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other\nhand, AUC is the area under the false positive rate and true\npositive rate (recall) which re\ufb02ects the in\ufb02uence of the true\nnegatives. The d-prime [1] is also used as an metric and be\ncalculated directly from AUC [1]. All metrics are calculated\non individual classes, and then averaged across all classes.\nThose metrics are also called macro metrics.\nC. AudioSet tagging results\n1) Comparison with previous methods: Table IV shows the\ncomparison of our proposed CNN14 system with previous Au-\ndioSet tagging systems. We choose CNN14 as a basic model\nto investigate a various of hyper-parameter con\ufb01gurations for\nAudioSet tagging, because CNN14 is a standard CNN that\nhas a simple architecture, and can be compared with previous\nCNN systems [3][33]. As a baseline, random guess achieves\nan mAP of 0.005, an AUC of 0.500 and a d-prime of 0.000,\nrespectively. The result released by Google [1] trained with\nembedding features from [13] achieved an mAP of 0.314\nand an AUC of 0.959, respectively. The single-level attention\nand multi-level attention systems [16], [17] achieved mAPs\nof 0.337 and 0.360, which were later improved by a feature-\nlevel attention neural network that achieved an mAP of 0.369.\nWang et al. [19] investigated \ufb01ve different types of attention\nfunctions and achieved an mAP of 0.362. All the above\nsystems were built on the embedding features released with\nAudioSet [1].", "mimetype": "text/plain", "start_char_idx": 3819, "end_char_idx": 5221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bf48078-f2ea-493c-af47-45e803f76b5b": {"__data__": {"id_": "0bf48078-f2ea-493c-af47-45e803f76b5b", "embedding": null, "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f4f2040fd6181d0e01dd031284d8dfcd58aa0fcecfe244ca993c278c984c220c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "60b3d70749c27a8028f41a2f8273042f4372ecdb3f3abd92c5012621d890bd71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wang et al. [19] investigated \ufb01ve different types of attention\nfunctions and achieved an mAP of 0.362. All the above\nsystems were built on the embedding features released with\nAudioSet [1]. The recent DeepRes system [48] was built on\nwaveforms downloaded from YouTube, and achieved an mAP\nof 0.392. The bottom rows of Table IV shows our proposed", "mimetype": "text/plain", "start_char_idx": 5032, "end_char_idx": 5377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9189a21b-108c-4167-bc7b-a172abf40ac2": {"__data__": {"id_": "9189a21b-108c-4167-bc7b-a172abf40ac2", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fd3458f-f365-4f58-b487-e3e73c098fda", "node_type": "1", "metadata": {}, "hash": "3dcbc07bd0eaa919dc34371a51c4fc6615919d95912c9e2d4afd7a2aee1d244d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(a) Comparison of architectures\nWavegram-Logmel-CNN\nCNN14\nMobileNetV1\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(b) Comparison of training data and augmentation\nCNN14,bal,mixup (1.9m)\nCNN14,bal,mixup-wav (1.9m)\nCNN14,bal,no-mixup (1.9m)\nCNN14,no-bal,no-mixup (1.9m)\nCNN14,bal,mixup (20k)\nCNN14,bal,no-mixup (20k)\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(c) Comparison of embedding size\nCNN14,emb=2048\nCNN14,emb=128\nCNN14,emb=32\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fd3458f-f365-4f58-b487-e3e73c098fda": {"__data__": {"id_": "8fd3458f-f365-4f58-b487-e3e73c098fda", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9189a21b-108c-4167-bc7b-a172abf40ac2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "3da57afd0ae70cafe6ceec8d15f7acdf924208e076012a08718af1793212b610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "546859c1-ef43-4741-beed-59ab66a07fb9", "node_type": "1", "metadata": {}, "hash": "dc7f22fe9031a59f3148d1ae22c362144d00fc6e851fb71331a5382175d9e839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4\n0.5\n0.6\n0.7\n0.8mAP\n(c) Comparison of embedding size\nCNN14,emb=2048\nCNN14,emb=128\nCNN14,emb=32\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(d) Comparison of amount of training data\nCNN14 (100% full)\nCNN14 (80% full)\ncnn14 (50% full)\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(e) Comparison of sampling rate\nCNN14,32kHz\nCNN14,16kHz\nCNN14,8kHz\n0 100k 200k 300k 400k 500k\nIterations\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mAP\n(f) Comparison of mel bins number\nCNN14,128-melbins\nCNN14,64-melbins\nCNN14,32-melbins\nFig. 4. Results of PANNs on AudioSet tagging. The transparent and solid lines are training mAP and evaluation mAP, respectively. The six plots show\nthe results with different: (a) architectures; (b) data balancing and data augmentation; (c) embedding size; (d) amount of training data; (e) sampling rate; (f)\nnumber of mel bins.", "mimetype": "text/plain", "start_char_idx": 477, "end_char_idx": 1373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "546859c1-ef43-4741-beed-59ab66a07fb9": {"__data__": {"id_": "546859c1-ef43-4741-beed-59ab66a07fb9", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fd3458f-f365-4f58-b487-e3e73c098fda", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "23bf52d5918417af378df40586d9e25ab4fe7fe7e52c8646abde65b0e0f7831e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf", "node_type": "1", "metadata": {}, "hash": "b30456b03740189ddc3a5f70f74bde31754f786597628a73068a92e67c37a0ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "64-melbins\nCNN14,32-melbins\nFig. 4. Results of PANNs on AudioSet tagging. The transparent and solid lines are training mAP and evaluation mAP, respectively. The six plots show\nthe results with different: (a) architectures; (b) data balancing and data augmentation; (c) embedding size; (d) amount of training data; (e) sampling rate; (f)\nnumber of mel bins.\nCNN14 system achieves an mAP of 0.431, outperforming the\nbest of previous systems. We use CNN14 as a backbone to\nbuild Wavegram-Logmel-CNN for fair comparison with the\nCNN14 system. Fig. 4(a) shows that Wavegram-Logmel-CNN\noutperforms the CNN14 system, and the MobileNetV1 system.\nDetailed results are shown in later this section in Table XI.\n2) Class-wise performance: Fig. 3 shows the class-wise\nAP of different sound classes with the CNN14 system. The\nleft, middle, right columns show the AP of sound classes\nwith the number of training clips ranked the 1st to 10th,\n250th to 260th and 517th to 527th in the training set of\nAudioSet. The performance of different sound classes can be\nvery different. For example, \u201cMusic\u201d and \u201cSpeech\u201d achieve\nAPs of over 0.80. On the other hand, some sound classes\nsuch as \u201cInside, small\u201d achieve an AP of only 0.19. Fig. 3\nshows that APs are usually not correlated with the number\nof training clips. For example, the left column shows that\n\u201cInside, small\u201d contains 70,159 training clips, while its AP\nis low.", "mimetype": "text/plain", "start_char_idx": 1017, "end_char_idx": 2419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf": {"__data__": {"id_": "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "546859c1-ef43-4741-beed-59ab66a07fb9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "188eee6d9729531ed4bb263ddc0f14a775e10f1381560ca10a2ff7e056992696", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db5318e8-ab2d-413f-98dc-2ef577a257fa", "node_type": "1", "metadata": {}, "hash": "66166579b430f905b2a46735f220d78bbf8c626d1ee0874f43d0c17aa7c33c14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, some sound classes\nsuch as \u201cInside, small\u201d achieve an AP of only 0.19. Fig. 3\nshows that APs are usually not correlated with the number\nof training clips. For example, the left column shows that\n\u201cInside, small\u201d contains 70,159 training clips, while its AP\nis low. In contrast, the right column shows that \u201cHoot\u201d only\nhas 106 training clips, but achieves an AP of 0.86, and is larger\nthan many other sound classes with more training clips. In the\nend of this article, we plot the mAP of all 527 sound classes\nin Fig. 12, which shows the class-wise comparison of the\nCNN14, MobileNetV1 and Wavegram-Logmel-CNN systems\nwith previous state-of-the-art audio tagging system [20] built\nwith embedding features released by [1]. The blue bars in Fig.\n12 show the number of training clips in logarithmic scale. The\n\u201c+\u201d symbol indicates label qualities between 0 and 1, which\nare measured by the percentage of correctly labelled audio\nclips veri\ufb01ed by an expert [1]. The label quality vary from\nsound class to sound class.", "mimetype": "text/plain", "start_char_idx": 2137, "end_char_idx": 3167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db5318e8-ab2d-413f-98dc-2ef577a257fa": {"__data__": {"id_": "db5318e8-ab2d-413f-98dc-2ef577a257fa", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "bb7c8d2d21625f91ea0b77359e243a38b923039ce92065fc5eb9ee4e75d2c212", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5540dae0-4f31-4352-98a8-6767675b0fcb", "node_type": "1", "metadata": {}, "hash": "14d5f240009ed1b125e4ef71ff71fd47354c88954bbddac804e6369af37060f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue bars in Fig.\n12 show the number of training clips in logarithmic scale. The\n\u201c+\u201d symbol indicates label qualities between 0 and 1, which\nare measured by the percentage of correctly labelled audio\nclips veri\ufb01ed by an expert [1]. The label quality vary from\nsound class to sound class. The \u201c \u2212\u201d symbol indicates the\nTABLE V\nRESULTS WITH DATA BALANCING AND AUGMENTATION\nAugmentation mAP AUC d-prime\nno-bal,no-mixup (20k) 0.224 0.894 1.763\nbal,no-mixup (20k) 0.221 0.879 1.652\nbal,mixup (20k) 0.278 0.905 1.850\nno-bal,no-mixup (1.9m) 0.375 0.971 2.690\nbal,no-mixup (1.9m) 0.416 0.968 2.613\nbal,mixup (1.9m) 0.431 0.973 2.732\nbal,mixup-wav (1.9m) 0.425 0.973 2.720\nTABLE VI\nRESULTS OF DIFFERENT HOP SIZES\nHop size Time resolution mAP AUC d-prime\n1000 31.25 ms 0.400 0.969 2.645\n640 20.00 ms 0.417 0.972 2.711\n500 15.63 ms 0.417 0.971 2.682\n320 10.00 ms 0.431 0.973 2.732\nsound classes whose label quality are not available. Fig. 12\nshows that the average precisions of some classes are higher\nthan others.", "mimetype": "text/plain", "start_char_idx": 2876, "end_char_idx": 3884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5540dae0-4f31-4352-98a8-6767675b0fcb": {"__data__": {"id_": "5540dae0-4f31-4352-98a8-6767675b0fcb", "embedding": null, "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "36ab3d5576e68d2313fed712e9d895bf214c27b14f4df8059f5332c7947e50f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db5318e8-ab2d-413f-98dc-2ef577a257fa", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "73cf84962288f6eda9ef53895b2052a5b8f4cb81ba65b46dcdfc1045468ec36f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 12\nshows that the average precisions of some classes are higher\nthan others. For example, sound classes such as \u201cbagpipes\u201d\nachieve an average precision of 0.90, while sound classes\nsuch as \u201cmouse\u201d achieve an average precision less than 0.2.\nOne explanation is that the audio tagging dif\ufb01culty is different\nbetween sound class to sound class. In addition, audio tagging\nperformance is not always correlated with the number of\ntraining clips and label qualities [20]. Fig. 12 shows that our\nproposed systems outperform previous state-of-the-art systems\n[16], [17] across a wide range of sound classes.", "mimetype": "text/plain", "start_char_idx": 3803, "end_char_idx": 4407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef9f03bc-8255-41cc-a32a-3c0e401d89a2": {"__data__": {"id_": "ef9f03bc-8255-41cc-a32a-3c0e401d89a2", "embedding": null, "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a0a835dba05b81ac7b67b55b9c776798ebf19dd92a6a8734abc23d92ec7073a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e058be7d-e2d2-494e-b923-de8e15b59f8b", "node_type": "1", "metadata": {}, "hash": "e528c9f790d84c16a9ca9dadd99bf5ce65b88f18bacc2f22d361fa89909071be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8\nTABLE VII\nRESULTS OF DIFFERENT EMBEDDING DIMENSIONS\nEmbedding mAP AUC d-prime\n32 0.364 0.958 2.437\n128 0.412 0.969 2.634\n512 0.420 0.971 2.689\n2048 0.431 0.973 2.732\nTABLE VIII\nRESULTS OF PARTIAL TRAINING DATA\nTraining data mAP AUC d-prime\n50% of full 0.406 0.964 2.543\n80% of full 0.426 0.971 2.677\n100% of full 0.431 0.973 2.732\n3) Data balancing: Section IV-A introduces the data bal-\nancing technique that we use to train AudioSet tagging sys-\ntems. Fig. 4(b) shows the performance of the CNN14 system\nwith and without data balancing. The blue curve shows that\nit takes a long time to train PANNs without data balancing.\nThe green curve shows that with data balancing, a system\nconverges faster within limited training iterations. In addition,\nthe systems trained with full 1.9 million training clips perform\nbetter than the systems trained with the balanced subset of 20k\ntraining clips. Table V shows that the CNN14 system achieves\nan mAP of 0.416 with data balancing, higher than that without\ndata balancing (0.375).\n4) Data augmentation: We show that mixup data augmen-\ntation plays an important role in training PANNs. By default,\nwe apply mixup on the log mel spectrogram. Fig.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e058be7d-e2d2-494e-b923-de8e15b59f8b": {"__data__": {"id_": "e058be7d-e2d2-494e-b923-de8e15b59f8b", "embedding": null, "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a0a835dba05b81ac7b67b55b9c776798ebf19dd92a6a8734abc23d92ec7073a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef9f03bc-8255-41cc-a32a-3c0e401d89a2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4b3d072a7d38cb3da62e48f0a9040c0496b38d4d23f7ab77e2e324092d6c32d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7e14b8b-c189-47ea-a7f5-7b54899af439", "node_type": "1", "metadata": {}, "hash": "bb59b70f4312a6a7e4add9ff4ab17baf5db2d2213ec1e11d5f00e333d6f271c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table V shows that the CNN14 system achieves\nan mAP of 0.416 with data balancing, higher than that without\ndata balancing (0.375).\n4) Data augmentation: We show that mixup data augmen-\ntation plays an important role in training PANNs. By default,\nwe apply mixup on the log mel spectrogram. Fig. 4(b) and\nTable V shows that the CNN14 system trained with mixup\ndata augmentation achieves an mAP of 0.431, outperforming\nthat trained without mixup data augmentation (0.416). Mixup\nis especially useful when training with the balanced subset\ncontaining only 20k training clips, yielding an mAP of 0.278,\ncompared to training without mixup (0.221). In addition, we\nshow that mixup on the log mel spectrogram achieves an mAP\nof 0.431, outperforming mixup in the time-domain waveform\nof 0.425, when training with full training data. This suggests\nthat mixup is more effective when used with the log mel\nspectrogram than with the time-domain waveform.\n5) Hop sizes: The hop size is the number of samples\nbetween adjacent frames. A small hop size leads to high\nresolution in the time domain. We investigate the in\ufb02uence\nof different hop sizes on AudioSet tagging with the CNN14\nsystem. We investigate hop sizes of 1000, 640, 500 and 320:\nthese correspond to time domain resolutions of 31.25 ms,\n20.00 ms, 15.63 ms and 10.00 ms between adjacent frames,\nrespectively. Table VI shows that the mAP score increases as\nhop sizes decrease.", "mimetype": "text/plain", "start_char_idx": 895, "end_char_idx": 2317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7e14b8b-c189-47ea-a7f5-7b54899af439": {"__data__": {"id_": "b7e14b8b-c189-47ea-a7f5-7b54899af439", "embedding": null, "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a0a835dba05b81ac7b67b55b9c776798ebf19dd92a6a8734abc23d92ec7073a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e058be7d-e2d2-494e-b923-de8e15b59f8b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "77d87a2f04197481e76406592183748e5b0a73c825d2db3a0416fd36d6283991", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe61e543-fef9-4dd6-9de7-75eb4c865356", "node_type": "1", "metadata": {}, "hash": "b1a2c0016cd359e49b4f830cb0811606c412da56a430e9e1dbc052b995d12d54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We investigate the in\ufb02uence\nof different hop sizes on AudioSet tagging with the CNN14\nsystem. We investigate hop sizes of 1000, 640, 500 and 320:\nthese correspond to time domain resolutions of 31.25 ms,\n20.00 ms, 15.63 ms and 10.00 ms between adjacent frames,\nrespectively. Table VI shows that the mAP score increases as\nhop sizes decrease. With a hop size of 320, the CNN14 system\nachieves an mAP of 0.431, outperforming the larger hop sizes\nof 500, 640 and 1000.\n6) Embedding dimensions: Embedding features are \ufb01xed-\nlength vectors that summarize audio clips. By default, the\nCNN14 has an embedding feature dimension of 2048. We\ninvestigate a rage of CNN14 systems with embedding dimen-\nTABLE IX\nRESULTS OF DIFFERENT SAMPLE RATES\nSample rate mAP AUC d-prime\n8 kHz 0.406 0.970 2.654\n16 kHz 0.427 0.973 2.719\n32 kHz 0.431 0.973 2.732\nTABLE X\nRESULTS OF DIFFERENT MEL BINS\nMel bins mAP AUC d-prime\n32 bins 0.413 0.971 2.691\n64 bins 0.431 0.973 2.732\n128 bins 0.442 0.973 2.735\nsions of 32, 128, 512 and 2048. Fig. 4(c) and Table VII show\nthat mAP performance increases as embedding dimension\nincreases.\n7) Training with partial data: The audio clips of AudioSet\nare sourced from YouTube.", "mimetype": "text/plain", "start_char_idx": 1977, "end_char_idx": 3163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe61e543-fef9-4dd6-9de7-75eb4c865356": {"__data__": {"id_": "fe61e543-fef9-4dd6-9de7-75eb4c865356", "embedding": null, "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a0a835dba05b81ac7b67b55b9c776798ebf19dd92a6a8734abc23d92ec7073a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7e14b8b-c189-47ea-a7f5-7b54899af439", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "dfe9c6c272cbf7109129a6293a763761ae009e04f626a0eb503c9e91911794b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d727d551-a1cc-4ed1-8997-0ab9b621c36f", "node_type": "1", "metadata": {}, "hash": "910282e74d0e4c13369e85b8ec8abd6fad3bca8266bef261465686d5c0ac6516", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 4(c) and Table VII show\nthat mAP performance increases as embedding dimension\nincreases.\n7) Training with partial data: The audio clips of AudioSet\nare sourced from YouTube. Same audio clips are no longer\ndownloadable, and others may be removed in the future. For\nbetter reproducibility of our work in future, we investigate the\nperformance of systems trained with randomly chosen partial\ndata ranging from 50% to 100% of our downloaded data. Fig.\n4(d) and Table VIII show that the mAP decreases slightly\nfrom 0.431 to 0.426 (a 1.2% drop) when the CNN14 system\nis trained with 80% of full data, and decreases to 0.406 (a\n5.8% drop) when trained with 50% of full data. This result\nshows that the amount of training data is important for training\nPANNs.\n8) Sample rate: Fig. 4(e) and Table IX show the perfor-\nmance of the CNN14 system trained with different sample\nrate. The CNN14 system trained with 16 kHz audio recordings\nachieves an mAP of 0.427, similar (within 1.0%) to the\nCNN14 system trained with a sample rate of 32 kHz. On\nthe other hand, the CNN14 system trained with 8 kHz audio\nrecordings achieves a lower mAP of 0.406 (5.8% lower). This\nindicates that information in the 4 kHz - 8 kHz range is useful\nfor audio tagging.\n9) Mel bins: Fig. 4(f) and Table X show the performance\nof the CNN14 system trained with different number of mel\nbins.", "mimetype": "text/plain", "start_char_idx": 2985, "end_char_idx": 4342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d727d551-a1cc-4ed1-8997-0ab9b621c36f": {"__data__": {"id_": "d727d551-a1cc-4ed1-8997-0ab9b621c36f", "embedding": null, "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a0a835dba05b81ac7b67b55b9c776798ebf19dd92a6a8734abc23d92ec7073a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe61e543-fef9-4dd6-9de7-75eb4c865356", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "183e030324780f3a33876700cd1327c48a90c9197e76086cbb5eb6beb41640ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On\nthe other hand, the CNN14 system trained with 8 kHz audio\nrecordings achieves a lower mAP of 0.406 (5.8% lower). This\nindicates that information in the 4 kHz - 8 kHz range is useful\nfor audio tagging.\n9) Mel bins: Fig. 4(f) and Table X show the performance\nof the CNN14 system trained with different number of mel\nbins. The system achieves an mAP of 0.413 with 32 mel\nbins, compared to 0.431 with 64 mel bins and 0.442 with\n128 mel bins. This result suggests that PANNs achieve better\nperformance with more mel bins, although the computation\ncomplexity increases linearly with the number of mel bins.\nThroughout this paper, we adopt 64 mel bins for extracting\nthe log mel spectrogram, as a trade-off between computational\ncomplexity and system performance.\n10) Number of CNN layers: We investigate the perfor-\nmance of CNN systems with 6, 10 and 14 layers, as de-\nscribed in Section II-A. Table XI shows that the 6-, 10- and\n14-layer CNNs achieve mAPs of 0.343, 0.380 and 0.431,\nrespectively. This result suggests that PANNs with deeper\nCNN architectures achieve better performance than shallower\nCNN architectures. This result is in contrast to previous audio\ntagging systems trained on smaller datasets where shallower", "mimetype": "text/plain", "start_char_idx": 4020, "end_char_idx": 5243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fe3f32a-fdd9-4270-8d1c-ae924f51db4a": {"__data__": {"id_": "5fe3f32a-fdd9-4270-8d1c-ae924f51db4a", "embedding": null, "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d82e-654b-4e6c-80ca-5a378f47741f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "6038a0e5c882b5b2f43234f143f2e9644be7d05265cf0f122cd3312c1cbcde25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31e29c65-2c1f-4769-8b50-c4b6b0d46aab", "node_type": "1", "metadata": {}, "hash": "ea90efe497a1b801cafe6d935529126482cd10ae912b4cb4de8a76f19d08e9a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9\nTABLE XI\nRESULTS OF DIFFERENT SYSTEMS\nArchitecture mAP AUC d-prime\nCNN6 0.343 0.965 2.568\nCNN10 0.380 0.971 2.678\nCNN14 0.431 0.973 2.732\nResNet22 0.430 0.973 0.270\nResNet38 0.434 0.974 2.737\nResNet54 0.429 0.971 2.675\nMobileNetV1 0.389 0.970 2.653\nMobileNetV2 0.383 0.968 2.624\nDaiNet [31] 0.295 0.958 2.437\nLeeNet11 [42] 0.266 0.953 2.371\nLeeNet24 0.336 0.963 2.525\nRes1dNet31 0.365 0.958 2.444\nRes1dNet51 0.355 0.948 2.295\nWavegram-CNN 0.389 0.968 2.612\nWavegram-Logmel-CNN 0.439 0.973 2.720\nCNNs such as 9-layer CNN performed better than deeper\nCNNs [33]. One possible explanation is that smaller datasets\nmay suffer from over\ufb01tting, while AudioSet is large enough\nto train deeper CNNs, at least up to the 14 layers CNNs that\nwe investigate.\n11) ResNets: We apply ResNets to investigate the perfor-\nmance of deeper PANNs. Table XI shows that the ResNet22\nsystem achieves an mAP of 0.430 similar to the CNN14\nsystem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31e29c65-2c1f-4769-8b50-c4b6b0d46aab": {"__data__": {"id_": "31e29c65-2c1f-4769-8b50-c4b6b0d46aab", "embedding": null, "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d82e-654b-4e6c-80ca-5a378f47741f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "6038a0e5c882b5b2f43234f143f2e9644be7d05265cf0f122cd3312c1cbcde25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fe3f32a-fdd9-4270-8d1c-ae924f51db4a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "b3257fd5f56199a937e14f4353c0bba94c08e3c72ebcddad731df7d5c67bca35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e68ee0c1-1035-4562-acff-2160207b1516", "node_type": "1", "metadata": {}, "hash": "0f9e16cb8bdf085a4d45a804fdddaf2662951659b1fe652f8f8cc3c14479eb0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One possible explanation is that smaller datasets\nmay suffer from over\ufb01tting, while AudioSet is large enough\nto train deeper CNNs, at least up to the 14 layers CNNs that\nwe investigate.\n11) ResNets: We apply ResNets to investigate the perfor-\nmance of deeper PANNs. Table XI shows that the ResNet22\nsystem achieves an mAP of 0.430 similar to the CNN14\nsystem. ResNet38 achieves an mAP of 0.434, slightly outper-\nforming other systems. ResNet54 achieves an mAP of 0.429,\nwhich does not further improve the performance.\n12) MobileNets: The systems mentioned above show that\nPANNs achieve good performance in AudioSet tagging. How-\never, those systems do not consider computational ef\ufb01ciency\nwhen implemented on portable devices. We investigate build-\ning PANNs with light weight MobileNets described in Section\nII-C. Table XI shows that MobileNetV1 achieves an mAP of\n0.389, 9.7% lower to the CNN14 system of 0.431. The number\nof multiplication and addition (multi-adds) and parameters\nof the MobileNetV1 system are only 8.6% and 5.9% of\nthe CNN14 system, respectively. The MobileNetV2 system\nachieves an mAP of 0.383, 11.1% lower than CNN14, and is\nmore computationally ef\ufb01cient than MobileNetV1, where the\nnumber of multi-adds and parameters are only 6.7% and 5.0%\nof the CNN14 system.\n13) One-dimensional CNNs: Table XI shows the perfor-\nmance of one-dimensional CNNs.", "mimetype": "text/plain", "start_char_idx": 562, "end_char_idx": 1931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e68ee0c1-1035-4562-acff-2160207b1516": {"__data__": {"id_": "e68ee0c1-1035-4562-acff-2160207b1516", "embedding": null, "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d82e-654b-4e6c-80ca-5a378f47741f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "6038a0e5c882b5b2f43234f143f2e9644be7d05265cf0f122cd3312c1cbcde25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31e29c65-2c1f-4769-8b50-c4b6b0d46aab", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "b1e17ca5f0042fa8ccb5d99182fe8f104cf5a50f9b0aefc3ca5d40f4c9deb91e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bacc7868-af4b-44eb-ac46-c5af82b6b737", "node_type": "1", "metadata": {}, "hash": "f0fd9dfb9dfc333a4efe682f196cc356253d38139a247b7c3e577133c8e029de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The MobileNetV2 system\nachieves an mAP of 0.383, 11.1% lower than CNN14, and is\nmore computationally ef\ufb01cient than MobileNetV1, where the\nnumber of multi-adds and parameters are only 6.7% and 5.0%\nof the CNN14 system.\n13) One-dimensional CNNs: Table XI shows the perfor-\nmance of one-dimensional CNNs. The DaiNet with 18 layers\n[31] achieves an mAP of 0.295. The LeeNet11 with 11 layers\n[42] achieves an mAP of 0.266. Our improved LeeNet with\n24 layers improves the mAP of LeeNet11 to 0.336. Our\nproposed Res1dNet31 and Res1dNet51 described in Section\nII-D3 achieve mAPs of 0.365 and 0.355 respectively, and\nachieve state-of-the-art performance among one-dimensional\nCNN systems.\n14) Wavegram-Logmel-CNN: The bottom rows of Table\nXI show the result of our proposed Wavegram-CNN and\nWavegram-Logmel-CNN systems. The Wavegram-CNN sys-\ntem achieves an mAP of 0.389, outperforming the best\nprevious one-dimensional CNN system (Res1dNet31).", "mimetype": "text/plain", "start_char_idx": 1630, "end_char_idx": 2565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bacc7868-af4b-44eb-ac46-c5af82b6b737": {"__data__": {"id_": "bacc7868-af4b-44eb-ac46-c5af82b6b737", "embedding": null, "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d82e-654b-4e6c-80ca-5a378f47741f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "6038a0e5c882b5b2f43234f143f2e9644be7d05265cf0f122cd3312c1cbcde25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e68ee0c1-1035-4562-acff-2160207b1516", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "93308d08dbb63c946d62b70225b377277df4921591a99c0cd158271050abba79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f5319cc-f3be-406b-8958-2da24b49901e", "node_type": "1", "metadata": {}, "hash": "f3d26317abe9285abdd1a881742273fa1e18127d8f9a58380db539adc8f0c939", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14) Wavegram-Logmel-CNN: The bottom rows of Table\nXI show the result of our proposed Wavegram-CNN and\nWavegram-Logmel-CNN systems. The Wavegram-CNN sys-\ntem achieves an mAP of 0.389, outperforming the best\nprevious one-dimensional CNN system (Res1dNet31). This\nTABLE XII\nNUMBER OF MULTI -ADDS AND PARAMETERS OF DIFFERENT SYSTEMS\nArchitecture Multi-Adds Parameters\nCNN6 21.986 \u00d7109 4,837,455\nCNN10 28.166 \u00d7109 5,219,279\nCNN14 42.220 \u00d7109 80,753,615\nResNet22 30.081 \u00d7109 63,675,087\nResNet38 48.962 \u00d7109 73,783,247\nResNet54 54.563 \u00d7109 104,318,159\nMobileNetV1 3.614 \u00d7109 4,796,303\nMobileNetV2 2.810 \u00d7109 4,075,343\nDaiNet 30.395 \u00d7109 4,385,807\nLeeNet11 4.741 \u00d7109 748,367\nLeeNet24 26.369 \u00d7109 10,003,791\nRes1dNet31 32.688 \u00d7109 80,464,463\nRes1dNet51 61.833 \u00d7109 106,538,063\nWavegram-CNN 44.234 \u00d7109 80,991,759\nWavegram-Logmel-CNN 53.510 \u00d7109 81,065,487\n0 10 20 30 40 50 60 70\nMulti-adds (million)\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50mAP Cnn6\nCnn10\nCnn14\nResNet22 ResNet38\nResNet54\nMobileNetV1\nMobileNetV2\nDaiNet\nLeeNet\nLeeNet18\nRes1dNet30\nRes1dNet44\nWavegram-CNN\nWavegram-\nLogmel-CNN\nFig.", "mimetype": "text/plain", "start_char_idx": 2310, "end_char_idx": 3391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f5319cc-f3be-406b-8958-2da24b49901e": {"__data__": {"id_": "0f5319cc-f3be-406b-8958-2da24b49901e", "embedding": null, "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d82e-654b-4e6c-80ca-5a378f47741f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "6038a0e5c882b5b2f43234f143f2e9644be7d05265cf0f122cd3312c1cbcde25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bacc7868-af4b-44eb-ac46-c5af82b6b737", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "87601fe3f30a3b7a453c0646d344bfc49d404de0fb12c8b71d34b47352041605", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. Multi-adds versus mAP of AudioSet tagging systems. The same types\nof architectures are grouped in the same color.\nindicates that the Wavegram is an effective learnt feature.\nFurthermore, our proposed Wavegram-Logmel-CNN system\nachieves a state-of-the-art mAP of 0.439 among all PANNs.\n15) Complexity analysis: We analyze the computational\ncomplexity of PANNs for inference. The number of multi-\nadds and parameters are two important factors for complexity\nanalysis. The middle column of Table XII shows the number\nof multi-adds to infer a 10-second audio clip. The right column\nof Table XII shows the number of parameters of different sys-\ntems. The number of multi-adds and parameters of the CNN14\nsystem are 42.2 \u00d7109 and 80.8 million, respectively, which\nare larger than the CNN6 and CNN10 systems. The number\nof multi-adds for the ResNets22 and ResNet38 systems are\nslightly less than for the CNN14 system. The ResNet54\nsystem contains the most multi-adds at 54.6 \u00d7109. One-\ndimensional CNNs have a similar computational cost to the\ntwo-dimensional CNNs. The best performing one-dimensional\nsystem, Res1dNet31, contains 32.7 \u00d7109 multi-adds and 80.5\nmillion parameters. Our proposed Wavegram-CNN system\ncontains 44.2 \u00d7109 multi-adds and 81.0 million parameters,\nwhich is similar to CNN14. The Wavegram-Logmel-CNN\nsystem slightly increases the multi-adds to 53.5\u00d7109, and the\nnumber of parameters is to 81.1 million, which is similar to\nCNN14. To reduce the number of multi-adds and parameters,", "mimetype": "text/plain", "start_char_idx": 3392, "end_char_idx": 4892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfde838b-685c-48fc-aa71-c4724cc8100e": {"__data__": {"id_": "bfde838b-685c-48fc-aa71-c4724cc8100e", "embedding": null, "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0687058-84d4-4857-b828-363bf10890f0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4b3c4a6b11d0280f8dfd4a140c7543363a757a1954ae174696037170fdd3ea14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b4d2cda-c0dc-4534-8c4c-e403eed972f7", "node_type": "1", "metadata": {}, "hash": "fc0f4460986cc8bfc1f6836ff4fbe1ff0a38831c821008fc9eaed65188ba98ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10\n2 5 10 20 32 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy Finetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 6. Accuracy of ESC-50 with various number of training clips per class.\nTABLE XIII\nACCURACY OF ESC-50\nSTOA [49] Scratch \ufb01ne-tune Freeze_L1 Freeze_L3\nAcc. 0.865 0.833 0.947 0.908 0.918\nMobileNets are applied. The MobileNetV1 and MobileNetV2\nsystems are light weight CNNs, with only 3.6 \u00d7109 and\n2.8 \u00d7109 multi-adds and around 4.8 million and 4.1 million\nparameters, respectively. MobileNets reduce both the com-\nputational cost and system size. Figure 5 summarizes the\nmAP versus multi-adds of different PANNs. The same type\nof systems are linked by lines of the same color. The mAP\nincreases from bottom to top. On the top-right is our proposed\nWavegram-Logmel-CNN system that achieves the best mAP.\nOn the top-left are MobileNetV1 and MobileNetV2 that are\nthe most computationally ef\ufb01cient systems.\nD. Transfer to other tasks\nIn this section, we investigate the application of PANNs\nto a range of other pattern recognition tasks. PANNs can\nuseful for few-shot learning, for the tasks where only a limited\nnumber of training clips are provided. Few-shot learning is\nan important research topic in audio pattern recognition, as\ncollecting labelled data can be time consuming.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b4d2cda-c0dc-4534-8c4c-e403eed972f7": {"__data__": {"id_": "2b4d2cda-c0dc-4534-8c4c-e403eed972f7", "embedding": null, "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0687058-84d4-4857-b828-363bf10890f0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4b3c4a6b11d0280f8dfd4a140c7543363a757a1954ae174696037170fdd3ea14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfde838b-685c-48fc-aa71-c4724cc8100e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "14a84e94a64646084c4e5582a5e5d9e5e2c083a8a4aff0cade1e94c993efa9d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c509ca0-3d15-4e9a-a841-405b12481f27", "node_type": "1", "metadata": {}, "hash": "554738387de0141c69783d9d5edb34958f92223ae3b53b0f0c1adc080c8ea8fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "D. Transfer to other tasks\nIn this section, we investigate the application of PANNs\nto a range of other pattern recognition tasks. PANNs can\nuseful for few-shot learning, for the tasks where only a limited\nnumber of training clips are provided. Few-shot learning is\nan important research topic in audio pattern recognition, as\ncollecting labelled data can be time consuming. We transfer\nPANNs to other audio pattern recognition tasks using the meth-\nods described in Section V. To begin with, we resample all\naudio recordings to 32 kHz, and convert them to monophonic\nto be consistent with the PANNs trained on AudioSet. We\nperform the following strategies described in Section V for\neach task: 1) Train a system from scratch; 2) Use a PANN\nas a feature extractor; 3) Fine-tune a PANN. When using a\nPANN as the feature extractor, we build classi\ufb01ers on the\nembedding features with one and three fully-connected layers,\nand call them \u201cFreeze + 1 layers\u201d (Freeze_L1) and \u201cFreeze\n+ 3 layers\u201d (Freeze_L3), respectively. We adopt the CNN14\nsystem for transfer learning to provide a fair comparison\nwith other CNN based systems for audio pattern recognition.\nWe also investigate the performance of PANNs trained with\ndifferent number of shots when training other audio pattern\nrecognition tasks.\n1) ESC-50: ESC-50 is an environmental sound dataset [50]\nconsisting of 50 sound events, such as \u201cDog\u201d and \u201cRain\u201d.", "mimetype": "text/plain", "start_char_idx": 936, "end_char_idx": 2339, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c509ca0-3d15-4e9a-a841-405b12481f27": {"__data__": {"id_": "9c509ca0-3d15-4e9a-a841-405b12481f27", "embedding": null, "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0687058-84d4-4857-b828-363bf10890f0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4b3c4a6b11d0280f8dfd4a140c7543363a757a1954ae174696037170fdd3ea14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b4d2cda-c0dc-4534-8c4c-e403eed972f7", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "f50653d666bc864650935f7998729e94192db3814aa65d87a7f9530e5baea645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa77525c-9b00-40ac-baa5-75cf1e19c06d", "node_type": "1", "metadata": {}, "hash": "30d9d4e52090c22982c50f1ec7891bf9ee24801d418ad0838fbbff2216b2e618", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We adopt the CNN14\nsystem for transfer learning to provide a fair comparison\nwith other CNN based systems for audio pattern recognition.\nWe also investigate the performance of PANNs trained with\ndifferent number of shots when training other audio pattern\nrecognition tasks.\n1) ESC-50: ESC-50 is an environmental sound dataset [50]\nconsisting of 50 sound events, such as \u201cDog\u201d and \u201cRain\u201d.\nThere are 2,000 5-second audio clips in the dataset, with\n2 5 10 20 50 100 200 500 ~918 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy Finetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 7. Accuracy of DCASE 2019 Task 1 with various number of training\nclips per class.\nTABLE XIV\nACCURACY OF DCASE 2019 T ASK 1\nSTOA [51] Scratch Fine-tune Freeze_L1 Freeze_L3\nAcc. 0.851 0.691 0.764 0.589 0.607\n2 5 10 32 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Map@3 Finetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 8. Accuracy of DCASE 2018 Task 2 with various number of training\nclips per class.", "mimetype": "text/plain", "start_char_idx": 1952, "end_char_idx": 2964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa77525c-9b00-40ac-baa5-75cf1e19c06d": {"__data__": {"id_": "aa77525c-9b00-40ac-baa5-75cf1e19c06d", "embedding": null, "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0687058-84d4-4857-b828-363bf10890f0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4b3c4a6b11d0280f8dfd4a140c7543363a757a1954ae174696037170fdd3ea14", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c509ca0-3d15-4e9a-a841-405b12481f27", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "eb438ae44d5b2a2bf255d082116281d9b305baf29b6ae178c09143cf6a3671f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8. Accuracy of DCASE 2018 Task 2 with various number of training\nclips per class.\nTABLE XV\nACCURACY OF DCASE 2018 T ASK 2\nSTOA [52] Scratch Fine-tune Freeze_L1 Freeze_L3\nmAP@3 0.954 0.902 0.941 0.717 0.768\n40 clips per class. Table XIII shows the 5-fold cross vali-\ndation [50] accuracy of the CNN14 system. Sailor et al. [49]\nproposed a state-of-the-art system for ESC-50, achieved an\naccuracy of 0.865 using unsupervised \ufb01lterbank learning with\na convolutional restricted Boltzmann machine. Our \ufb01ne-tuned\nsystem achieves an accuracy of 0.947, outperforming previous\nstate-of-the-art system by a large margin. The Freeze_L1 and\nFreeze_L3 systems achieve accuracies of 0.918 and 0.908, re-\nspectively. Training the CNN14 system from scratch achieves\nan accuracy of 0.833. Fig. 6 shows the accuracy of ESC-\n50 with different numbers of training clips of each sound\nclass. Using a PANN as a feature extractor achieves the\nbest performance when fewer than 10 clips per sound class\nare available for training. With more training clips, the \ufb01ne-\ntuned systems achieve better performance. Both the \ufb01ne-tuned\nsystem and the system using the PANN as a feature extractor\noutperform the systems trained from scratch.\n2) DCASE 2019 Task 1: DCASE 2019 Task 1 is an acoustic\nscene classi\ufb01cation task [2], with a dataset consisting of over", "mimetype": "text/plain", "start_char_idx": 2883, "end_char_idx": 4208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbfad1ef-ac41-407e-ae49-9784c666b863": {"__data__": {"id_": "cbfad1ef-ac41-407e-ae49-9784c666b863", "embedding": null, "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cc75df5efeca45adf0fec57ec93418abdbf8fe3ae985c15cc8acd4c385a2647d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "907c69ce-8a05-4dc3-b4f9-26d5e52711a2", "node_type": "1", "metadata": {}, "hash": "0893befd1b48fa0f6a2b4ed839c43799e4e29b7ba53ce8fdccf83bc079637123", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11\n2 5 10 20 50 100 200 300 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy Finetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 9. Accuracy of MSoS with various number of training clips per class.\nTABLE XVI\nACCURACY OF MSOS\nSTOA [53] Scratch Fine-tune Freeze_L1 Freeze_L3\nAcc. 0.930 0.760 0.960 0.886 0.930\n40 hours of stereo recordings collected from various acoustic\nscenes in 12 European cities. We focus on Subtask A, where\neach audio recording has two channels with a sampling rate\nof 48 kHz. In the development set, there are 9185 and 4185\naudio clips for training and validation respectively. We convert\nthe stereo recordings to monophonic by averaging the stereo\nchannels. CNN14 trained from scratch achieves an accuracy\nof 0.691, while the \ufb01ne-tuned system achieves an accuracy\nof 0.764. Freeze_L1 and Freeze_L3 achieve accuracies of\n0.689 and 0.607 respectively, and do not outperform the\nCNN14 trained from scratch. One possible explanation for\nthis underperformance is that the audio recordings of acoustic\nscene classi\ufb01cation have different distribution of AudioSet. So\nusing PANN as a feature extractor does not outperform \ufb01ne-\ntune or train a system from scratch. The \ufb01ne-tuned system\nachieves better performance than the system trained from\nscratch. Fig. 7 shows the classi\ufb01cation accuracy of systems\nwith various numbers of training clips per class.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "907c69ce-8a05-4dc3-b4f9-26d5e52711a2": {"__data__": {"id_": "907c69ce-8a05-4dc3-b4f9-26d5e52711a2", "embedding": null, "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cc75df5efeca45adf0fec57ec93418abdbf8fe3ae985c15cc8acd4c385a2647d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbfad1ef-ac41-407e-ae49-9784c666b863", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "48e5c1d9147390b59fba2889ad8591ff461495cae7228ff00012a6b6efcb16f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7", "node_type": "1", "metadata": {}, "hash": "d5c6b936e92f44eb8a7ea98a297c733e4e3b59c95276d3c997a5638ea5703234", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So\nusing PANN as a feature extractor does not outperform \ufb01ne-\ntune or train a system from scratch. The \ufb01ne-tuned system\nachieves better performance than the system trained from\nscratch. Fig. 7 shows the classi\ufb01cation accuracy of systems\nwith various numbers of training clips per class. Table XIV\nshows the overall performance. The state-of-the-art system of\nChen et al [51]. achieves an accuracy of 0.851 using the\ncombination of various classi\ufb01ers and stereo recordings as\ninput, while we do not use any ensemble methods and stereo\nrecordings.\n3) DCASE 2018 Task 2: DCASE 2018 Task 2 is a general-\npurpose automatic audio tagging task [54] using a dataset of\naudio recordings from Freesound annotated with a vocabulary\nof 41 labels from the AudioSet ontology. The development\nset consists of 9,473 audio recordings with durations from\n300 ms to 30 s. The mAP@3 is used for evaluating system\nperformance [54]. In training, we break or pad audio record-\nings into 4-second audio segments. In inference, we average\nthe predictions of those segments to obtain the prediction of\nan audio recording. Table XV shows that the best previous\nmethod proposed by Jeong and Lim [52] achieves an mAP@3\nof 0.954 using an ensemble of several systems. In comparison,\nour CNN14 system trained from scratch achieves an accuracy\nof 0.902. The \ufb01ne-tuned CNN14 system achieves an mAP@3\nof 0.941. The Freeze_L1 and Freeze_L3 systems achieve\naccuracies of 0.717 and 0.768 respectively. Fig.", "mimetype": "text/plain", "start_char_idx": 1101, "end_char_idx": 2569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7": {"__data__": {"id_": "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7", "embedding": null, "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cc75df5efeca45adf0fec57ec93418abdbf8fe3ae985c15cc8acd4c385a2647d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "907c69ce-8a05-4dc3-b4f9-26d5e52711a2", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "14a7d0779e901a724d0b4943424173f8d1748397d9ee18a365a9a67b1872dc21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94e8b838-d3f7-425e-8b15-ce1ebae63303", "node_type": "1", "metadata": {}, "hash": "9e8a02e34cd77bdaad940d1b0647b0258a9a987fddcf78033b068330a38c6854", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In comparison,\nour CNN14 system trained from scratch achieves an accuracy\nof 0.902. The \ufb01ne-tuned CNN14 system achieves an mAP@3\nof 0.941. The Freeze_L1 and Freeze_L3 systems achieve\naccuracies of 0.717 and 0.768 respectively. Fig. 8 shows the\n2 5 10 20 50 100 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy Finetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 10. Accuracy of GTZAN with various number of training clips per class.\nTABLE XVII\nACCURACY OF GTZAN\nSTOA [56] Scratch Fine-tune Freeze_L1 Freeze_L3\nAcc. 0.939 0.758 0.915 0.827 0.858\nmAP@3 with different numbers of training clips. The \ufb01ne-\ntuned CNN14 system outperforms the systems trained from\nscratch and the systems using PANN as a feature extractor.\nThe \ufb01ne-tuned CNN14 system achieves comparable results to\nthe state-of-the-art system.\n4) MSoS: The Making Sense of Sounds (MSoS) data\nchallenge [55] is a task to predict an audio recording to one\nof \ufb01ve categories: \u201cNature\u201d, \u201cMusic\u201d, \u201cHuman\u201d, \u201cEffects\u201d\nand \u201cUrban\u201d. The dataset consists of a development set of\n1,500 audio clips and an evaluation set of 500 audio clips. All\naudio clips have a duration of 4 seconds. The state-of-the-art\nsystem proposed by Chen and Gupta [53] achieves an accuracy\nof 0.930.", "mimetype": "text/plain", "start_char_idx": 2338, "end_char_idx": 3583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94e8b838-d3f7-425e-8b15-ce1ebae63303": {"__data__": {"id_": "94e8b838-d3f7-425e-8b15-ce1ebae63303", "embedding": null, "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cc75df5efeca45adf0fec57ec93418abdbf8fe3ae985c15cc8acd4c385a2647d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "48daaf494fa9539caa688ba8de5c64a8e3a5c050aa190e1746d2519a0dd4aa69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34462f83-2d9c-4a7f-9e74-3b5034a859e7", "node_type": "1", "metadata": {}, "hash": "34ab7537b28e0ab2590ac41e15e0f0fef703851a17ca8468c937d6d6b0aa9e44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dataset consists of a development set of\n1,500 audio clips and an evaluation set of 500 audio clips. All\naudio clips have a duration of 4 seconds. The state-of-the-art\nsystem proposed by Chen and Gupta [53] achieves an accuracy\nof 0.930. Our \ufb01ne-tuned CNN14 achieves an accuracy of\n0.960, outperforming previous state-of-the-art system. CNN14\ntrained from scratch achieves an accuracy of 0.760. Fig. 9\nshows the accuracy of the systems with different number of\ntraining clips. The \ufb01ne-tuned CNN14 system and the system\nusing CNN14 as a feature extractor outperforms CNN14\ntrained from scratch.\n5) GTZAN: The GTZAN dataset [57] is a music genre\nclassi\ufb01cation dataset containing 1,000 30-second music clips\nof 10 genres of music, such as \u201cClassical\u201d and \u201cCountry\u201d. All\nmusic clips have a duration of 30 seconds and a sampling rate\nof 22,050 Hz. In development, 10-fold cross validation is used\nto evaluate the performance of systems. Table XVII shows that\nprevious state-of-the-art system proposed by Liu et al. [56]\nachieves an accuracy of 0.939 using a bottom-up broadcast\nneural network. The \ufb01ne-tuned CNN14 system achieves an\naccuracy of 0.915, outperforming CNN14 trained from scratch\nwith an accuracy of 0.758 and the Freeze_L1 and Freeze_L3\nsystems with accuracies of 0.827 and 0.858 respectively. Fig.\n10 shows the accuracy of systems with different numbers of\ntraining clips.", "mimetype": "text/plain", "start_char_idx": 3342, "end_char_idx": 4728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34462f83-2d9c-4a7f-9e74-3b5034a859e7": {"__data__": {"id_": "34462f83-2d9c-4a7f-9e74-3b5034a859e7", "embedding": null, "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "cc75df5efeca45adf0fec57ec93418abdbf8fe3ae985c15cc8acd4c385a2647d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94e8b838-d3f7-425e-8b15-ce1ebae63303", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "ba11ac47ac8573530836249e24306ffb37b568813ab0a41bc97d51bc88ce6c56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \ufb01ne-tuned CNN14 system achieves an\naccuracy of 0.915, outperforming CNN14 trained from scratch\nwith an accuracy of 0.758 and the Freeze_L1 and Freeze_L3\nsystems with accuracies of 0.827 and 0.858 respectively. Fig.\n10 shows the accuracy of systems with different numbers of\ntraining clips. The Freeze_L1 and Freeze_L3 systems achieve\nbetter performance than other systems trained with less than\n10 clips per class. With more training clips, the \ufb01ne-tuned\nCNN14 system performs better than other systems.\n6) RAVDESS: The Ryerson Audio-Visual Database of\nEmotional Speech and Song (RA VDESS) is a human speech\nemotion dataset [59]. The database consists of sounds from", "mimetype": "text/plain", "start_char_idx": 4435, "end_char_idx": 5105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9c7e71c-e04a-4a37-b9f8-30657480b6cb": {"__data__": {"id_": "f9c7e71c-e04a-4a37-b9f8-30657480b6cb", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fa862e4-4fc5-46e1-9336-c2984bdc796b", "node_type": "1", "metadata": {}, "hash": "0f8363137a3020ebc73cb3eb5921dbede8e99646ee3df06a8739e6c20398a94e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12\n2 5 10 20 50 192 (all)\nTraining samples per class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nFinetune\nScratch\nFreeze + 1layer\nFreeze + 3 layers\nFig. 11. Accuracy of RA VDESS with various number of training clips per\nclass.\nTABLE XVIII\nACCURACY OF RAVDESS\nSTOA [58] Scratch Fine-tune Freeze_L1 Freeze_L3\nAcc. 0.645 0.692 0.721 0.397 0.401\n24 professional actors including 12 female and 12 male\nsimulating 8 emotions, such as \u201cHappy\u201d and \u201cSad\u201d. The task\nis to classify each sound clip into an emotion. There are\n1,440 audio clips in the development set. We evaluate our\nsystems with 4-fold cross validation. Table XVIII shows that\nprevious state-of-the-art system proposed by Zeng et al. [58]\nachieves an accuracy of 0.645. Our CNN14 system trained\nfrom scratch achieves an accuracy of 0.692. The \ufb01ne-tuned\nCNN14 system achieves a state-of-the-art accuracy of 0.721.\nThe Freeze_L1 and Freeze_L3 systems achieve much lower\naccuracies of 0.397 and 0.401 respectively. Fig. 11 shows the\naccuracy of systems with respect to a range of training clips.\nThe \ufb01ne-tuned systems and the system trained from scratch\noutperform the system using PANN as a feature extractor. This\nsuggests that audio recordings of the RA VDESS dataset may\nhave different distributions of the AudioSet dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fa862e4-4fc5-46e1-9336-c2984bdc796b": {"__data__": {"id_": "4fa862e4-4fc5-46e1-9336-c2984bdc796b", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9c7e71c-e04a-4a37-b9f8-30657480b6cb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "4cd90651edd6536040d6a96b128cfd7950fe91834e968747f7ad898351986598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8e94427-6955-4529-a647-7c09b51e5b45", "node_type": "1", "metadata": {}, "hash": "37f9f1d6fb4736eb8cff8797608ed836bb49c108a14a08ad1ebf1409fa3d1625", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Freeze_L1 and Freeze_L3 systems achieve much lower\naccuracies of 0.397 and 0.401 respectively. Fig. 11 shows the\naccuracy of systems with respect to a range of training clips.\nThe \ufb01ne-tuned systems and the system trained from scratch\noutperform the system using PANN as a feature extractor. This\nsuggests that audio recordings of the RA VDESS dataset may\nhave different distributions of the AudioSet dataset. Therefore,\nthe parameters of a PANN need be \ufb01ne-tuned to achieve good\nperformance on the RA VDESS classi\ufb01cation task.\nE. Discussion\nIn this article, we have investigated a wide range of\nPANNs for AudioSet tagging. Several of our proposed PANNs\nhave outperformed previous state-of-the-art AudioSet tag-\nging systems, including CNN14 achieves an mAP of 0.431,\nand ResNet38 achieves an mAP of 0.434, outperforming\nGoogle\u2019s baseline of 0.314. MobileNets are light-weight sys-\ntems that have fewer multi-adds and numbers of parame-\nters. MobileNetV1 achieves an mAP of 0.389. Our adapted\none-dimensional system Res1dNet31 achieves an mAP of\n0.365, outperforming previous one-dimensional CNNs includ-\ning DaiNet [31] of 0.295 and LeeNet11 [42] of 0.266. Our\nproposed Wavegram-Logmel-CNN system achieves the high-\nest mAP of 0.439 among all PANNs. PANNs can be used as\na pretrained model for new audio pattern recognition tasks.\nPANNs trained on the AudioSet dataset were transferred to\nsix audio pattern recognition tasks.", "mimetype": "text/plain", "start_char_idx": 857, "end_char_idx": 2286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8e94427-6955-4529-a647-7c09b51e5b45": {"__data__": {"id_": "c8e94427-6955-4529-a647-7c09b51e5b45", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fa862e4-4fc5-46e1-9336-c2984bdc796b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "df0e94a79f17dd2e75b2a1507ba8cb901e2d58b771bbe41a6465bcd1600612bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a254f9c4-ad73-4a09-a9c4-78285cfabf49", "node_type": "1", "metadata": {}, "hash": "c4ba3c5c22c10f86200077e0bfb3ac203bc79cb97b8ee7fb48dfd4f9e5e7f642", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our\nproposed Wavegram-Logmel-CNN system achieves the high-\nest mAP of 0.439 among all PANNs. PANNs can be used as\na pretrained model for new audio pattern recognition tasks.\nPANNs trained on the AudioSet dataset were transferred to\nsix audio pattern recognition tasks. We show that \ufb01ne-tuned\nPANNs achieve state-of-the-art performance in the ESC-50,\nMSOS and RA VDESS classi\ufb01cation tasks, and approach the\nstate-of-the-art performance in the DCASE 2018 Task 2 and\nthe GTZAN classi\ufb01cation task. Of the PANN systems, the \ufb01ne-\ntuned PANNs always outperform PANNs trained from scratch\non new tasks. The experiments show that PANNs have been\nsuccessful in generalizing to other audio pattern recognition\ntasks with limited number of training data.\nVII. C ONCLUSION\nWe have presented pretrained audio neural networks\n(PANNs) trained on the AudioSet for audio pattern recog-\nnition. A wide range of neural networks are investigated to\nbuild PANNs. We propose a Wavegram feature learnt from\nwaveform, and a Wavegram-Logmel-CNN that achieves state-\nof-the-art performance in AudioSet tagging, archiving an mAP\nof 0.439. We also investigate the computational complexity\nof PANNs. We show that PANNs can be transferred to a\nwide range of audio pattern recognition tasks and outperform\nseveral previous state-of-the-art systems. PANNs can be useful\nwhen \ufb01ne-tuned on a small amount of data on new tasks.\nIn the future, we will extend PANNs to more audio pattern\nrecognition tasks.", "mimetype": "text/plain", "start_char_idx": 2018, "end_char_idx": 3486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a254f9c4-ad73-4a09-a9c4-78285cfabf49": {"__data__": {"id_": "a254f9c4-ad73-4a09-a9c4-78285cfabf49", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8e94427-6955-4529-a647-7c09b51e5b45", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "1535f01dd5ffd92c836b00fda161d24ef68bbf0e09b582fa42f14cc599061663", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cce72ca5-1488-4d6f-8938-ad15d28efedd", "node_type": "1", "metadata": {}, "hash": "35257fb2a8449bc7f66ec6f22c38b99dc3e9a78c3ce6a8e8af98ab75be65a7e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also investigate the computational complexity\nof PANNs. We show that PANNs can be transferred to a\nwide range of audio pattern recognition tasks and outperform\nseveral previous state-of-the-art systems. PANNs can be useful\nwhen \ufb01ne-tuned on a small amount of data on new tasks.\nIn the future, we will extend PANNs to more audio pattern\nrecognition tasks.\nREFERENCES\n[1] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.\nMoore, M. Plakal, and M. Ritter, \u201cAudio Set: An ontology and human-\nlabeled dataset for audio events,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2017, pp. 776\u2013780.\n[2] A. Mesaros, T. Heittola, and T. Virtanen, \u201cA multi-device dataset for\nurban acoustic scene classi\ufb01cation,\u201d in Workshop on Detection and\nClassi\ufb01cation of Acoustic Scenes and Events (DCASE) , 2018, pp. 9\u2013\n13.\n[3] K. Choi, G. Fazekas, and M. Sandler, \u201cAutomatic tagging using deep\nconvolutional neural networks,\u201d in Conference of the International\nSociety for Music Information Retrieval (ISMIR) , 2016, pp. 805\u2013811.\n[4] E. Cakir, T. Heittola, H. Huttunen, and T. Virtanen, \u201cPolyphonic sound\nevent detection using multi label deep neural networks,\u201d in International\nJoint Conference on Neural Networks (IJCNN) , 2015.", "mimetype": "text/plain", "start_char_idx": 3129, "end_char_idx": 4400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cce72ca5-1488-4d6f-8938-ad15d28efedd": {"__data__": {"id_": "cce72ca5-1488-4d6f-8938-ad15d28efedd", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a254f9c4-ad73-4a09-a9c4-78285cfabf49", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "803e41d5407d7314e18eb565105460f9acfc76a6fc508e34f1fe3cbcad59efd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1165a9b2-3fd8-476f-a275-1301add68a23", "node_type": "1", "metadata": {}, "hash": "338b1b336c0431a96631a77e6db2d8728b9f3adc78a22f5a5dfb34595b071031", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "805\u2013811.\n[4] E. Cakir, T. Heittola, H. Huttunen, and T. Virtanen, \u201cPolyphonic sound\nevent detection using multi label deep neural networks,\u201d in International\nJoint Conference on Neural Networks (IJCNN) , 2015.\n[5] J. P. Woodard, \u201cModeling and classi\ufb01cation of natural sounds by product\ncode hidden Markov models,\u201d IEEE Transactions on Signal Processing,\nvol. 40, pp. 1833\u20131835, 1992.\n[6] D. P. W. Ellis, \u201cDetecting alarm sounds,\u201d https://academiccommons.\ncolumbia.edu/doi/10.7916/D8F19821/download, 2001.\n[7] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D.\nPlumbley, \u201cDetection and classi\ufb01cation of acoustic scenes and events,\u201d\nIEEE Transactions on Multimedia , vol. 17, pp. 1733\u20131746, 2015.\n[8] A. Mesaros, T. Heittola, E. Benetos, P. Foster, M. Lagrange, T. Virtanen,\nand M. D. Plumbley, \u201cDetection and classi\ufb01cation of acoustic scenes\nand events: Outcome of the DCASE 2016 challenge,\u201d IEEE/ACM\nTransactions on Audio, Speech and Language Processing (TASLP) ,\nvol. 26, pp. 379\u2013393, 2018.", "mimetype": "text/plain", "start_char_idx": 4191, "end_char_idx": 5193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1165a9b2-3fd8-476f-a275-1301add68a23": {"__data__": {"id_": "1165a9b2-3fd8-476f-a275-1301add68a23", "embedding": null, "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "133dc0f43e16e43ea731cf84105202663f3c776e290ca65ce635402c50767eed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cce72ca5-1488-4d6f-8938-ad15d28efedd", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "25330c0cfc4ce375003d292eab4cf6ec09779228a2879a1e6bd7bee266deef58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "26, pp. 379\u2013393, 2018.\n[9] A. Mesaros, T. Heittola, A. Diment, B. Elizalde, A. Shah, E. Vincent,\nB. Raj, and T. Virtanen, \u201cDCASE 2017 challenge setup: Tasks, datasets\nand baseline system,\u201d in Workshop on Detection and Classi\ufb01cation of\nAcoustic Scenes and Events (DCASE) , 2017, pp. 85\u201392.\n[10] \u201cDCASE Challenge 2019,\u201d http://dcase.community/challenge2019,\n2019.\n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImageNet:\nA large-scale hierarchical image database,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2009, pp. 248\u2013255.\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-\ntraining of deep bidirectional transformers for language understanding,\u201d\nin Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics (NAACL) , 2018, pp. 4171\u20134186.\n[13] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.\nMoore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al. , \u201cCNN\narchitectures for large-scale audio classi\ufb01cation,\u201d in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2017,\npp. 131\u2013135.", "mimetype": "text/plain", "start_char_idx": 5171, "end_char_idx": 6316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0f2b242-14ac-45f6-9834-6dfb33cd516c": {"__data__": {"id_": "f0f2b242-14ac-45f6-9834-6dfb33cd516c", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b6a7402-536e-4512-b48a-f78e2f3833ba", "node_type": "1", "metadata": {}, "hash": "8b3db1cb4051d53d4938fc842b096998bf61bdbe2c919ad71f298121f0c120ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13\nBagpipes\nChange ringing (campanolo\nMusic\nAngry music\nHarmonica\nAccordion\nEmergency vehicle\nHarpsichord\nDidgeridoo\nBrass instrument\nSiren\nSpeech\nWhispering\nBattle cry\nCivil defense siren\nShofar\nRail transport\nTrain\nRailroad car, train wagon\nWind instrument, woodwind\nTrain wheels squealing\nChoir\nBanjo\nCrowd\nFire engine, fire truck\nSanding\nHarp\nThunder\nGuitar\nPurr\nSizzle\nThunderstorm\nPizzicato\nStir\nElectric guitar\nHi-hat\nOrgan\nApplause\nChatter\nHair dryer\nScratching (performance\nHeart sounds, heartbeat\nClicking\nSteelpan\nSkateboard\nTimpani\nSteam whistle\nFrying (food)\nTabla\nDrum kit\nCymbal\nString section\nBee, wasp, etc.\nA capella\nFire alarm\nOpera\nBlender\nChildren shouting\nInsect\nFrench horn\nSonar\nRimshot\nBasketball bounce\nAircraft\nFly, housefly\nCello\nAircraft engine\nSnare drum\nGong\nWater\nChainsaw\nEffects unit\nChant\nPercussion\nChurch bell\nBird flight, flapping win\nSaxophone\nCheering\nElectric shaver, electric\nAmbulance (siren)\nFusillade\nRain\nHowl\nFireworks\nBass drum\nRapping\nDouble bass\nChewing, mastication\nComputer keyboard\nZither\nRain on surface\nDubstep\nPlucked string instrument\nChopping (food)\nMachine gun\nSubway, metro, undergroun\nElectronic dance music\nCoo\nMusic of Bollywood\nDrum\nDistortion\nRub\nClarinet\nSalsa music\nPigeon,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b6a7402-536e-4512-b48a-f78e2f3833ba": {"__data__": {"id_": "9b6a7402-536e-4512-b48a-f78e2f3833ba", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0f2b242-14ac-45f6-9834-6dfb33cd516c", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "d4b60c5717f1fba2fad5471886e34cf3e4d01723bc122bc3b4d6e1504931844e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d211d9f-40b8-4995-b008-79ce8889a88f", "node_type": "1", "metadata": {}, "hash": "89f83d455a672bb1754839fc8e879edf28f71466be89e103db13b11289435e64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mastication\nComputer keyboard\nZither\nRain on surface\nDubstep\nPlucked string instrument\nChopping (food)\nMachine gun\nSubway, metro, undergroun\nElectronic dance music\nCoo\nMusic of Bollywood\nDrum\nDistortion\nRub\nClarinet\nSalsa music\nPigeon, dove\nWind chime\nIce cream truck, ice crea\nSkidding\nFowl\nElectronic tuner\nHands\nFlute\nSitar\nFixed-wing aircraft, airp\nSmoke detector, smoke ala\nChildren playing\nTechno\nCrowing, cock-a-doodle-do\nWaterfall\nAfrobeat\nCowbell\nPolice car (siren)\nDog\nTrain horn\nTrombone\nClickety-clack\nKeyboard (musical)\nAcoustic guitar\nChime\nTapping (guitar technique\nTyping\nBoom\n103\n105\nNumber of audio clips\nVehicle\nVacuum cleaner\nSynthetic singing\nSnoring\nSplash, splatter\nMoo\nMusic of Africa\nToilet flush\nStomach rumble\nPour\nDomestic animals, pets\nJackhammer\nBass guitar\nWhale vocalization\nBluegrass\nCattle, bovinae\nBowed string instrument\nBathtub (filling or washi\nJet engine\nCash register\nSinging bowl\nSailboat, sailing ship\nZing\nPlop\nTrumpet\nGunshot, gunfire\nBow-wow\nBoat, Water vehicle\nVideo game music\nCar alarm\nPower windows, electric\nReversing beeps\nSine wave\nWood block\nDrum roll\nBusy signal\nWind noise (microphone)\nCricket\nPink noise\nChicken, rooster\nUkulele\nRoar\nAnimal\nTambourine\nDial tone\nRumble\nSteam\nCap gun\nSink (filling or washing)\nChorus effect\nWater tap,", "mimetype": "text/plain", "start_char_idx": 1005, "end_char_idx": 2294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d211d9f-40b8-4995-b008-79ce8889a88f": {"__data__": {"id_": "1d211d9f-40b8-4995-b008-79ce8889a88f", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b6a7402-536e-4512-b48a-f78e2f3833ba", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "08b9bb80ae0af20925cc18fdd80525eedc2174c53f72931a6ed9e2a37582e4a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67b07245-49bb-44ab-984c-26e2cd9f2b11", "node_type": "1", "metadata": {}, "hash": "c218c8b21fecb6be57b482bbbceb10eb9e35b6b158b8540cd3cd75611314a754", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Water vehicle\nVideo game music\nCar alarm\nPower windows, electric\nReversing beeps\nSine wave\nWood block\nDrum roll\nBusy signal\nWind noise (microphone)\nCricket\nPink noise\nChicken, rooster\nUkulele\nRoar\nAnimal\nTambourine\nDial tone\nRumble\nSteam\nCap gun\nSink (filling or washing)\nChorus effect\nWater tap, faucet\nRattle (instrument)\nWhistling\nCaterwaul\nChild speech, kid speakin\nStatic\nHammer\nFlamenco\nMarimba, xylophone\nHammond organ\nQuack\nSmash, crash\nGrunge\nAir horn, truck horn\nElectronica\nRace car, auto racing\nCarnatic music\nClapping\nCroak\nFrog\nCar\nScratch\nJingle bell\nBeatboxing\nWind\nMains hum\nBabbling\nCat\nDrum and bass\nBoing\nViolin, fiddle\nTypewriter\nChild singing\nSniff\nReggae\nScary music\nBark\nGospel music\nHubbub, speech noise, spe\nTrain whistle\nGlockenspiel\nLullaby\nFiling (rasp)\nMandolin\nGargling\nBaby laughter\nMusical instrument\nMantra\nRaindrop\nLaughter\nTrance music\nTurkey\nOrchestra\nTuning fork\nLivestock, farm animals,\nSka\nDrill\nFirecracker\nSteel guitar, slide guita\nGobble\nCrumpling, crinkling\nMosquito\nCaw\nSewing machine\nEnvironmental noise\nProgressive rock\nHonk\nHip hop music\nDisco\nRowboat, canoe,", "mimetype": "text/plain", "start_char_idx": 1998, "end_char_idx": 3105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67b07245-49bb-44ab-984c-26e2cd9f2b11": {"__data__": {"id_": "67b07245-49bb-44ab-984c-26e2cd9f2b11", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d211d9f-40b8-4995-b008-79ce8889a88f", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "df1445a034c15bc08b7f99224ec6493b0564129161051d3cbdf24e3b5de6bbed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28723e00-7566-4c40-837b-15ff158ca753", "node_type": "1", "metadata": {}, "hash": "057cfb96df6c903c856b384e4e6e8bfd29ebc67e964946169d9d0795f8d3c139", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "farm animals,\nSka\nDrill\nFirecracker\nSteel guitar, slide guita\nGobble\nCrumpling, crinkling\nMosquito\nCaw\nSewing machine\nEnvironmental noise\nProgressive rock\nHonk\nHip hop music\nDisco\nRowboat, canoe, kayak\nCluck\nEngine\nSpeech synthesizer\nBird\nElectric toothbrush\nArtillery fire\nSinging\nFart\nLight engine (high freque\nHoot\nCrow\nMallet percussion\n103\n105\nNumber of audio clips\nVibraphone\nCutlery, silverware\nTheremin\nGoose\nGrunt\nDance music\nLawn mower\nReverberation\nTelephone bell ringing\nElectronic organ\nRadio\nAmbient music\nToot\nVehicle horn, car horn,\nStrum\nOcean\nOwl\nClip-clop\nDrum machine\nWaves, surf\nHeart murmur\nConversation\nBaby cry, infant cry\nShuffle\nBlues\nHelicopter\nPiano\nAccelerating, revving,\nChop\nSwing music\nElectric piano\nTearing\nStream\nNew-age music\nTelephone dialing, DTMF\nDrawer open or close\nMeow\nWhistle\nFoghorn\nBreaking\nThrobbing\nBicycle bell\nTools\nSad music\nMotorcycle\nBiting\nRingtone\nScreaming\nSigh\nFinger snapping\nCrunch\nClock\nTire squeal\nHeavy metal\nKnock\nTruck\nRock and roll\nEruption\nHouse music\nVocal music\nSnake\nDishes, pots, and pans\nCountry\nThump, thud\nWood\nBackground music\nSqueak\nSquawk\nMiddle Eastern music\nHiss\nTheme music\nMaraca\nGush\nEngine knocking\nRoll\nField recording\nClang\nRoaring cats (lions,", "mimetype": "text/plain", "start_char_idx": 2910, "end_char_idx": 4138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28723e00-7566-4c40-837b-15ff158ca753": {"__data__": {"id_": "28723e00-7566-4c40-837b-15ff158ca753", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67b07245-49bb-44ab-984c-26e2cd9f2b11", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "888f85065ed609c8637d1276d4f4b5a2382f5f9cda54b2fe9595799e31afb42c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23d4bff9-886a-477b-84ca-c08f40d4ea38", "node_type": "1", "metadata": {}, "hash": "7cdf9c8a3d9cb7c73cb49b6dd835930b66b0d6a83ed35365b4861e2471c7931f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pots, and pans\nCountry\nThump, thud\nWood\nBackground music\nSqueak\nSquawk\nMiddle Eastern music\nHiss\nTheme music\nMaraca\nGush\nEngine knocking\nRoll\nField recording\nClang\nRoaring cats (lions, tige\nAir conditioning\nBird vocalization, bird\nShuffling cards\nGrowling\nBurping, eructation\nPump (liquid)\nChuckle, chortle\nPing\nCupboard open or close\nIdling\nShatter\nYodeling\nAlarm clock\nPsychedelic rock\nBelly laugh\nCanidae, dogs, wolves\nMusic for children\nHum\nWalk, footsteps\nPrinter\nSidetone\nNeigh, whinny\nTelephone\nMusic of Latin America\nScissors\nFunk\nEngine starting\nChirp tone\nIndependent music\nHumming\nHorse\nJingle, tinkle\nPower tool\nWhack, thwack\nDoorbell\nCacophony\nEcho\nCrying, sobbing\nMusic of Asia\nDental drill, dentist's\nPop music\nRock music\nMotorboat, speedboat\nThunk\nSampler\nPunk rock\nWhir\nWhoosh, swoosh, swish\nWhip\nSplinter\nSawing\nYell\nBus\nTap\n103\n105\nNumber of audio clips\nAir brake\nChristian music\nPropeller, airscrew\nGlass\nSlosh\nHappy music\nPant\nRhythm and blues\nDrip\nWriting\nCough\nInside, small room\nArrow\nSoul music\nFemale singing\nWild animals\nPig\nTraditional music\nShout\nCamera\nYip\nBouncing\nRodents, rats, mice\nTubular bells\nSliding door\nCoin (dropping)\nWail,", "mimetype": "text/plain", "start_char_idx": 3954, "end_char_idx": 5118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23d4bff9-886a-477b-84ca-c08f40d4ea38": {"__data__": {"id_": "23d4bff9-886a-477b-84ca-c08f40d4ea38", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28723e00-7566-4c40-837b-15ff158ca753", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e07ce51ed599c7fde5bfd4d92b38f9990485ea8309a6ce204256fac1aac625a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d33f7ca-6300-427a-b774-6290ee326e43", "node_type": "1", "metadata": {}, "hash": "7534159daf4b37ac0f4a1a83480c8cb889613ae9594f4582cce086b769a99f2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "airscrew\nGlass\nSlosh\nHappy music\nPant\nRhythm and blues\nDrip\nWriting\nCough\nInside, small room\nArrow\nSoul music\nFemale singing\nWild animals\nPig\nTraditional music\nShout\nCamera\nYip\nBouncing\nRodents, rats, mice\nTubular bells\nSliding door\nCoin (dropping)\nWail, moan\nTick-tock\nSynthesizer\nShip\nToothbrush\nGiggle\nRustling leaves\nSlap, smack\nChirp, tweet\nVibration\nAlarm\nBellow\nSheep\nWhimper\nExplosion\nWheeze\nJingle (music)\nCar passing\nSlam\nFolk music\nGasp\nPulse\nCrushing\nTelevision\nFemale speech, woman spea\nSnicker\nBeep, bleep\nTender music\nMale singing\nWhimper (dog)\nSqueal\nBuzzer\nBoiling\nSoundtrack music\nMechanical fan\nDuck\nBell\nDing\nMicrowave oven\nSong\nChristmas music\nElectronic music\nBang\nSneeze\nCrackle\nMechanisms\nTraffic noise, roadway\nFire\nWedding music\nDing-dong\nClatter\nOutside, urban or manmade\nThroat clearing\nExciting music\nMedium engine (mid freque\nCrack\nBleat\nBreathing\nFunny music\nFill (with liquid)\nHiccup\nGoat\nMotor vehicle (road)\nKeys jangling\nSpray\nDoor\nPatter\nWhite noise\nPulleys\nRun\nOink\nFlap\nGears\nNarration, monologue\nMale speech, man speaking\nGroan\nZipper (clothing)\nWhoop\nRatchet, pawl\nSilence\nInside, large room or hal\nSnort\nInside,", "mimetype": "text/plain", "start_char_idx": 4864, "end_char_idx": 6016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d33f7ca-6300-427a-b774-6290ee326e43": {"__data__": {"id_": "5d33f7ca-6300-427a-b774-6290ee326e43", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23d4bff9-886a-477b-84ca-c08f40d4ea38", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "acf6b566043819baa527a293ce03d62239a256160a829c0d97ee840456c082f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e6369ec-83ab-41fb-86ed-df86cf2a44d2", "node_type": "1", "metadata": {}, "hash": "2ada87bae35d3c59cc9a2a3b3f1f59a8c75eaad8d0672869af3b49d14d4b6675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "monologue\nMale speech, man speaking\nGroan\nZipper (clothing)\nWhoop\nRatchet, pawl\nSilence\nInside, large room or hal\nSnort\nInside, public space\nHeavy engine (low frequen\nBicycle\nChink, clink\nTrickle, dribble\nJazz\nClassical music\nNoise\nCreak\nRustle\nSingle-lens reflex camera\nSound effect\nOutside, rural or natural\nHarmonic\nBuzz\nSquish\nLiquid\nTick\nGurgling\nBurst, pop\nRattle\nScrape\nMouse\n103\n105\nNumber of audio clips\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage precision\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage precision\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage precision\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage precision\nAP with Wavegram-Logmel-CNN\nAP with CNN14\nAP with MobileNetV1\nAP with averaging instances (baseline)\nLabel quality\nFig. 12. Class-wise performance of AudioSet tagging systems. Red, blue and black curves are APs of CNN14, MobileNetV1 and the audio tagging system\n[20]. The blue bars show the number of training clips in logarithmic scale.\n[14] K. Choi, G. Fazekas, M. Sandler, and K. Cho, \u201cTransfer learning\nfor music classi\ufb01cation and regression tasks,\u201d in Conference of the\nInternational Society of Music Information Retrieval (ISMIR) , 2017, pp.\n141\u2013149.", "mimetype": "text/plain", "start_char_idx": 5889, "end_char_idx": 7026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e6369ec-83ab-41fb-86ed-df86cf2a44d2": {"__data__": {"id_": "8e6369ec-83ab-41fb-86ed-df86cf2a44d2", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d33f7ca-6300-427a-b774-6290ee326e43", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "3878a79c1c4c8b0849a9bbfc2e880f3d1ecbe35ca33e3cc3b1ad21f65027045b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbdedb5d-830d-41d6-901c-9bf52167b7bd", "node_type": "1", "metadata": {}, "hash": "fca6e3af566f9eeb1f8ce61de91963ffc5787c4933ae5bb13efc08a0b3d4e418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue bars show the number of training clips in logarithmic scale.\n[14] K. Choi, G. Fazekas, M. Sandler, and K. Cho, \u201cTransfer learning\nfor music classi\ufb01cation and regression tasks,\u201d in Conference of the\nInternational Society of Music Information Retrieval (ISMIR) , 2017, pp.\n141\u2013149.\n[15] J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann, and X. Serra,\n\u201cEnd-to-end learning for music audio tagging at scale,\u201d in Conference\nof the International Society for Music Information Retrieval (ISMIR) ,\n2017, pp. 637\u2013644.\n[16] Q. Kong, Y . Xu, W. Wang, and M. D. Plumbley, \u201cAudio Set classi\ufb01cation\nwith attention model: A probabilistic perspective,\u201d in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2018,\npp. 316\u2013320.\n[17] C. Yu, K. S. Barsim, Q. Kong, and B. Yang, \u201cMulti-level attention\nmodel for weakly supervised audio classi\ufb01cation,\u201d in Detection and\nClassi\ufb01cation of Acoustic Scenes and Events (DCASE) , 2018, pp. 188\u2013\n192.\n[18] S.-Y . Chou, J.-S. R. Jang, and Y .-H. Yang, \u201cLearning to recognize\ntransient sound events using attentional supervision,\u201d in International\nJoint Conferences on Arti\ufb01cial Intelligence (IJCAI) , 2018, pp. 3336\u2013\n3342.\n[19] Y .", "mimetype": "text/plain", "start_char_idx": 6738, "end_char_idx": 7934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbdedb5d-830d-41d6-901c-9bf52167b7bd": {"__data__": {"id_": "dbdedb5d-830d-41d6-901c-9bf52167b7bd", "embedding": null, "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f32560-9f7c-47ae-8ec8-39342886a365", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "953186c5537e65624a30405ce663eed0a3607e27c814e57397a83fed53462243", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e6369ec-83ab-41fb-86ed-df86cf2a44d2", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "1ece2562177dfb83a27f2a45bf6107d50e273dd358ff499054a86f06fbd5c117", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "188\u2013\n192.\n[18] S.-Y . Chou, J.-S. R. Jang, and Y .-H. Yang, \u201cLearning to recognize\ntransient sound events using attentional supervision,\u201d in International\nJoint Conferences on Arti\ufb01cial Intelligence (IJCAI) , 2018, pp. 3336\u2013\n3342.\n[19] Y . Wang, J. Li, and F. Metze, \u201cA comparison of \ufb01ve multiple instance\nlearning pooling functions for sound event detection with weak labeling,\u201d", "mimetype": "text/plain", "start_char_idx": 7695, "end_char_idx": 8074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81ca14b4-4c57-4797-a6c9-406349cde7b9": {"__data__": {"id_": "81ca14b4-4c57-4797-a6c9-406349cde7b9", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "146b5908-3a87-4228-a108-6d7576a8a9d0", "node_type": "1", "metadata": {}, "hash": "58afa75cf43ff4aa6ef65944311bf235c43dfc0c5c72384350e873e2bed8a606", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14\nin IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2019, pp. 31\u201335.\n[20] Q. Kong, C. Yu, Y . Xu, T. Iqbal, W. Wang, and M. D. Plumbley, \u201cWeakly\nlabelled audioset tagging with attention neural networks,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing , vol. 27, pp.\n1791\u20131802, 2019.\n[21] A. Van Den Oord, S. Dieleman, and B. Schrauwen, \u201cTransfer learning\nby supervised pre-training for audio-based music classi\ufb01cation,\u201d in\nConference of the International Society for Music Information Retrieval\n(ISMIR), 2014, pp. 29\u201334.\n[22] Y . Wang, \u201cPolyphonic sound event detection with weak labeling,\u201d PhD\nthesis, Carnegie Mellon University , 2018.\n[23] E. Law and L. V on Ahn, \u201cInput-agreement: a new mechanism for\ncollecting data using human computation games,\u201d in Proceedings of the\nSIGCHI Conference on Human Factors in Computing Systems , 2009,\npp. 1197\u20131206.\n[24] A. Mesaros, T. Heittola, and T. Virtanen, \u201cTUT database for acoustic\nscene classi\ufb01cation and sound event detection,\u201d in Conference on Euro-\npean Signal Processing Conference (EUSIPCO) , 2016, pp. 1128\u20131132.\n[25] J. Pons and X. Serra, \u201cMUSICNN: Pre-trained convolutional neural\nnetworks for music audio tagging,\u201d arXiv preprint arXiv:1909.06654 ,\n2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "146b5908-3a87-4228-a108-6d7576a8a9d0": {"__data__": {"id_": "146b5908-3a87-4228-a108-6d7576a8a9d0", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81ca14b4-4c57-4797-a6c9-406349cde7b9", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "0e10981211000c6a0a9710f63db2d816303743ce0f6c2862d5a038963894fbf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb28101b-1754-4bbd-a0c8-ef34cd45e6af", "node_type": "1", "metadata": {}, "hash": "4ba8c68102b285d2443181e300d78e08ffcc54e2bb3094e4eb71944e8c9b84e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1128\u20131132.\n[25] J. Pons and X. Serra, \u201cMUSICNN: Pre-trained convolutional neural\nnetworks for music audio tagging,\u201d arXiv preprint arXiv:1909.06654 ,\n2019.\n[26] A. Diment and T. Virtanen, \u201cTransfer learning of weakly labelled audio,\u201d\nin IEEE Workshop on Applications of Signal Processing to Audio and\nAcoustics (WASPAA), 2017, pp. 6\u201310.\n[27] D. Li, I. K. Sethi, N. Dimitrova, and T. McGee, \u201cClassi\ufb01cation of general\naudio data for content-based retrieval,\u201d Pattern Recognition Letters ,\nvol. 22, pp. 533\u2013544, 2001.\n[28] L. Vuegen, B. Broeck, P. Karsmakers, J. F. Gemmeke, B. Vanrumste,\nand H. Hamme, \u201cAn MFCC-GMM approach for event detection and\nclassi\ufb01cation,\u201d in IEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics (WASPAA) , 2013.\n[29] A. Mesaros, T. Heittola, A. Eronen, and T. Virtanen, \u201cAcoustic event\ndetection in real life recordings,\u201d in European Signal Processing Con-\nference (EUSIPCO), 2010, pp. 1267\u20131271.\n[30] B. Uzkent, B. D. Barkana, and H. Cevikalp, \u201cNon-speech environmental\nsound classi\ufb01cation using SVMs with a new set of features,\u201d Interna-\ntional Journal of Innovative Computing, Information and Control, vol. 8,\npp. 3511\u20133524, 2012.", "mimetype": "text/plain", "start_char_idx": 1109, "end_char_idx": 2286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb28101b-1754-4bbd-a0c8-ef34cd45e6af": {"__data__": {"id_": "fb28101b-1754-4bbd-a0c8-ef34cd45e6af", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "146b5908-3a87-4228-a108-6d7576a8a9d0", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "a1e3493ca80087222641533df104c3cc9d668f243081b1a77a97f0df0e50e377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dac5908-de69-45eb-b98d-0dc1b198c1ad", "node_type": "1", "metadata": {}, "hash": "6ea28517900792d3a8bc1d71fd5b7e8c2b71622a5927532e7d17ac8033f10fd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1267\u20131271.\n[30] B. Uzkent, B. D. Barkana, and H. Cevikalp, \u201cNon-speech environmental\nsound classi\ufb01cation using SVMs with a new set of features,\u201d Interna-\ntional Journal of Innovative Computing, Information and Control, vol. 8,\npp. 3511\u20133524, 2012.\n[31] W. Dai, C. Dai, S. Qu, J. Li, and S. Das, \u201cVery deep convolutional\nneural networks for raw waveforms,\u201d in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , 2017, pp. 421\u2013\n425.\n[32] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for\nimage recognition,\u201d inIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770\u2013778.\n[33] Q. Kong, Y . Cao, T. Iqbal, Y . Xu, W. Wang, and M. D. Plumbley,\n\u201cCross-task learning for audio tagging, sound event detection and\nspatial localization: DCASE 2019 baseline systems,\u201d arXiv preprint\narXiv:1904.03476, 2019.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classi\ufb01ca-\ntion with deep convolutional neural networks,\u201d in Advances in Neural\nInformation Processing Systems (NeurIPS) , 2012, pp. 1097\u20131105.", "mimetype": "text/plain", "start_char_idx": 2039, "end_char_idx": 3112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6dac5908-de69-45eb-b98d-0dc1b198c1ad": {"__data__": {"id_": "6dac5908-de69-45eb-b98d-0dc1b198c1ad", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb28101b-1754-4bbd-a0c8-ef34cd45e6af", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "70ec459b0c79362ddfb9f61e7dc45e8a311bf53f9bb1641926210d8243522c64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad128772-d46e-4256-bc31-81ad71f51ce8", "node_type": "1", "metadata": {}, "hash": "b4fbcf31d4b4f4a10520fa776d2529f1782557738d2008a3bf1ef0325b2ed9ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classi\ufb01ca-\ntion with deep convolutional neural networks,\u201d in Advances in Neural\nInformation Processing Systems (NeurIPS) , 2012, pp. 1097\u20131105.\n[35] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for\nlarge-scale image recognition,\u201d International Conference on Learning\nRepresentations (ICLR), 2015.\n[36] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,\u201d in International\nConference on Machine Learning (ICML) , 2015, pp. 448\u2013456.\n[37] V . Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted\nboltzmann machines,\u201d in International Conference on Machine Learning\n(ICML), 2010, pp. 807\u2013814.\n[38] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, \u201cDropout: A simple way to prevent neural networks from over-\n\ufb01tting,\u201d The Journal of Machine Learning Research , vol. 15, pp. 1929\u2013\n1958, 2014.\n[39] M. Lin, Q. Chen, and S. Yan, \u201cNetwork in network,\u201d in International\nConference on Learning Representations (ICLR) , 2014.", "mimetype": "text/plain", "start_char_idx": 2907, "end_char_idx": 4011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad128772-d46e-4256-bc31-81ad71f51ce8": {"__data__": {"id_": "ad128772-d46e-4256-bc31-81ad71f51ce8", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dac5908-de69-45eb-b98d-0dc1b198c1ad", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9c3902acf09b1133ea7e5dbdbdc88cfd3f43207898ed49dd58db939e3f25ccbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6e9a70e-6a72-401e-bf1f-9129b76ea142", "node_type": "1", "metadata": {}, "hash": "a48ee0ec081a344dfc761f5318e9cdc95cbbd194e3f444fce09a20098c36cd0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15, pp. 1929\u2013\n1958, 2014.\n[39] M. Lin, Q. Chen, and S. Yan, \u201cNetwork in network,\u201d in International\nConference on Learning Representations (ICLR) , 2014.\n[40] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, \u201cMobilenets: Ef\ufb01cient convo-\nlutional neural networks for mobile vision applications,\u201d arXiv preprint\narXiv:1704.04861, 2017.\n[41] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n\u201cMobileNetV2: Inverted residuals and linear bottlenecks,\u201d in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2018,\npp. 4510\u20134520.\n[42] J. Lee, J. Park, K. L. Kim, and J. Nam, \u201cSample-level deep convolutional\nneural networks for music auto-tagging using raw waveforms,\u201d in Sound\nand Music Computing Conference , 2017, pp. 220\u2013226.\n[43] J. Salamon, C. Jacoby, and J. P. Bello, \u201cA dataset and taxonomy for urban\nsound research,\u201d in Proceedings of the ACM International Conference\non Multimedia, 2014, pp. 1041\u20131044.\n[44] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, \u201cmixup: Beyond\nempirical risk minimization,\u201d in International Conference on Learning\nRepresentations (ICLR), 2018.", "mimetype": "text/plain", "start_char_idx": 3859, "end_char_idx": 5009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6e9a70e-6a72-401e-bf1f-9129b76ea142": {"__data__": {"id_": "f6e9a70e-6a72-401e-bf1f-9129b76ea142", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad128772-d46e-4256-bc31-81ad71f51ce8", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "7a4f7e554351510e7f18ec807e5ed637cb084f1b5f16766a7fa54671b46af2a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab501c3c-685e-4dec-8fbb-a834bd4f5e73", "node_type": "1", "metadata": {}, "hash": "6a976efe04cebf9a569f1723b123b0232c87fb028c7d45b6372f21199ab76cdb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1041\u20131044.\n[44] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, \u201cmixup: Beyond\nempirical risk minimization,\u201d in International Conference on Learning\nRepresentations (ICLR), 2018.\n[45] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, \u201cSpecAugment: A simple data augmentation method for\nautomatic speech recognition,\u201d in INTERSPEECH, 2019.\n[46] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg,\nand O. Nieto, \u201clibrosa: Audio and music signal analysis in python,\u201d\nin Proceedings of the Python in Science Conference , vol. 8, 2015, pp.\n18\u201325.\n[47] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\nin International Conference on Learning Representations (ICLR) , 2015.\n[48] L. Ford, H. Tang, F. Grondin, and J. Glass, \u201cA deep residual network\nfor large-scale acoustic scene analysis,\u201dINTERSPEECH, pp. 2568\u20132572,\n2019.\n[49] H. B. Sailor, D. M. Agrawal, and H. A. Patil, \u201cUnsupervised \ufb01lterbank\nlearning using convolutional restricted Boltzmann machine for environ-\nmental sound classi\ufb01cation,\u201d in INTERSPEECH, 2017, pp. 3107\u20133111.", "mimetype": "text/plain", "start_char_idx": 4825, "end_char_idx": 5929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab501c3c-685e-4dec-8fbb-a834bd4f5e73": {"__data__": {"id_": "ab501c3c-685e-4dec-8fbb-a834bd4f5e73", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6e9a70e-6a72-401e-bf1f-9129b76ea142", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "8e5b1397ad1362b9773c1bb8b7bbea303ac60c7339a090a5d4945056bbbfddb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7792e5c-1b7b-4831-8bf6-9817c9338395", "node_type": "1", "metadata": {}, "hash": "eb0cf6040b9bafd19ce5035e8d80fc21daefc0787a337fb0f69ee1691017d6d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2568\u20132572,\n2019.\n[49] H. B. Sailor, D. M. Agrawal, and H. A. Patil, \u201cUnsupervised \ufb01lterbank\nlearning using convolutional restricted Boltzmann machine for environ-\nmental sound classi\ufb01cation,\u201d in INTERSPEECH, 2017, pp. 3107\u20133111.\n[50] K. J. Piczak, \u201cESC: Dataset for environmental sound classi\ufb01cation,\u201d in\nACM International Conference on Multimedia , 2015, pp. 1015\u20131018.\n[51] H. Chen, Z. Liu, Z. Liu, P. Zhang, and Y . Yan, \u201cIntegrating the\ndata augmentation scheme with various classi\ufb01ers for acoustic scene\nmodeling,\u201d DCASE2019 Challenge, Tech. Rep. , 2019.\n[52] I.-Y . Jeong and H. Lim, \u201cAudio tagging system for DCASE 2018:\nfocusing on label noise data augmentation and its ef\ufb01cient learning,\u201d\nDCASE Challenge Tech. Rep. , 2018.\n[53] T. Chen and U. Gupta, \u201cAttention-based convolutional neural network\nfor audio event classi\ufb01cation with feature transfer learning,\u201d\nhttps://cvssp.org/projects/making_sense_of_sounds/site/assets/\nchallenge_abstracts_and_\ufb01gures/Tianxiang_Chen.pdf , 2018.", "mimetype": "text/plain", "start_char_idx": 5701, "end_char_idx": 6690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7792e5c-1b7b-4831-8bf6-9817c9338395": {"__data__": {"id_": "c7792e5c-1b7b-4831-8bf6-9817c9338395", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab501c3c-685e-4dec-8fbb-a834bd4f5e73", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "760fe2b3f68cb237359d23d9697586a2389465d8d0c0935029123f231f3aa589", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7a6bb0b-3ed8-4597-a07d-b7a1e6a303e3", "node_type": "1", "metadata": {}, "hash": "05619e89897d8fbe55fb93b05deabe11281036fa535051d52fa482081daeddbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rep. , 2018.\n[53] T. Chen and U. Gupta, \u201cAttention-based convolutional neural network\nfor audio event classi\ufb01cation with feature transfer learning,\u201d\nhttps://cvssp.org/projects/making_sense_of_sounds/site/assets/\nchallenge_abstracts_and_\ufb01gures/Tianxiang_Chen.pdf , 2018.\n[54] E. Fonseca, M. Plakal, F. Font, D. P. W. Ellis, X. Favory, J. Pons, and\nX. Serra, \u201cGeneral-purpose tagging of freesound audio with audioset la-\nbels: Task description, dataset, and baseline,\u201d in Workshop on Detection\nand Classi\ufb01cation of Acoustic Scenes and Events (DCASE) , November\n2018, pp. 69\u201373.\n[55] C. Kroos, O. Bones, Y . Cao, L. Harris, P. J. Jackson, W. J. Davies,\nW. Wang, T. J. Cox, and M. D. Plumbley, \u201cGeneralisation in environ-\nmental sound classi\ufb01cation: The \u2018Making Sense of Sounds\u2019 data set and\nchallenge,\u201d in IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 2019, pp. 8082\u20138086.\n[56] C. Liu, L. Feng, G. Liu, H. Wang, and S. Liu, \u201cBottom-up broad-\ncast neural network for music genre classi\ufb01cation,\u201d arXiv preprint\narXiv:1901.08928, 2019.\n[57] G. Tzanetakis and P. Cook, \u201cMusical genre classi\ufb01cation of audio\nsignals,\u201d IEEE Transactions on Speech and Audio Processing , vol. 10,\npp.", "mimetype": "text/plain", "start_char_idx": 6421, "end_char_idx": 7632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7a6bb0b-3ed8-4597-a07d-b7a1e6a303e3": {"__data__": {"id_": "d7a6bb0b-3ed8-4597-a07d-b7a1e6a303e3", "embedding": null, "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "9d94c1f0d3750e87a930a78cb23106fca8c4b8e761eb86fab8a87c0173b4553f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7792e5c-1b7b-4831-8bf6-9817c9338395", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "0ce639489f89d69c640b351892296d9772209baf3f46e507b34eccdda8b58877", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[57] G. Tzanetakis and P. Cook, \u201cMusical genre classi\ufb01cation of audio\nsignals,\u201d IEEE Transactions on Speech and Audio Processing , vol. 10,\npp. 293\u2013302, 2002.\n[58] Y . Zeng, H. Mao, D. Peng, and Z. Yi, \u201cSpectrogram based multi-task\naudio classi\ufb01cation,\u201d Multimedia Tools and Applications , vol. 78, pp.\n3705\u20133722, 2019.\n[59] S. R. Livingstone, K. Peck, and F. A. Russo, \u201cRA VDESS: The Ryerson\naudio-visual database of emotional speech and song,\u201d in Annual Meeting\nof the Canadian Society for Brain, Behaviour and Cognitive Science\n(CSBBCS), 2012, pp. 1459\u20131462.", "mimetype": "text/plain", "start_char_idx": 7489, "end_char_idx": 8050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5daf3360-184e-4a38-a691-509fb35e860d": {"__data__": {"id_": "5daf3360-184e-4a38-a691-509fb35e860d", "embedding": null, "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59eef68f-016b-4d5d-87fc-59674048a3c2", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e34c95ae9fe692778d63be1a8a4acff44514d05ad7f72194eb46c883026a6c62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2da1461d-d860-4f75-9bf8-18b20f510508", "node_type": "1", "metadata": {}, "hash": "6ab9d35d41813e140f05bf7bf3661bec5492e2251d09a4fd1e0031ca562639f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\nQiuqiang Kong (S\u201917) received the B.Sc. degree in\n2012, and the M.E. degree in 2015 from South China\nUniversity of Technology, Guangzhou, China. He\nreceived the Ph.D. degree from University of Surrey,\nGuildford, UK in 2020. Following his PhD, he\njoined ByteDance AI Lab as a research scientist. His\nresearch topic includes the classi\ufb01cation, detection\nand separation of general sounds and music. He\nis known for developing attention neural networks\nfor audio tagging, and winning the audio tagging\ntask in the detection and classi\ufb01cation of acoustic\nscenes and events (DCASE) challenge in 2017. He was nominated as the\npostgraduate research student of the year in University of Surrey, 2019. He\nis a frequent reviewer for journals and conferences in the area including\nIEEE/ACM Transactions on Audio, Speech, and Language Processing.\nYin Cao (M\u201918) received the B.Sc. degree in Elec-\ntronic Science and Engineering from Nanjing Uni-\nversity, China in 2008, and Ph.D. degree from In-\nstitute of Acoustics, Chinese Academy of Sciences,\nChina in 2013. He then worked in Acoustics group\nat Brigham Young University, US, and at Institute of\nAcoustics, Chinese Academy of Sciences, China. In\n2018, he joined Centre for Vision, Speech and Signal\nProcessing at the University of Surrey. His research\ntopic includes active noise control, air acoustics\nand signal processing, detection, classi\ufb01cation and\nseparation of audio. He is known for his work on decentralized active noise\ncontrol, weighted spatial gradients control metric, and polyphonic sound event\ndetection and localization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2da1461d-d860-4f75-9bf8-18b20f510508": {"__data__": {"id_": "2da1461d-d860-4f75-9bf8-18b20f510508", "embedding": null, "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59eef68f-016b-4d5d-87fc-59674048a3c2", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e34c95ae9fe692778d63be1a8a4acff44514d05ad7f72194eb46c883026a6c62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5daf3360-184e-4a38-a691-509fb35e860d", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "b4491ebb0ea53feee2bf4036860df31d0ef149716695c69db0619495506fe75d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406", "node_type": "1", "metadata": {}, "hash": "04974fb2e5af4e7c858e1cc7694831df43ac70a17aee05f89454ac2afad5851f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In\n2018, he joined Centre for Vision, Speech and Signal\nProcessing at the University of Surrey. His research\ntopic includes active noise control, air acoustics\nand signal processing, detection, classi\ufb01cation and\nseparation of audio. He is known for his work on decentralized active noise\ncontrol, weighted spatial gradients control metric, and polyphonic sound event\ndetection and localization. He was the winner of urban sound tagging in\ndetection and classi\ufb01cation of acoustic scenes and events (DCASE) 2020\nchallenge and achieved second-best of sound event detection and localization\ntasks in DCASE 2019 challenge. He has served as an Associate Editor for\nNoise Control Engineering Journal since 2020. He is also a frequent reviewer\nfor IEEE/ACM Transactions on Audio, Speech, and Language Processing.\nTurab Iqbal received the B.Eng. degree in Elec-\ntronic Engineering from the University of Surrey,\nU.K., in 2017. Currently, he is working towards a\nPh.D. degree from the Centre for Vision, Speech\nand Signal Processing (CVSSP) in the University\nof Surrey. During his time as a Ph.D. student, he\nhas worked on a number of projects in the area of\naudio classi\ufb01cation and localization using machine\nlearning methods. His research is mainly focused on\nlearning with weakly-labeled or noisy training data.\nWenwu Wang (M\u201902-SM\u201911) was born in Anhui,\nChina. He received the B.Sc. degree in 1997, the\nM.E. degree in 2000, and the Ph.D. degree in 2002,\nall from Harbin Engineering University, China.", "mimetype": "text/plain", "start_char_idx": 1186, "end_char_idx": 2680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406": {"__data__": {"id_": "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406", "embedding": null, "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59eef68f-016b-4d5d-87fc-59674048a3c2", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e34c95ae9fe692778d63be1a8a4acff44514d05ad7f72194eb46c883026a6c62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2da1461d-d860-4f75-9bf8-18b20f510508", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "71c091d2b1d4a40cf68a8b3d16da220b9e61e40f8af7c7bd60bb5fb95e8171b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab17625-df39-4728-9b21-0b82470b49d0", "node_type": "1", "metadata": {}, "hash": "443ab57b7b913530c01bc3467986e183b6d14b85c39a5f0f8208e420d8fcb7fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "His research is mainly focused on\nlearning with weakly-labeled or noisy training data.\nWenwu Wang (M\u201902-SM\u201911) was born in Anhui,\nChina. He received the B.Sc. degree in 1997, the\nM.E. degree in 2000, and the Ph.D. degree in 2002,\nall from Harbin Engineering University, China. He\nthen worked in King\u2019s College London, Cardiff Uni-\nversity, Tao Group Ltd. (now Antix Labs Ltd.), and\nCreative Labs, before joining University of Surrey,\nUK, in May 2007, where he is currently a professor\nin signal processing and machine learning, and a\nCo-Director of the Machine Audition Lab within\nthe Centre for Vision Speech and Signal Processing.\nHe has been a Guest Professor at Qingdao University of Science and\nTechnology, China, since 2018. His current research interests include blind\nsignal processing, sparse signal processing, audio-visual signal processing,\nmachine learning and perception, machine audition (listening), and statistical\nanomaly detection. He has (co)-authored over 200 publications in these\nareas. He served as an Associate Editor for IEEE Transactions on Signal\nProcessing from 2014 to 2018. He is also Publication Co-Chair for ICASSP\n2019, Brighton, UK. He currently serves as Senior Area Editor for IEEE\nTransactions on Signal Processing and an Associate Editor for IEEE/ACM\nTransactions on Audio Speech and Language Processing.\nMark D. Plumbley (S\u201988-M\u201990-SM\u201912-F\u201915) re-\nceived the B.A.(Hons.) degree in electrical sciences\nand the Ph.D. degree in neural networks from Uni-\nversity of Cambridge, Cambridge, U.K., in 1984 and\n1991, respectively.", "mimetype": "text/plain", "start_char_idx": 2404, "end_char_idx": 3965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ab17625-df39-4728-9b21-0b82470b49d0": {"__data__": {"id_": "9ab17625-df39-4728-9b21-0b82470b49d0", "embedding": null, "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59eef68f-016b-4d5d-87fc-59674048a3c2", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "e34c95ae9fe692778d63be1a8a4acff44514d05ad7f72194eb46c883026a6c62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}, "hash": "5b1f8129300f326b9442fa7a1ce2e6fdd07f5d05e9da6c27209699bfb32200f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mark D. Plumbley (S\u201988-M\u201990-SM\u201912-F\u201915) re-\nceived the B.A.(Hons.) degree in electrical sciences\nand the Ph.D. degree in neural networks from Uni-\nversity of Cambridge, Cambridge, U.K., in 1984 and\n1991, respectively. Following his PhD, he became a\nLecturer at King\u2019s College London, before moving\nto Queen Mary University of London in 2002. He\nsubsequently became Professor and Director of the\nCentre for Digital Music, before joining the Uni-\nversity of Surrey in 2015 as Professor of Signal\nProcessing. He is known for his work on analysis\nand processing of audio and music, using a wide range of signal processing\ntechniques, including matrix factorization, sparse representations, and deep\nlearning. He is a co-editor of the recent book on Computational Analysis of\nSound Scenes and Events, and Co-Chair of the recent DCASE 2018 Workshop\non Detection and Classi\ufb01cations of Acoustic Scenes and Events. He is a\nMember of the IEEE Signal Processing Society Technical Committee on\nSignal Processing Theory and Methods, and a Fellow of the IET and IEEE.", "mimetype": "text/plain", "start_char_idx": 3748, "end_char_idx": 4801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fabd56dc-5425-42f3-8da3-4c08ee4e9107": {"__data__": {"id_": "fabd56dc-5425-42f3-8da3-4c08ee4e9107", "embedding": null, "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7225db7c242b76520531363125fbc9f38d4234fd069ef2bc77dc9995fde95ccb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08317b88-fcf7-47cf-8bde-5a4a6c23705e", "node_type": "1", "metadata": {}, "hash": "87e6119c9957aac97e39ebb190b6a9733f6478d7af369397e39a565147c928e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1\nPSLA: Improving Audio Tagging with Pretraining,\nSampling, Labeling, and Aggregation\nYuan Gong, Member, IEEE, Yu-An Chung, Member, IEEE, and James Glass, Fellow, IEEE\nAbstract\u2014Audio tagging is an active research area and has\na wide range of applications. Since the release of AudioSet,\ngreat progress has been made in advancing model performance,\nwhich mostly comes from the development of novel model\narchitectures and attention modules. However, we \ufb01nd that ap-\npropriate training techniques are equally important for building\naudio tagging models with AudioSet, but have not received\nthe attention they deserve. To \ufb01ll the gap, in this work, we\npresent PSLA, a collection of model agnostic training tech-\nniques that can noticeably boost the model accuracy including\nImageNet pretraining, balanced sampling, data augmentation,\nlabel enhancement, model aggregation. While many of these\ntechniques have been previously explored, we conduct a thorough\ninvestigation on their design choices and combine them together.\nBy training an Ef\ufb01cientNet with pretraining, balanced sampling,\ndata augmentation, and model aggregation, we obtain a single\nmodel (with 13.6M parameters) and an ensemble model that\nachieve mean average precision (mAP) scores of 0.444 and 0.474\non AudioSet, respectively, outperforming the previous best system\nof 0.439 with 81M parameters. In addition, our model also\nachieves a new state-of-the-art mAP of 0.567 on FSD50K. We\nalso investigate the impact of label enhancement on the model\nperformance. Code at https://github.com/YuanGongND/psla.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08317b88-fcf7-47cf-8bde-5a4a6c23705e": {"__data__": {"id_": "08317b88-fcf7-47cf-8bde-5a4a6c23705e", "embedding": null, "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7225db7c242b76520531363125fbc9f38d4234fd069ef2bc77dc9995fde95ccb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fabd56dc-5425-42f3-8da3-4c08ee4e9107", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "b5ac81972ef80af569708c0603b116cbfecc64dd8fe4963e2ae2fa9db0296970", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "050258ec-7ae1-45ef-a38f-a7d3449c215f", "node_type": "1", "metadata": {}, "hash": "0e3c8ad291801cf1eec7c07212b013df92e05323e2ee49c0e0d26152c9521c6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, our model also\nachieves a new state-of-the-art mAP of 0.567 on FSD50K. We\nalso investigate the impact of label enhancement on the model\nperformance. Code at https://github.com/YuanGongND/psla.\nIndex Terms\u2014Audio tagging, Audio event classi\ufb01cation, trans-\nfer learning, imbalanced learning, noisy label, ensemble\nI. I NTRODUCTION\nAudio tagging aims to identify sound events that occur in\na given audio recording, and enables a variety of Arti\ufb01cial\nIntelligence-based systems to disambiguate sounds and un-\nderstand the acoustic environment. Audio tagging has a wide\nrange of health and safety applications in the home, of\ufb01ce,\nindustry, transportation, and has become an active research\ntopic in the \ufb01eld of acoustic signal processing.\nIn recent years, audio tagging and classi\ufb01cation research has\nmoved from small and/or constrained datasets such as ESC-\n50 [1] and CHiME-Home [2] to much larger datasets with\na greater variety and range of real-world audio events and\nsubstantially more training data. A signi\ufb01cant milestone in\nthis \ufb01eld occurred with the release of the AudioSet corpus [3]\ncontaining over 2 million 10-second audio clips extracted\nfrom video and tagged at the utterance level with a set of\n527 event labels. AudioSet is currently the largest and most\ncomprehensive publicly available dataset for audio tagging.\nNot surprisingly, it has subsequently become the primary\nsource of training and evaluation material for audio tagging\nresearch.", "mimetype": "text/plain", "start_char_idx": 1423, "end_char_idx": 2891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "050258ec-7ae1-45ef-a38f-a7d3449c215f": {"__data__": {"id_": "050258ec-7ae1-45ef-a38f-a7d3449c215f", "embedding": null, "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7225db7c242b76520531363125fbc9f38d4234fd069ef2bc77dc9995fde95ccb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08317b88-fcf7-47cf-8bde-5a4a6c23705e", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "dcd37f132a89bf2922bbe5c390249a0b18e11041ff32f1206ce20eb722f46dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ef40cf7-9c5c-470e-ba37-4d926b72ca18", "node_type": "1", "metadata": {}, "hash": "4e1ca7e1fea33953fac9a4bb96b314595933a97a956b8ad00468130a42ef2a47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A signi\ufb01cant milestone in\nthis \ufb01eld occurred with the release of the AudioSet corpus [3]\ncontaining over 2 million 10-second audio clips extracted\nfrom video and tagged at the utterance level with a set of\n527 event labels. AudioSet is currently the largest and most\ncomprehensive publicly available dataset for audio tagging.\nNot surprisingly, it has subsequently become the primary\nsource of training and evaluation material for audio tagging\nresearch. The availability of AudioSet has encouraged much\naudio tagging research that has steadily seen the standard\nThe authors are with the Computer Science and Arti\ufb01cial Intelligence\nLaboratory, Massachusetts Institute of Technology, Cambridge, MA 02139\nUSA (e-mail: yuangong@mit.edu; andyyuan@mit.edu; glass@mit.edu).\nAudioSetNoisy Label SetClass-imbalanced dataset\nBalanced SamplingData Augmentation\nEfficientNetModelImageNetCross-modal PretrainingEnhancedLabel SetLabel EnhancementEfficientNetModelEfficientNetModelWeight Averaging and EnsembleProposed Model\nFig. 1. The proposed Pretraining, Sampling, Labeling, and Aggregation\n(PSLA) training pipeline. AudioSet is extremely class imbalanced and has\nprevalent annotation errors, we propose a data augmentation/balanced sam-\npling strategy and a label enhancement strategy to alleviate these two prob-\nlems. We also pretrain the convolutional neural networks with ImageNet and\n\ufb01nd it leads to a noticeable performance improvement.", "mimetype": "text/plain", "start_char_idx": 2437, "end_char_idx": 3870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ef40cf7-9c5c-470e-ba37-4d926b72ca18": {"__data__": {"id_": "2ef40cf7-9c5c-470e-ba37-4d926b72ca18", "embedding": null, "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7225db7c242b76520531363125fbc9f38d4234fd069ef2bc77dc9995fde95ccb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "050258ec-7ae1-45ef-a38f-a7d3449c215f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c9dd5e41e21282f0eb26819209de32ea7c26e38af0d552703f096e0c4e917bbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cbae6f0-59b3-4166-9d9d-7846cb093a6e", "node_type": "1", "metadata": {}, "hash": "b02163c11b06e02ee41d2ac5ef455ac57cfaca4256f19208b795a529c88e6826", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. The proposed Pretraining, Sampling, Labeling, and Aggregation\n(PSLA) training pipeline. AudioSet is extremely class imbalanced and has\nprevalent annotation errors, we propose a data augmentation/balanced sam-\npling strategy and a label enhancement strategy to alleviate these two prob-\nlems. We also pretrain the convolutional neural networks with ImageNet and\n\ufb01nd it leads to a noticeable performance improvement. By further aggregating\nmultiple models with weight averaging and ensemble techniques, we get a\nmodel that performs much better than that trained with a conventional pipeline\nand achieves a new state-of-the-art mAP of 0.474.\nevaluation metric of mean average precision (mAP) increase\nfrom, for example, 0.314 with shallow fully-connected net-\nworks [3], to 0.392 with a residual network with attention [4]\nto, most recently, 0.439 with spectrogram and waveform-based\nconvolutional neural networks (CNNs) [5]. In order to cope\nwith the weakly labeled data, multiple instance learning and\nattention mechanisms have also been the subject of much\ninvestigation [6], [7], [8], [9].\nIn our audio tagging experiments using Audioset we have\nobserved that, in addition to the particular model architecture\nbeing evaluated, signi\ufb01cant performance improvements can be\nachieved via training techniques including cross-modal pre-\ntraining, data augmentation, label enhancement, and ensemble\nmodeling. Our empirical evaluations show that these model\nagnostic techniques lead to signi\ufb01cant accuracy improvements,\nand combining them together can further boost the model ac-\ncuracy.", "mimetype": "text/plain", "start_char_idx": 3453, "end_char_idx": 5035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7cbae6f0-59b3-4166-9d9d-7846cb093a6e": {"__data__": {"id_": "7cbae6f0-59b3-4166-9d9d-7846cb093a6e", "embedding": null, "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345", "node_type": "4", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7225db7c242b76520531363125fbc9f38d4234fd069ef2bc77dc9995fde95ccb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef40cf7-9c5c-470e-ba37-4d926b72ca18", "node_type": "1", "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2054674e3b5c8774d2e472a3a6f29ad63da2b33b1f763d979659e08062ab4971", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our audio tagging experiments using Audioset we have\nobserved that, in addition to the particular model architecture\nbeing evaluated, signi\ufb01cant performance improvements can be\nachieved via training techniques including cross-modal pre-\ntraining, data augmentation, label enhancement, and ensemble\nmodeling. Our empirical evaluations show that these model\nagnostic techniques lead to signi\ufb01cant accuracy improvements,\nand combining them together can further boost the model ac-\ncuracy. Speci\ufb01cally, we train an ensemble of Ef\ufb01cientNet [10]\nmodels with the proposed set of training techniques and\nachieve a new state-of-the-art mAP of 0.474 on AudioSet,\nour single model with 13.6M parameters also achieves an\nmAP of 0.444, outperforming the previous the best system\nthat contained 81M parameters. In addition, our model also\nachieves a new state-of-the-art mAP of 0.567 on the FSD50K\nbenchmark [11].\nAs shown in Figure 1, the training techniques we investi-\narXiv:2102.01243v3  [cs.SD]  17 Nov 2021", "mimetype": "text/plain", "start_char_idx": 4547, "end_char_idx": 5548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9624ed82-27b2-4111-b2d2-48e4cabc77ad": {"__data__": {"id_": "9624ed82-27b2-4111-b2d2-48e4cabc77ad", "embedding": null, "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536708b9-e556-4812-9f4d-6d823e99f4e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f397b13e1e933bdc8ecdf2cf8408df2eb8b2fa2d9ced19e9bec9b3ff38240768", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf768e2f-d631-4843-b4b7-b7d61cd91fdc", "node_type": "1", "metadata": {}, "hash": "2fba0c837cea287d213b7b31c94a7223120c5024ae9f577ef6c52b49bdf7097c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 2\ngated fall into four main categories. First, we \ufb01nd cross-modal\npretraining with ImageNet [12] improves the performance of\naudio tagging CNNs even though AudioSet already contains a\nsubstantial amount of in-domain data. Second, we address the\nAudioset label imbalance by adopting balanced sampling and\ndata augmentation. Third, we observed that there are pervasive\nannotation errors in AudioSet and studied the impact of such\nannotation errors on the model performance. We further devel-\noped a method to improve training label quality. Fourth, we use\nweight averaging and ensemble methods to improve the overall\nperformance. Many of these techniques have been proposed\npreviously in isolation. For example, ImageNet pretraining\nhas been used in [13] for small datasets, balanced sampling\nand data augmentation have been used in [5], label enhance-\nment has been proposed in [14], and ensemble modeling has\nbeen used in [4], [15], [16]. To the best of our knowledge\nhowever, none of the prior efforts have used more than two\nof these simultaneously, and the particular implementation is\noften only brie\ufb02y mentioned in the literature. In this paper,\nwe thoroughly investigate each of these techniques, a more\nthorough understanding of the bene\ufb01ts of different training\ntechniques should facilitate a more meaningful comparison\nbetween various works because performance differences due to\nthe particular training procedure could overshadow the model\narchitecture or other novel techniques being investigated. The\ntraining pipeline we propose is model-agnostic and can serve\nas a recipe for AudioSet tagging experiments to facilitate fair\ncomparisons with new techniques.\nThe contributions of this work are summarized as follows:\n1) We present a collection of training strategies and design\nchoices for audio tagging. We quantify the improvement\nof each component via extensive experimentation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf768e2f-d631-4843-b4b7-b7d61cd91fdc": {"__data__": {"id_": "bf768e2f-d631-4843-b4b7-b7d61cd91fdc", "embedding": null, "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536708b9-e556-4812-9f4d-6d823e99f4e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f397b13e1e933bdc8ecdf2cf8408df2eb8b2fa2d9ced19e9bec9b3ff38240768", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9624ed82-27b2-4111-b2d2-48e4cabc77ad", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "619e0efbe012be0bebbd5eaa5822a22510e51aab99efa79090c6e7427d080671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "399fda60-8bba-4abb-a099-0a96bf6bb0d0", "node_type": "1", "metadata": {}, "hash": "3ff087708aad971f47d1288970ff5a574124c3e0aa784fc48338493a884ffac6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The\ntraining pipeline we propose is model-agnostic and can serve\nas a recipe for AudioSet tagging experiments to facilitate fair\ncomparisons with new techniques.\nThe contributions of this work are summarized as follows:\n1) We present a collection of training strategies and design\nchoices for audio tagging. We quantify the improvement\nof each component via extensive experimentation.\n2) By training an ensemble of standard Ef\ufb01cientNet models\nwith the proposed training procedure, we achieve a new\nstate-of-the-art mAP of 0.474 on AudioSet, outperform-\ning the best previous system of 0.439.\n3) We release the code, model, and enhanced label set.\nThe training pipeline can serve as a recipe of AudioSet\ntraining to facilitate future audio tagging research.\nThe paper is organized as follows. We \ufb01rst describe the\nbaseline model architecture in Section II, then we gradually\nimprove the baseline model performance on AudioSet by\nadding new training techniques in Sections III, IV, V, and VI.\nIn each section, we \ufb01rst review the corresponding technique\nand then present our implementation and results. We present\nan ablation study, experiments on FSD50K and other model\narchitectures, and a discussion of the results in Section VII.\nWe conclude the paper in Section VIII.\nII. E XPERIMENT SETTING AND BASELINE MODEL\nA. Dataset\nIn this work, we mainly focus on AudioSet [3], a collection\nof over 2 million 10-second audio clips excised from YouTube\nvideos and labeled with the sounds that the clip contains from\na set of 527 labels. AudioSet is a weakly labeled and multi-\nlabel dataset, i.e., labels are given to a clip with no indication\nof where in the clip the associated sound occurred, and every\nTABLE I\nTHE AUDIO SET [3] S TATISTICS .", "mimetype": "text/plain", "start_char_idx": 1573, "end_char_idx": 3310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "399fda60-8bba-4abb-a099-0a96bf6bb0d0": {"__data__": {"id_": "399fda60-8bba-4abb-a099-0a96bf6bb0d0", "embedding": null, "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536708b9-e556-4812-9f4d-6d823e99f4e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f397b13e1e933bdc8ecdf2cf8408df2eb8b2fa2d9ced19e9bec9b3ff38240768", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf768e2f-d631-4843-b4b7-b7d61cd91fdc", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cdb6e903c18c60a3b6a609c607488082183e8b5c8cc5298297f8fc514e30ea33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60d14978-751a-4035-a64b-2c35a8774b7d", "node_type": "1", "metadata": {}, "hash": "d93c352a57a0a23eee555a18b0f9ce744fec66c33e0ca4366237b886a51a0b95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AudioSet is a weakly labeled and multi-\nlabel dataset, i.e., labels are given to a clip with no indication\nof where in the clip the associated sound occurred, and every\nTABLE I\nTHE AUDIO SET [3] S TATISTICS .\nBalanced Train Full Train Evaluation\nAudioSet 22,176 2,065,161 20,383\nDownloaded 20,785 1,953,082 19,185\nDownloaded Ratio 93.7% 94.6% 94.1%\nclip can, and often does, have multiple labels associated\nwith it. As shown in Table I, the dataset is split into three\nsubsets: balanced train, unbalanced train, and evaluation. In\nthis paper, we combine the balanced and unbalanced training\nset as the full training set. The balanced train dataset is a\nset of 22,176 recordings, where each class has at least 49\nsamples, while the full train set contains the entire 2 million\nrecordings. The evaluation set consists of 20,383 recordings\nand contains at least 59 examples for each class. To obtain\nthe raw audio, we extracted the dataset from YouTube. Due\nto the constant change in video availability (e.g., videos being\nremoved, taken down) there is a natural shrinkage (about 5%)\nfrom the original dataset [3]. Speci\ufb01cally, we downloaded\n20,785 (94%), 1,953,082 (95%), and 19,185 (94%) recordings\nfor the balanced train, full train, and evaluation set, respec-\ntively, which is consistent with previous literature (e.g., [5]).\nTherefore, we do make fair comparisons with previous state-\nof-the-art models by evaluating on the same subset of the\nevaluation dataset.", "mimetype": "text/plain", "start_char_idx": 3102, "end_char_idx": 4567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60d14978-751a-4035-a64b-2c35a8774b7d": {"__data__": {"id_": "60d14978-751a-4035-a64b-2c35a8774b7d", "embedding": null, "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536708b9-e556-4812-9f4d-6d823e99f4e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f397b13e1e933bdc8ecdf2cf8408df2eb8b2fa2d9ced19e9bec9b3ff38240768", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "399fda60-8bba-4abb-a099-0a96bf6bb0d0", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "86d93d0edf5a7bbd3a99e6077dbbf770e1d2fa329d7aca5629bba67484bdcbf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cac01f70-5501-4a11-bbd1-0c911cc47565", "node_type": "1", "metadata": {}, "hash": "e977df3b8e6c316f939a80e10926d8d33bbea25331cec899ece0b455295f0c65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Speci\ufb01cally, we downloaded\n20,785 (94%), 1,953,082 (95%), and 19,185 (94%) recordings\nfor the balanced train, full train, and evaluation set, respec-\ntively, which is consistent with previous literature (e.g., [5]).\nTherefore, we do make fair comparisons with previous state-\nof-the-art models by evaluating on the same subset of the\nevaluation dataset.\nWe also evaluate the proposed PSLA training framework\non FSD50K [11], a recently collected data set of sound event\naudio clips with 200 classes drawn from the AudioSet ontol-\nogy to see how the PSLA framework generalizes. FSD50K\ncontains 37,134 audio clips for training, 4,170 audio clips for\nvalidation, and 10,231 audio clips for evaluation. The audio\nclips are of variable length from 0.3 to 30s with an average\nof 7.6s (7.1s for the training and validation set, 9.8s for the\nevaluation set). For both AudioSet and FSD50K, we sample\nthe audio at 16kHz.\nB. Training and Evaluation Details\nFor all AudioSet experiments in this paper, we train the\nneural network model with a batch size of 100, the Adam\noptimizer [17], and use binary cross-entropy (BCE) loss. We\nuse a \ufb01xed initial learning rate of 1e-3 and 1e-4 and cut it\nin half every 5 epochs after the 35th and 10th epoch for all\nbalanced set and full set experiments, respectively.", "mimetype": "text/plain", "start_char_idx": 4214, "end_char_idx": 5506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cac01f70-5501-4a11-bbd1-0c911cc47565": {"__data__": {"id_": "cac01f70-5501-4a11-bbd1-0c911cc47565", "embedding": null, "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536708b9-e556-4812-9f4d-6d823e99f4e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f397b13e1e933bdc8ecdf2cf8408df2eb8b2fa2d9ced19e9bec9b3ff38240768", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60d14978-751a-4035-a64b-2c35a8774b7d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "6640d1d9e2bcf51a12f51e67ca7e3b27330388cc2c50c4cd319587617b3865d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We\nuse a \ufb01xed initial learning rate of 1e-3 and 1e-4 and cut it\nin half every 5 epochs after the 35th and 10th epoch for all\nbalanced set and full set experiments, respectively. The reason\nwhy a smaller learning rate is used for the full AudioSet is\nthat the full set is about 100 times larger than the balanced\nset, using a smaller learning rate can avoid the model falling\ninto a local minima before it sees all samples. We use a linear\nlearning rate warm-up strategy for the \ufb01rst 1,000 iterations. As\nin previous efforts, we train the model with 60 and 30 epochs\nfor all balanced set and full set experiments, respectively, and\nreport the mean result on the evaluation set of the last 5 epochs.\nWe use the mean average precision (mAP) of all the classes\nas our main evaluation metric since it is the most commonly\nused audio tagging evaluation metric on AudioSet. Mean\naverage precision is an approximation of the area under a", "mimetype": "text/plain", "start_char_idx": 5329, "end_char_idx": 6258, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae2475ef-2e4f-442e-8df5-f88ca66ea1cc": {"__data__": {"id_": "ae2475ef-2e4f-442e-8df5-f88ca66ea1cc", "embedding": null, "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5da07a27b6136b12e8577f8e7030c5377cd9a1651ff2e92634e5dd83a94a002c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6eeea9b-7c28-44e1-8579-329a78e4526d", "node_type": "1", "metadata": {}, "hash": "ac1a24399670223695f7b2ceb1eefda30e0a58ee325ae00c4122bbc0303857a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 3\nLog fbank(1056 X 128)\nEfficientNet\nTime-frequency Representation\n(4 X 527)\nInput(33 X 1408)\n1X1 Conv1X1 Conv\n(33 X 527)Temporal Mean Pooling\nSigmoidSigmoidFrequency Mean Pooling NormalizeElement-wise Product\n(33 X 527)\nOutput(527)\n(33 X 1408)\n4X\nAttention PoolingAttention PoolingAttention PoolingAttention Pooling\nPrediction (527)Weighted Averaging\n(33 X 4 X 1408)\nFig. 2. The audio tagging model used in this work. The 10-second waveform\nis \ufb01rst converted to a 1056 \u00d7128 log Mel \ufb01lterbank (fbank) feature vector\nand input to the Ef\ufb01cientNet model. The output of the penultimate layer of\nEf\ufb01cientNet is a 33 \u00d74 \u00d71408 tensor. We apply a frequency mean pooling\nto produce a 33 \u00d71408 representation that is fed into a 4-headed attention\npooling module. In each head, the CNN output is transformed into a 33 \u00d7\n527 dimensional tensor via a set of 1 \u00d71 convolution layers with a parallel\nattention branch and classi\ufb01cation branch. We multiply the output of each\nbranch element-wise and apply a temporal mean pooling (implemented by\nsummation). Finally, we sum the weighted output of each attention head after\nit has been scaled by a learnable weight and produce the \ufb01nal prediction for\nall classes.\nclass\u2019s precision-recall curve, which is more informative of\nperformance when dealing with imbalanced datasets such as\nAudioSet and FSD50k compared with the average area under\nthe curve of the receiver operating characteristic curve [18],\n[19].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6eeea9b-7c28-44e1-8579-329a78e4526d": {"__data__": {"id_": "d6eeea9b-7c28-44e1-8579-329a78e4526d", "embedding": null, "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5da07a27b6136b12e8577f8e7030c5377cd9a1651ff2e92634e5dd83a94a002c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae2475ef-2e4f-442e-8df5-f88ca66ea1cc", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f758b10d42d11ac244c0bb2b15caf83d3292a0629737f05993a7abb7a8b85d56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22", "node_type": "1", "metadata": {}, "hash": "b6edfa6fc60afc3dce77d0b821b0883e907a1e0917eac15df94ca415d33f91fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we sum the weighted output of each attention head after\nit has been scaled by a learnable weight and produce the \ufb01nal prediction for\nall classes.\nclass\u2019s precision-recall curve, which is more informative of\nperformance when dealing with imbalanced datasets such as\nAudioSet and FSD50k compared with the average area under\nthe curve of the receiver operating characteristic curve [18],\n[19]. In the discussion section, we also report the average area\nunder the curve (AUC) of the receiver operating characteristic\ncurve and sensitivity index (d-prime) in order to compare our\nmodel with previous work that only reports AUC and d-prime.\nC. Baseline Model\nIn this work, we use a similar model structure as in [4],\nillustrated in Figure 2. Each 10-second audio waveform is \ufb01rst\nconverted to a sequence of 128 dimensional log Mel \ufb01lterbank\n(fbank) features computed with a 25ms Hamming window\nevery 10ms. We conduct zero padding to make all audio clips\nhave 1056 frames. This results in a 1056 \u00d7128 feature vector\nthat is input to a CNN model. In [4] the CNN was based\non the ResNet50 model [20]. In our work, the CNN is based\non the Ef\ufb01cientNet-B2 model [10] since it requires a smaller\nnumber of parameters and is faster for training and inference.\nThe Ef\ufb01cientNet model effectively downsamples the time and\nfrequency dimensions by a factor of 32. The penultimate\noutput of the model is a 33 \u00d74 \u00d71408 tensor. We apply\nmean pooling over the 4 frequency dimensions to produce a\n33\u00d71408 representation that is fed into a multi-head attention\nmodule. The attention module consists of an attention branch\nand a classi\ufb01cation branch.", "mimetype": "text/plain", "start_char_idx": 1105, "end_char_idx": 2738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22": {"__data__": {"id_": "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22", "embedding": null, "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5da07a27b6136b12e8577f8e7030c5377cd9a1651ff2e92634e5dd83a94a002c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6eeea9b-7c28-44e1-8579-329a78e4526d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "e8a2c0b62753d42918d6f6bcd59a59f0d0a31be1f15ee365ba9a61b41c30a0df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7b12349-9fd0-41e8-80a6-44cc23a24014", "node_type": "1", "metadata": {}, "hash": "c46b1a566b0e75559f9a7a13adae53934377143f7b3ca35888d34041ef93d5dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Ef\ufb01cientNet model effectively downsamples the time and\nfrequency dimensions by a factor of 32. The penultimate\noutput of the model is a 33 \u00d74 \u00d71408 tensor. We apply\nmean pooling over the 4 frequency dimensions to produce a\n33\u00d71408 representation that is fed into a multi-head attention\nmodule. The attention module consists of an attention branch\nand a classi\ufb01cation branch. Each branch transforms the CNN\nmean pooled output into a 33 \u00d7527 dimensional tensor via a\nTABLE II\nMEAN AVERAGE PRECISION (MAP) C OMPARISON OF THE RESNET\nMODEL [4] AND THE EFFICIENT NET MODEL USED IN THIS PAPER .\n# Parameters Balanced Set Full Set\nResNet-50 25.66M 0.1635 0.3790\nEf\ufb01cientNet-B2 13.64M 0.1570 0.3723\nset of 1\u00d71 convolutional \ufb01lters. After a sigmoid non-linearity\nand a normalization on the attention branch, we combine the\ntwo branches via a element-wise product. A temporal mean\npooling (implemented by summation) is then performed to\nproduce a \ufb01nal 527 dimensional output for each class label.\nUnlike [4], we use a 4-headed attention module instead of a\nsingle-head one in this work. We sum the weighted output\nof each attention head after it has been scaled by a learnable\nweight to produce the \ufb01nal output.\nEf\ufb01cientNet [10] is a recent proposed convolutional neural\nnetwork architecture that has shown an advantage on both\naccuracy and ef\ufb01ciency over previous architectures. Such\nadvantage mainly comes from two design: First, Ef\ufb01cientNet\nis based on the mobile inverted bottleneck convolution (MB-\nConv) block [21], [22], an ef\ufb01cient residual convolution block.", "mimetype": "text/plain", "start_char_idx": 2360, "end_char_idx": 3920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7b12349-9fd0-41e8-80a6-44cc23a24014": {"__data__": {"id_": "c7b12349-9fd0-41e8-80a6-44cc23a24014", "embedding": null, "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5da07a27b6136b12e8577f8e7030c5377cd9a1651ff2e92634e5dd83a94a002c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c7de0213c6e1b26ff1d679197b87dd360966cf41927713cb3e6f609be49410e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "988c203a-2456-4b32-84f7-9d3bc5f921b1", "node_type": "1", "metadata": {}, "hash": "52fc14aca3b84894c4f607581dfeb7ad3a5ad80d1797ee9a92e6646aeb291242", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ef\ufb01cientNet [10] is a recent proposed convolutional neural\nnetwork architecture that has shown an advantage on both\naccuracy and ef\ufb01ciency over previous architectures. Such\nadvantage mainly comes from two design: First, Ef\ufb01cientNet\nis based on the mobile inverted bottleneck convolution (MB-\nConv) block [21], [22], an ef\ufb01cient residual convolution block.\nSecond, Ef\ufb01cientNet scales the network on all dimensions (i.e.,\nwidth, depth, and input resolution), which is demonstrated to\nbe a better strategy than scaling only one dimension. In this\nwork, we use Ef\ufb01cientNet-B2 that consists of 9 stages, 339\nlayers. The original Ef\ufb01cientNet-B2 model for image classi\ufb01-\ncation has 9.11M parameters, after adding the attention module\nand adjusting the classi\ufb01cation layer, our audio tagging model\nhas 13.64M parameters in total. As shown in Table II, the\nEf\ufb01cientNet model achieves slightly worse performance than\nthe ResNet-50 model, but has 12 million fewer parameters. In\nthe rest of the paper, we keep using the Ef\ufb01cientNet model and\nshow that a signi\ufb01cant improvement can be achieved without\nmodifying its model architecture.\nIII. N ETWORK PRETRAINING\nTransfer learning and network pretraining have been widely\nused in computer vision, natural language processing, speech\nand audio processing in recent years [23], [24], [25]. The\ntypical process is to \ufb01rst train a model with either a large\nout-of-domain or unlabeled dataset using an auxiliary task and\nthen \ufb01ne-tune the model with in-domain data for the main task.\nThe idea being that the knowledge learned from the pretraining\ntask can be transferred to the main task.", "mimetype": "text/plain", "start_char_idx": 3565, "end_char_idx": 5185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "988c203a-2456-4b32-84f7-9d3bc5f921b1": {"__data__": {"id_": "988c203a-2456-4b32-84f7-9d3bc5f921b1", "embedding": null, "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800", "node_type": "4", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5da07a27b6136b12e8577f8e7030c5377cd9a1651ff2e92634e5dd83a94a002c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7b12349-9fd0-41e8-80a6-44cc23a24014", "node_type": "1", "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1d3f7f9962da1aa03f5779696d3622253afc0778c1630349a572b988150857ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The\ntypical process is to \ufb01rst train a model with either a large\nout-of-domain or unlabeled dataset using an auxiliary task and\nthen \ufb01ne-tune the model with in-domain data for the main task.\nThe idea being that the knowledge learned from the pretraining\ntask can be transferred to the main task.\nFor the audio tagging task, both supervised pretraining (e.g.,\nin [5]) and self-supervised pretraining (e.g., in [26], [27], [28],\n[29], [30]) using audio data have been studied in recent years.\nPerformance improvement is typically achieved when the in-\ndomain dataset is small (e.g., ESC-50 [1], UrbanSound [31],\nand balanced AudioSet). However, it has not been reported\nthat a pretrained model can outperform a state-of-the-art audio\ntagging model trained from scratch using the full AudioSet,\npossibly because the full AudioSet contains 2 million audio\nrecordings and there is no larger annotated dataset available.\nWhile theoretically self-supervised pretraining can leverage an", "mimetype": "text/plain", "start_char_idx": 4890, "end_char_idx": 5868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17b5b257-855f-4d5b-99fa-f52187320cf6": {"__data__": {"id_": "17b5b257-855f-4d5b-99fa-f52187320cf6", "embedding": null, "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ea0bebf6fedf418c4beff634e747b974e8806d335617563e2b8d1ebf474fb89b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb89fdcd-438c-4fa5-929d-653fb15dabfa", "node_type": "1", "metadata": {}, "hash": "b9d2718969f43fc961b8d2ef4c645b2ed31298e49cefdb0c752d3ddc07a374bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 4\nTABLE III\nPERFORMANCE IMPACT ON M AP D UE TO\nPRETRAINING WITH IMAGE NET DATA.\nBalanced Set Full Set\nNo pretraining 0.1570 0.3723\nWith pretraining 0.2385 0.3939\nFig. 3. Comparison of the performance of ImageNet-pretrained model and\nrandom-initialized model with different training data volume.\nunlimited amount of unlabelled audio data, in practice it takes\neffort to \ufb01nd and process large scale data with suf\ufb01cient variety\nand coverage of the 527 sound classes.\nIn contrast to the above-mentioned efforts, we \ufb01nd notice-\nable performance improvement can be achieved by pretraining\nthe CNN with the ImageNet dataset [12] used for visual object\nclassi\ufb01cation, even when the training data for the end task\nof audio tagging is the full AudioSet. In our experiment,\nwe initialize the Ef\ufb01cientNet (the second to the penultimate\nlayer) with 1) ImageNet-pretrained weights (released by the\nauthors of [10]), and 2) random weights (He Uniform initial-\nization [32]). We then train both models in exactly the same\nway as described in Section II-B.\nAs shown in Table III, ImageNet pretraining leads to a\n51.9% and 5.8% relative improvement for the balanced set\nand full set experiment, respectively. To see the relationship\nbetween the performance improvement and the end-task train-\ning data volume, we further evaluate the performance when the\naudio tagging training data volume is 100k, 200k, 300k, and\n500k (all comprised of the entire balanced set and samples\nrandomly taken from the full set). As shown in Figure 3,\nthe performance improvement decreases with the training data\nvolume, but is always noticeable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb89fdcd-438c-4fa5-929d-653fb15dabfa": {"__data__": {"id_": "bb89fdcd-438c-4fa5-929d-653fb15dabfa", "embedding": null, "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ea0bebf6fedf418c4beff634e747b974e8806d335617563e2b8d1ebf474fb89b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b5b257-855f-4d5b-99fa-f52187320cf6", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "58a70a9f219b2db2a300296963311b8ebf6ea490740b5ae21e7a594de51abce6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aeea271d-1dae-4742-9e69-2be174351555", "node_type": "1", "metadata": {}, "hash": "8da2e22514573d20cdcf17c40a23fe84b293fd996ef571603cbfcecf0b4f1f3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To see the relationship\nbetween the performance improvement and the end-task train-\ning data volume, we further evaluate the performance when the\naudio tagging training data volume is 100k, 200k, 300k, and\n500k (all comprised of the entire balanced set and samples\nrandomly taken from the full set). As shown in Figure 3,\nthe performance improvement decreases with the training data\nvolume, but is always noticeable. In addition, we \ufb01nd the\nperformance improvement led by ImageNet pretraining is\nmuch larger than that led by more training iterations, e.g.,\nwhen trained with the balanced AudioSet, the model trained\nwith 120 epochs achieves an mAP of 0.1694, which is only\nslightly better than the model trained with 60 epochs and\nis signi\ufb01cantly worse than the model trained with ImageNet\npretraining that achieves an mAP of 0.2385.\nIn some sense, it is surprising that pretraining a model with\ndata from a different modality can be effective. However,\ntransfer learning from computer vision tasks to audio tasks\nis not new and has been previously studied in [33], [34],\n[35], [13]. However, we believe this is the \ufb01rst time it has\nbeen demonstrated to be effective when the dataset of the\naudio task is at this scale, indicating the auxiliary image\nclassi\ufb01cation task helps the model learn some complementary\nknowledge. We hypothesize that the improvements may be due\nto the model learning to recognize low-level features such as\nedges during pretraining. Such knowledge could potentially be\nrelevant for \ufb01nding acoustic \u201cedges\u201d in the spectrogram.\nIn practice, many commonly used CNN architectures (e.g.,\nInception [36], ResNet [20], Ef\ufb01cientNet [10]) have off-the-\nshelf ImageNet-pretrained models for both TensorFlow and\nPyTorch.", "mimetype": "text/plain", "start_char_idx": 1255, "end_char_idx": 2989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aeea271d-1dae-4742-9e69-2be174351555": {"__data__": {"id_": "aeea271d-1dae-4742-9e69-2be174351555", "embedding": null, "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ea0bebf6fedf418c4beff634e747b974e8806d335617563e2b8d1ebf474fb89b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb89fdcd-438c-4fa5-929d-653fb15dabfa", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "05b53e8986a4cebf7db37c5839e38c64272b1d1e63cad45d50ba357fccc7949d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "235d8915-3e9e-4c17-b1b7-94313b57761a", "node_type": "1", "metadata": {}, "hash": "767b25357476b30eb43c3e2ee3eb18b31743ccda7105dec85755fc6105a1cb53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We hypothesize that the improvements may be due\nto the model learning to recognize low-level features such as\nedges during pretraining. Such knowledge could potentially be\nrelevant for \ufb01nding acoustic \u201cedges\u201d in the spectrogram.\nIn practice, many commonly used CNN architectures (e.g.,\nInception [36], ResNet [20], Ef\ufb01cientNet [10]) have off-the-\nshelf ImageNet-pretrained models for both TensorFlow and\nPyTorch. It is also straightforward to adapt these off-the-shelf\nmodels to audio tasks. The only things that need to be modi\ufb01ed\nare the \ufb01rst convolution layer and the last classi\ufb01cation layer.\nSince the input of vision tasks is a 3-channel image while\nthe input to the audio task is a single-channel spectrogram,\nwe adjust the input channel of the \ufb01rst convolutional layer\nfrom 3 to 1 and initialize it with random weights. Since\nthe classi\ufb01cation task is essentially different, we abandon\nthe last classi\ufb01cation layer of the pretrained model and feed\nthe output of the penultimate layer to our succeeding layers.\nWe implement this using the efficientnet_pytorch1\npackage.\nIn summary, the advantages of using ImageNet pretraining\nare as follows. First, no additional in-domain labeled or\nunlabeled datasets are needed. This is important because cur-\nrently there is no audio tagging dataset of comparable size to\nAudioSet. Second, ImageNet pretraining can lead to consistent\nperformance improvement even when the in-domain training\ndata size is huge. Third, ImageNet pretraining is practically\neasy to implement. The limitation is that it is only applicable\nto models that take 2D image-like input (e.g., spectrogram).\nNevertheless, a majority of deep learning models for audio\ntasks do fall in this category.", "mimetype": "text/plain", "start_char_idx": 2577, "end_char_idx": 4290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "235d8915-3e9e-4c17-b1b7-94313b57761a": {"__data__": {"id_": "235d8915-3e9e-4c17-b1b7-94313b57761a", "embedding": null, "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7", "node_type": "4", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ea0bebf6fedf418c4beff634e747b974e8806d335617563e2b8d1ebf474fb89b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aeea271d-1dae-4742-9e69-2be174351555", "node_type": "1", "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "d416992d5cc5bac68d7e4d4b6ddd7184fb79284db2179a106ffbc1a4b8260565", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, ImageNet pretraining can lead to consistent\nperformance improvement even when the in-domain training\ndata size is huge. Third, ImageNet pretraining is practically\neasy to implement. The limitation is that it is only applicable\nto models that take 2D image-like input (e.g., spectrogram).\nNevertheless, a majority of deep learning models for audio\ntasks do fall in this category. In the following sections, we\nuse Imagenet pretraining by default for all experiments.\nIV. B ALANCED SAMPLING AND DATA AUGMENTATION\nA. Balanced Sampling\nAs might be expected, the frequency of occurrence of\ndifferent sound events ranges widely. It is not surprising then\nthat a large scale audio tagging dataset is class imbalanced.\nAs shown in Figure 4, the most frequent AudioSet class is\n\u201cMusic\u201d which has 949,029 samples, while the most infre-\nquent class \u201cToothbrush\u201d only has 61 samples, leading to a\nratio of 15,557. Such imbalances can have a large impact on\nperformance, particularly for low-frequency classes [37].\nWith such large data imbalance, simple upsampling or\ndownsampling are dif\ufb01cult to implement because upsampling\nwill make the dataset unacceptably large while downsampling\nwill waste a large portion of the data. Moreover, AudioSet\nis a multi-label dataset, making it even harder to implement\nup/downsampling methods. In this work, we propose a random\nbalanced sampling method to alleviate the class imbalance\nproblem. Note that balanced sampling on AudioSet has been\n1https://github.com/lukemelas/Ef\ufb01cientNet-PyTorch", "mimetype": "text/plain", "start_char_idx": 3904, "end_char_idx": 5430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02c1c66e-2e62-4d83-b26f-3a6ad431f674": {"__data__": {"id_": "02c1c66e-2e62-4d83-b26f-3a6ad431f674", "embedding": null, "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c0c2331fd11c0ad9057b12ee622af3c0e4ae846c00c5137b91afdeed8a9cd30d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32b1f121-e3e5-447a-bbea-88dc8eb1b014", "node_type": "1", "metadata": {}, "hash": "d70fa47ce946c64f6f65355a788c17623ed47d38e2ac30dcae1b45956b807340", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 5\nTABLE IV\nPERFORMANCE IMPACT ON M AP D UE TO VARIOUS BALANCED\nSAMPLING AND DATA AUGMENTATION STRATEGIES .\nBalanced Set Full Set\nBaseline 0.2385 0.3939\n+ Balanced Sampling - 0.3721\n+ Time-Frequency Masking 0.2818 0.4265\n+ Mix-up Training 0.3108 0.4397\nFig. 4. Sample count of each class in the full AudioSet (vertical axis is in\nlog scale). Note that the sample count of the \u201cSpeech\u201d class is substantially\nlarger than the sum of sample counts of the \u201cMale Speech\u201d, \u201cFemale Speech\u201d,\nand \u201cChild Speech\u201d class. Similarly, the sample count of the \u201cMusic\u201d class is\nsubstantially larger than the sum of sample counts of the \u201cHappy music\u201d and\n\u201cSad music\u201d class. This indicates a potential prevalent miss annotation issue\nin AudioSet.\nused in [6], [8], [5], but is only brie\ufb02y mentioned and the\ndetails can only be found in the source code.\nThe proposed random balanced sampling approach is shown\nin Algorithm 1, lines 1-8. We \ufb01rst count the sample number\nck of each class k over the entire dataset. We then assign a\nsampling weight for each sample, speci\ufb01cally, the weight w(i)\nof the ith sample is \u2211527\nk=1 1 {k\u2208y(i)}1/ck. This assigns a higher\nweight for samples containing rare audio events and also takes\nall audio events that appear in the sample into consideration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32b1f121-e3e5-447a-bbea-88dc8eb1b014": {"__data__": {"id_": "32b1f121-e3e5-447a-bbea-88dc8eb1b014", "embedding": null, "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c0c2331fd11c0ad9057b12ee622af3c0e4ae846c00c5137b91afdeed8a9cd30d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02c1c66e-2e62-4d83-b26f-3a6ad431f674", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1da7fb9dcc2a0fa593329e4a3425e5696c1f959d7f95c41e60f2fdda74921052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cfed885-3663-4885-af61-7ea22736305b", "node_type": "1", "metadata": {}, "hash": "2dc6ef70fc2bac2bbac778902f172fc352fae0d92bd61249744745ce078de28b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \ufb01rst count the sample number\nck of each class k over the entire dataset. We then assign a\nsampling weight for each sample, speci\ufb01cally, the weight w(i)\nof the ith sample is \u2211527\nk=1 1 {k\u2208y(i)}1/ck. This assigns a higher\nweight for samples containing rare audio events and also takes\nall audio events that appear in the sample into consideration.\nDuring training, we still feed N samples ( N is the dataset\nsize) to the model for each epoch, but instead of traversing the\ndataset, we draw a sample from the multinomial distribution\nparameterized by the above-mentioned sample weights with\nreplacement. That makes rare sound event samples more likely\nto be seen by the model. The advantages of the proposed\nrandom sampling are 1) it is a compromise of upsampling and\ndownsampling. It wastes fewer samples than downsampling\nwhile keeping the number of N samples fed to the model\nevery epoch; 2) it is applicable to multi-label datasets; and\n3) the model sees a different set of data every epoch, so the\nmodel checkpoints after every epoch have a greater diversity,\nwhich is helpful for ensembles [38], [39], as we will discuss\nin Section VI.\nAs shown in Figure 5, while the proposed balanced sam-\npling algorithm greatly alleviates the data imbalance issue,\nFig. 5. Sorted sampled frequency of each class after 30 training epochs.", "mimetype": "text/plain", "start_char_idx": 981, "end_char_idx": 2311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cfed885-3663-4885-af61-7ea22736305b": {"__data__": {"id_": "2cfed885-3663-4885-af61-7ea22736305b", "embedding": null, "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c0c2331fd11c0ad9057b12ee622af3c0e4ae846c00c5137b91afdeed8a9cd30d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b1f121-e3e5-447a-bbea-88dc8eb1b014", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ed09b0f4ccf66b3e08ed689f832522dfa6cc83f6540059106fb107813a11af17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8043580b-588a-4bd9-9544-a7250011982b", "node_type": "1", "metadata": {}, "hash": "bb88fdde58db50a0fce12609fde52d7da4ada868c821b6010ef131df4d87b86d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figure 5, while the proposed balanced sam-\npling algorithm greatly alleviates the data imbalance issue,\nFig. 5. Sorted sampled frequency of each class after 30 training epochs.\nAlgorithm 1 Balanced Sampling and Data Augmentation\nRequire:\nMulti-label Dataset D= {x(i),y(i)}, i\u2208{1,...,N }\nProcedure 1: Generate Sampling Weight\nInput: Label Set {y(i)}\nOutput: Sample Weight Set W= {w(i)}, i\u2208{1,...,N }\n1: traverse {y(i)}, count sample number ck of each class k\n2: initialize w(i) = 0, i\u2208{1,...,N }\n3: for each sample i do\n4: for each class k\u2208y(i) do\n5: w(i) = w(i) + 1/ck\nreturn W= {w(i)}\nProcedure 2: Sampling and Augmentation in Training\nInput: {x(i),y(i)}, W, F, T, M\n6: for every epoch do\n7: for n\u2208{1,...,N } do\n8: draw i\u223cmultinomial(W)\n9: if unif(0,1) < mixup rate M then\n10: draw j \u223cunif{1,N}\n11: draw \u03bb\u223cBeta(\u03b1,\u03b1)\n12: x= \u03bbx(i) + (1\u2212\u03bb)x(j)\n13: y= \u03bby(i) + (1\u2212\u03bb)y(j)\n14: else\n15: x= x(i), y= y(i)\n16: draw f \u223cunif(0,F), f0 \u223cunif(0,128 \u2212f)\n17: draw t\u223cunif(0,T), t0 \u223cunif(0,1056 \u2212t)\n18: x= Masking(f0,t0,f,t )(x)\n19: use (x,y) to train the neural network\nthe sampled frequency of each class is still imbalanced after\nthe balanced sampling algorithm is applied.", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 3293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8043580b-588a-4bd9-9544-a7250011982b": {"__data__": {"id_": "8043580b-588a-4bd9-9544-a7250011982b", "embedding": null, "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce", "node_type": "4", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c0c2331fd11c0ad9057b12ee622af3c0e4ae846c00c5137b91afdeed8a9cd30d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cfed885-3663-4885-af61-7ea22736305b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "db4bf8da8a4189576dfd6a42a22ee98dda05ad8fa3a9bed7fc27ca376272df2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is because\nAudioSet is a multi-label dataset and minority classes are\nusually paired with majority classes, thus oversampling the\nminority class also directly oversamples the majority class.\nWe compare the performance of the model trained with plain\ndataset traversal (with data reshuf\ufb02ed at every epoch) and with\nthe proposed random sampling. As shown in Table IV, we \ufb01nd\nrandom balanced sampling actually lowers the performance.\nThis result is not surprising because: 1) while better than", "mimetype": "text/plain", "start_char_idx": 3294, "end_char_idx": 3789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9cfc59a-74d1-424d-8322-082dde4b290e": {"__data__": {"id_": "b9cfc59a-74d1-424d-8322-082dde4b290e", "embedding": null, "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "af4c3f3074d2505b3bec53b6fbbcc4f56678e4aa74a81e344514b0af0895d593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e3853e2-4806-4eb7-8120-cf5e0d4c903a", "node_type": "1", "metadata": {}, "hash": "54cd3c75f1cac28e5a3cff0aefdcb5a9acacc3c9c45e89e6beb81f1a4c836b00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 6\nFig. 6. The proportion of unseen samples with the training epochs. Mixup rate\nis the probability that the sample input to the model is a mixed-up sample.\nIn our implementation, one of the two mixed-up samples is drawn from a\nuniform distribution, while the other is drawn using the balanced sampling\nmultinomial distribution.\ndownsampling, there is still a substantial amount of data\nwasted every epoch. As shown in Figure 6, 40.9% data is not\nseen by the model after 30 training epochs; 2) while the low-\nfrequency class samples and high-frequency class samples are\nroughly equally seen by the model, the low-frequency class\nsamples are actually repeated samples. Both issues increase\nthe risk of model over\ufb01tting. Therefore, we explored the use\nof data augmentation to overcome this problem.\nB. Time and Frequency Masking\nWe \ufb01rst consider simple time and frequency masking for\ndata augmentation, which has been found to be effective for\naudio tagging [5] and speech recognition [40]. Frequency\nmasking is applied so that f consecutive frequency chan-\nnels [ f0, f0 + f) are masked, where f \u223c unif(0,F),\nf0 \u223c unif(0,128 \u2212f), and F is the maximum possi-\nble length of the frequency mask. Similarly, time mask-\ning is applied so that t consecutive frequency channels\n[t0, t0 + t) are masked, where t \u223c unif(0,T), t0 \u223c\nunif(0,1056 \u2212t), and T is the maximum possible length of\nthe frequency mask. Note that 128 and 1056 are the input\ndimensions of our model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e3853e2-4806-4eb7-8120-cf5e0d4c903a": {"__data__": {"id_": "4e3853e2-4806-4eb7-8120-cf5e0d4c903a", "embedding": null, "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "af4c3f3074d2505b3bec53b6fbbcc4f56678e4aa74a81e344514b0af0895d593", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9cfc59a-74d1-424d-8322-082dde4b290e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "9a467501488e09ff884c1f2a3a70994cc24f0fe73000dbc5a98baa5cc6189e4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225", "node_type": "1", "metadata": {}, "hash": "43ebe091a28ca949dfb91822d1cf1404933d557e3111046e19993381c7a400dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, time mask-\ning is applied so that t consecutive frequency channels\n[t0, t0 + t) are masked, where t \u223c unif(0,T), t0 \u223c\nunif(0,1056 \u2212t), and T is the maximum possible length of\nthe frequency mask. Note that 128 and 1056 are the input\ndimensions of our model. We use the implementation of\ntorchaudio.transforms.FrequencyMasking and\nTimeMasking, F = 48and T = 192. The masking param-\neters f0,t0,f,t are sampled on-the-\ufb02y for each audio sample\nduring training to minimize the chance of repeated audio\nsamples being fed to the model. As shown in Table IV, time\nand frequency masking improves audio tagging performance\nconsiderably, with relative improvements of 18.2% and 14.6%\nachieved for the balanced set and full set experiment, respec-\ntively. Note that the overall amount of training samples per\nepoch remains the same. We hypothesize that the effectiveness\nof masking is due to the reduction of repeated samples in the\ntraining data, especially for low-frequency samples.\nTABLE V\nPERFORMANCE AS A FUNCTION OF MIX-UP RATE\n(TRAINING ON BALANCED SET WITH \u03b1= 10).\nMixup Rate 0 0.2 0.5 0.8 1.0\nmAP 0.2818 0.3060 0.3108 0.3119 0.2928\nC. Mix-up Training\nAn additional form of data augmentation we explored is\ncalled mix-up training where weighted combinations of audio\nsamples are combined to make new samples. Mix-up training\ncreates convex combinations of pairs of examples and their\ncorresponding labels.", "mimetype": "text/plain", "start_char_idx": 1254, "end_char_idx": 2667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225": {"__data__": {"id_": "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225", "embedding": null, "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "af4c3f3074d2505b3bec53b6fbbcc4f56678e4aa74a81e344514b0af0895d593", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e3853e2-4806-4eb7-8120-cf5e0d4c903a", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1df6ee2df301d0c7769442fc5526e890679810c05e05f98a69e331a2b8534d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6bd5b5f-60a4-46a8-9f73-b26fd8880699", "node_type": "1", "metadata": {}, "hash": "92ace164fab564e0c3a7ec153b4786bdf472ea9ad1a21a8f48077b2b85015635", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mix-up training\ncreates convex combinations of pairs of examples and their\ncorresponding labels. Studies have shown it can improve the\nperformance of image classi\ufb01cation, voice command recogni-\ntion [41], [42], and audio tagging [5], [43]. Speci\ufb01cally, mix-\nup training constructs augmented training examples as follows:\nx= \u03bbx(i) + (1\u2212\u03bb)x(j)\ny= \u03bby(i) + (1\u2212\u03bb)y(j)\nTABLE VI\nPERFORMANCE AS A FUNCTION OF \u03b1\n(TRAINING ON BALANCED SET WITH MIX-UP RATE = 0.5).\n\u03b1 \u2212\u221e 0.1 1 10\nmAP 0.2818 0.3004 0.3087 0.3108\nwhere x(i) and x(j) are two different training audio samples,\ny(i) and y(j) are the corresponding labels, \u03bb\u2208[0,1] and x is\nthe mixed-up new audio sample, and y is the resulting label.\nWe conduct mix-up on the waveform level.\nPast explanations for why mix-up training improves per-\nformance include: 1) it increases the variation of the training\ndata [5], [43]; 2) it leads to an enlargement of Fisher\u2019s criterion\nin the feature space and a regularization of the positional\nrelationship among the feature distributions of the classes [41],\n[43]; and 3) it reduces the model\u2019s memorization of corrupt\nlabels [42].\nIn addition to these observations, we \ufb01nd mix-up training\nhas an additional advantage for imbalanced datasets. As we\ndiscussed in Section IV-A, balanced sampling, while making\nthe low-frequency class samples more prevalent, has the unfor-\ntunate side effect of wasting a large number of (40.9%) class\nsamples. By adopting the mixup strategy, the model can see\ntwice the number of samples within the same training epoch.", "mimetype": "text/plain", "start_char_idx": 2571, "end_char_idx": 4102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6bd5b5f-60a4-46a8-9f73-b26fd8880699": {"__data__": {"id_": "f6bd5b5f-60a4-46a8-9f73-b26fd8880699", "embedding": null, "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff", "node_type": "4", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "af4c3f3074d2505b3bec53b6fbbcc4f56678e4aa74a81e344514b0af0895d593", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225", "node_type": "1", "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f739305c56310b9038a077b4def101b0ba133df8e83c07d447a0e37315a785b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition to these observations, we \ufb01nd mix-up training\nhas an additional advantage for imbalanced datasets. As we\ndiscussed in Section IV-A, balanced sampling, while making\nthe low-frequency class samples more prevalent, has the unfor-\ntunate side effect of wasting a large number of (40.9%) class\nsamples. By adopting the mixup strategy, the model can see\ntwice the number of samples within the same training epoch.\nThis advantage can be increased if one of the two mixed-up\nsamples is drawn from a uniform distribution, while the other\nis drawn using the balanced sampling multinomial distribution\nintroduced in the previous section. Intuitively, mixing up a\nrare sound event (e.g., toothbrush) with a frequent one (e.g.,\nmusic) is more reasonable than mixing up two rare sound\nevents. Some previous synthetic audio event detection datasets\nuse a similar method to construct samples [44]. As shown in\nFigure 6, the mix-up strategy can reduce the unseen samples\nto almost zero in just a few epochs.", "mimetype": "text/plain", "start_char_idx": 3683, "end_char_idx": 4685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa9e3d52-33cf-411b-a030-5b763d54f6b3": {"__data__": {"id_": "aa9e3d52-33cf-411b-a030-5b763d54f6b3", "embedding": null, "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f3ce524fcb269250b7188146b158eb470cd302a19e9edbfb59ce458b7ba0814d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed8f5ce9-293d-476b-b54e-4e72eac78dff", "node_type": "1", "metadata": {}, "hash": "5a5b97d113d17ea3450a1d84db1fa928f6c49cac3c957e57fc049cefb99f16f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 7\nWe further make two modi\ufb01cations based on previous\nefforts. In prior work \u03bb is drawn from a uniform distribu-\ntion unif(0,1) [43] or Beta distribution Beta(\u03b1,\u03b1) with\n\u03b1< 1 [42], where\nBeta(\u03b1,\u03b1) : prob(x; \u03b1,\u03b1) =x\u03b1\u22121(1 \u2212x)\u03b1\u22121\nB(\u03b1,\u03b1)\nwhere B is the beta function\nB(\u03b1,\u03b1) =\n\u222b 1\n0\nt\u03b1\u22121(1 \u2212t)\u03b1\u22121dt\nThus \u03bbhas a relatively high likelihood to be close to either\n0 or 1. From the perspective of sound mixing and reducing\nthe number of unseen samples, a \u03bb close to 0.5 could be\nmore reasonable since it leads to more \u201cevenly\u201d mixed up\nsamples and the model can see both samples. Second, since\nsamples in the evaluation set are not mixed up, mixing up all\nsamples during training might lead to a gap between training\nand evaluation. Thus we set a mix-up rate to control the\nnumber of samples to mix up during training, a mixup rate\nof 0.5 means that 50% training samples are mixup samples\nand the rest 50% training samples are non-synthetic samples.\nTherefore, the model can see non-synthetic samples during\ntraining. As shown in Figure 6, a mix-up rate of 0.5 results in\n95% samples being seen by the model in 5 epochs. For non\nmix-up samples, the data loader only needs to load one audio\nsample instead of two.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed8f5ce9-293d-476b-b54e-4e72eac78dff": {"__data__": {"id_": "ed8f5ce9-293d-476b-b54e-4e72eac78dff", "embedding": null, "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f3ce524fcb269250b7188146b158eb470cd302a19e9edbfb59ce458b7ba0814d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa9e3d52-33cf-411b-a030-5b763d54f6b3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "85b1068b39aa7064924074bbbd08e49c2d11ec8bcd582a8465057b3d8f52f24c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45a48615-a087-4685-a35e-1ea3621b3a73", "node_type": "1", "metadata": {}, "hash": "07b81a191d79e7ddd9eb5bd920678b01d2181be83026230e31749e50fb6c6614", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the model can see non-synthetic samples during\ntraining. As shown in Figure 6, a mix-up rate of 0.5 results in\n95% samples being seen by the model in 5 epochs. For non\nmix-up samples, the data loader only needs to load one audio\nsample instead of two. A low mix-up rate can also reduce the\ndata loading and pre-processing cost during training, which is\nnon-negligible because it is almost impossible to \ufb01t the full\nAudioSet into memory.\nWe evaluate the impact of mix-up rate and \u03b1, as shown\nin Tables V and VI. A larger \u03b1 and a medium mix-up rate\nindeed lead to better classi\ufb01cation performance. Combining\nthem achieves 0.3108 mAP, which is better than a plain setting\nof \u03b1=mixup rate=1 that achieves 0.3079 mAP. We use \u03b1= 10\nand mix-up rate = 0.5 in all subsequent experiments.\nD. Summary\nWe combine the balanced sampling and masking and mix-\nup data augmentation strategies together, as described in Algo-\nrithm 1. We summarize the contribution of each component in\nTable IV. It is worth mentioning that while balanced sampling\nalone lowers the performance, it is helpful when combined\nwith data augmentation strategies. By adopting balanced sam-\npling and data augmentation, an 11.6% relative improvement\nand an mAP of 0.4397 are achieved for the full set experiment.\nWe only do data augmentation for balanced set experiments\nas the data is already roughly balanced and obtain a 30.3%\nrelative improvement and an mAP of 0.3108, demonstrating\nthe effectiveness of data augmentation for small datasets.", "mimetype": "text/plain", "start_char_idx": 1002, "end_char_idx": 2516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45a48615-a087-4685-a35e-1ea3621b3a73": {"__data__": {"id_": "45a48615-a087-4685-a35e-1ea3621b3a73", "embedding": null, "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f3ce524fcb269250b7188146b158eb470cd302a19e9edbfb59ce458b7ba0814d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed8f5ce9-293d-476b-b54e-4e72eac78dff", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "e7d7374e2d3326a3a42e8076f7775b19772b520699e0a42a24600a9726413538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0b8f408-5af4-42c5-9311-a7dd34f519c6", "node_type": "1", "metadata": {}, "hash": "436f507eb2b6ec7605ed53a60076762ad3b72e51e9956c1c338b1aa3ccd65600", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By adopting balanced sam-\npling and data augmentation, an 11.6% relative improvement\nand an mAP of 0.4397 are achieved for the full set experiment.\nWe only do data augmentation for balanced set experiments\nas the data is already roughly balanced and obtain a 30.3%\nrelative improvement and an mAP of 0.3108, demonstrating\nthe effectiveness of data augmentation for small datasets. Fi-\nnally, it is worth mentioning that by merely adopting ImageNet\npretraining, balanced sampling, and data augmentation with\na standard Ef\ufb01cientNet architecture, the model already out-\nperforms the previous best system. In the following sections,\nwe use balanced sampling (for the full AudioSet) and data\naugmentation as defaults for all experiments.\nFig. 7. Sorted class-wise average precision (AP) and its standard deviation\nof the model trained on full set. Note that the \u201cSpeech\u201d class has a much\nhigher AP than the \u201cMale Speech\u201d, \u201cFemale Speech\u201d, and \u201cChild Speech\u201d\nclass. Similarly, the \u201cMusic\u201d class has a much higher AP than the \u201cHappy\nMusic\u201d and \u201cSad Music\u201d class. \u201cSinging\u201d and \u201cSong\u201d have similar de\ufb01nition\nbut very different AP. Classes with low AP also have a larger AP variance.\nV. L ABEL ENHANCEMENT\nIn this section, we explore the noisy label aspect of Au-\ndioSet: how it impacts audio tagging performance, and how\nto alleviate it. This line of research is motivated by observing\nthe model\u2019s class-wise performance. In Figure 7, we show\nthe class-wise average precision (AP) of the model trained\nwith the full set. From the \ufb01gure it is immediately apparent\nthat the AP of each class differs greatly, indicating that the\nmodel has a range of ability to recognize various sounds.", "mimetype": "text/plain", "start_char_idx": 2136, "end_char_idx": 3811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0b8f408-5af4-42c5-9311-a7dd34f519c6": {"__data__": {"id_": "d0b8f408-5af4-42c5-9311-a7dd34f519c6", "embedding": null, "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f3ce524fcb269250b7188146b158eb470cd302a19e9edbfb59ce458b7ba0814d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45a48615-a087-4685-a35e-1ea3621b3a73", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "00346bb59e5f2962dc7ed5dfebbb5da958f986159cf645774fd8108b18d8ef30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "829d0d83-dab0-43c9-8887-8059825939e3", "node_type": "1", "metadata": {}, "hash": "2a04ff0f94c7c591e2fd351cd103c62ca643b928671bf636f32cb9180fad7fee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This line of research is motivated by observing\nthe model\u2019s class-wise performance. In Figure 7, we show\nthe class-wise average precision (AP) of the model trained\nwith the full set. From the \ufb01gure it is immediately apparent\nthat the AP of each class differs greatly, indicating that the\nmodel has a range of ability to recognize various sounds. This\nis not an issue speci\ufb01c to our model or training pipeline,\nbut has been widely reported in prior work [5], [8], [14],\n[45], [46]. The order of class-speci\ufb01c performance reported by\nindependent research also appears to be similar. For example,\nthe \u201cMale speech\u201d, \u201cBicycle\u201d, \u201cHarmonic\u201d, \u201cRattle\u201d, and\n\u201cScrape\u201d classes are among the 10 worst performing classes\nin [45], and they are also are among the 10 worst performing\nclasses for our model when trained with the balanced set. We\nfurther con\ufb01rm that models with different architectures have\nsimilar class-speci\ufb01c performance order with experiments in\nSection VII-B. This consistency suggests that the issue might\nbe due to an intrinsic problem with the data or the task.\nSince the class-wise AP is not strongly correlated with either\nclass sample count in the training set or the class annotation\nquality estimate released by the AudioSet authors (as shown\nin Table VII), it has been hypothesized that the class-wise\nperformance variation is due in part to the dif\ufb01culty in reliably\ntagging the different sound classes themselves [5], [46].\nWhile we agree that the poor performance of some classes\ncould be due to particular audio events being dif\ufb01cult to\nidentify, it is not true for all poor-performing classes.", "mimetype": "text/plain", "start_char_idx": 3466, "end_char_idx": 5080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "829d0d83-dab0-43c9-8887-8059825939e3": {"__data__": {"id_": "829d0d83-dab0-43c9-8887-8059825939e3", "embedding": null, "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799", "node_type": "4", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f3ce524fcb269250b7188146b158eb470cd302a19e9edbfb59ce458b7ba0814d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0b8f408-5af4-42c5-9311-a7dd34f519c6", "node_type": "1", "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1e559975ab41d48dcb3bce71d4e5bfe3e99f4b145e9bfa9b232644db28f54a25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While we agree that the poor performance of some classes\ncould be due to particular audio events being dif\ufb01cult to\nidentify, it is not true for all poor-performing classes. For\nexample, the \u201cMale Speech\u201d, \u201cFemale Speech\u201d, and \u201cChild\nSpeech\u201d classes have APs of 0.07, 0.09, 0.45, respectively\nwhile the AP of the \u201cSpeech\u201d class is 0.80. This discrepancy", "mimetype": "text/plain", "start_char_idx": 4908, "end_char_idx": 5260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2f93468-b715-4847-a9af-24e305b43e4e": {"__data__": {"id_": "d2f93468-b715-4847-a9af-24e305b43e4e", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6524f901-c391-4bd3-82a2-9afe1de48fb5", "node_type": "1", "metadata": {}, "hash": "51aedac485ef94f19329dbc976a1554878fea24d2f768df3263baa6484973abe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 8\nTABLE VII\nCORRELATION COEFFICIENTS BETWEEN CLASS -WISE AP AND CLASS\nSAMPLE COUNT /ANNOTATION QUALITY ESTIMATE RELEASED BY\nAUDIO SET AUTHORS .\nBalanced Set Full Set\nAP and Sample Count 0.1692 0.0946\nAP and Annotation Quality Estimate 0.2464 0.2629\ncannot be explained by the class dif\ufb01culty hypothesis because\nrecognizing speaker gender from speech is a relatively easy\ntask [47], [48], [49], and the performances of the speech\nclasses should not be so disparate. By examining the class\nsample counts, we \ufb01nd another issue that the sample count\nof the \u201cSpeech\u201d class is substantially larger than the sum of\nsample counts of the \u201cMale Speech\u201d, \u201cFemale Speech\u201d, and\n\u201cChild Speech\u201d classes. Speci\ufb01cally, in the balanced set, there\nare 5,309 audio clips with the label \u201cSpeech\u201d but only 55,\n55, 128 audio clips are with label \u201cMale Speech\u201d, \u201cFemale\nSpeech\u201d, and \u201cChild Speech\u201d, respectively. The same thing\nhappens in the full set (shown in Figure 4): the \u201cSpeech\nClass\u201d has 947,009 samples while the sum of the other three\nclasses is 34,878. In other words, only 4.5% and 3.7% of\nspeech samples are labeled as either male, female, or child\nspeech in the balanced and full AudioSet, respectively. This\nindicates that a large portion of samples are not correctly\nlabeled.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6524f901-c391-4bd3-82a2-9afe1de48fb5": {"__data__": {"id_": "6524f901-c391-4bd3-82a2-9afe1de48fb5", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2f93468-b715-4847-a9af-24e305b43e4e", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2f8db80ede9e8d4654ccf82ebc571a3eb7f51ddcb979c7d14425e76833975321", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0af99402-5274-4862-aaa7-7de365929725", "node_type": "1", "metadata": {}, "hash": "a62b35da65dd343882e5acb8a31515c1d7324e4363cacf70354f3abb3877e456", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In other words, only 4.5% and 3.7% of\nspeech samples are labeled as either male, female, or child\nspeech in the balanced and full AudioSet, respectively. This\nindicates that a large portion of samples are not correctly\nlabeled. Based on these two observations, we hypothesize that\nthe low performance of the male, female, and child speech\nclasses is not due a small number of samples, or inherent\nclassi\ufb01cation dif\ufb01culty, but that they have only a small fraction\nof correctly labeled data, which ultimately confuses the model.\nWe refer to this phenomenon as a Type I error.\nWe also \ufb01nd that there are substantial samples labeled\nwith sub-classes, but not with the corresponding parent class\nde\ufb01ned by the AudioSet ontology. For example, there are\n40 and 3,201 audio clips labeled as either \u201cMale Speech\u201d,\n\u201cFemale Speech\u201d, or \u201cChild Speech\u201d, but not labeled as\n\u201cSpeech\u201d in the balanced and full AudioSet, respectively. We\nrefer to this phenomenon as Type II error.\nWe formalize the two types of error as follows:\n1) Type I error: an audio clip is labeled with a parent class,\nbut not also labeled as a child class when it does in fact\ncontain the audio event of the child class.\n2) Type II error: an audio clip is labeled with a child class,\nbut not labeled with corresponding parent classes.\nIt is worth mentioning that neither type of error are included\nin the quality estimate released by the AudioSet authors\nbecause the quality estimate checked 10 random audio clips\nof each class and veri\ufb01ed that they actually contained the\ncorresponding sound event. In other words, the quality esti-\nmate counts the false positive annotation errors, but not false\nnegatives.", "mimetype": "text/plain", "start_char_idx": 1104, "end_char_idx": 2769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0af99402-5274-4862-aaa7-7de365929725": {"__data__": {"id_": "0af99402-5274-4862-aaa7-7de365929725", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6524f901-c391-4bd3-82a2-9afe1de48fb5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c524de1f4fb104ef5ac2903a0c45e7bdc9ea498090999221047103c40011554d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "193b9ae3-3431-43d5-880d-09d2ef130c82", "node_type": "1", "metadata": {}, "hash": "575fac810ae7a5fc9b47cf0be0f289bc6cc1e470f4194ad34b0c5f976bccd1e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is worth mentioning that neither type of error are included\nin the quality estimate released by the AudioSet authors\nbecause the quality estimate checked 10 random audio clips\nof each class and veri\ufb01ed that they actually contained the\ncorresponding sound event. In other words, the quality esti-\nmate counts the false positive annotation errors, but not false\nnegatives. As a consequence, the quality estimate of the \u201cMale\nSpeech\u201d, \u201cFemale Speech\u201d, and \u201cChild Speech\u201d is 90%, 100%,\nand 100%, respectively, while they have obvious false negative\nannotation errors.\nUnfortunately, false negatives are prevalent in AudioSet.\nAnother example are the music classes (see Figure 4 and 7 for\nsample counts and class-wise AP of music classes).", "mimetype": "text/plain", "start_char_idx": 2396, "end_char_idx": 3133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "193b9ae3-3431-43d5-880d-09d2ef130c82": {"__data__": {"id_": "193b9ae3-3431-43d5-880d-09d2ef130c82", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0af99402-5274-4862-aaa7-7de365929725", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "e38c5b5168f9e8b1fb22b46c60c9db1545e6c1fbf2c7568903a35c1641749b2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5", "node_type": "1", "metadata": {}, "hash": "246454f904fe50466d082e33ddb55293fb29ec8be4776471098c8484d94f81ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a consequence, the quality estimate of the \u201cMale\nSpeech\u201d, \u201cFemale Speech\u201d, and \u201cChild Speech\u201d is 90%, 100%,\nand 100%, respectively, while they have obvious false negative\nannotation errors.\nUnfortunately, false negatives are prevalent in AudioSet.\nAnother example are the music classes (see Figure 4 and 7 for\nsample counts and class-wise AP of music classes). The reason\nAlgorithm 2 Label Enhancement\nRequire:\nTeacher Model M\nDataset D= {x(i),y(i)}, i\u2208{1,...,N }\nLabel Ontology O\nProcedure 1: Generate Label Modi\ufb01cation Threshold\nInput: M,D\nOutput: Threshold Set T = {tk},k \u2208{1,..., 527}\n1: for k\u2208{1,..., 527}do\n2: tk = \u2211N\ni=1 1 {k\u2208y(i)}M(x(i))(k)/\u2211N\ni=1 1 {k\u2208y(i)}\nreturn T = {tk}\nProcedure 2: Enhance the Label Set\nInput: M,D,O,T\nOutput: Enhanced Label Set {y\u2032(i)}, i\u2208{1,...,N }\n3: Initialize {y\u2032(i)}= {y(i)}\n4: for i\u2208{1,...,N } do\n5: for k\u2208y(i) do\n6: for kn \u2208O(k) do \u22bf parent or child class of k\n7: if M(x(i))(kn) >tkn and kn \u0338\u2208y(i) then\n8: y\u2032(i) = y\u2032(i) \u222a{kn}\nreturn {y\u2032(i)}\nfor these types of errors is due to the AudioSet annotation\npipeline. In the pipeline, the human annotator veri\ufb01es the\ncandidate labels nominated by a series of automatic methods\n(e.g., by using metadata).", "mimetype": "text/plain", "start_char_idx": 2770, "end_char_idx": 3958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5": {"__data__": {"id_": "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "193b9ae3-3431-43d5-880d-09d2ef130c82", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "35ce06ce679687f1688f170bd7e6abeef85cdbd5cb2d5bf156bd2e7abcabe8d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25d71e38-5247-4d10-bf4b-5afa4f0a1656", "node_type": "1", "metadata": {}, "hash": "1384befbfb715c05594068d91d5a6ccf173669c9f4ef39c7d050ff028762d70a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the pipeline, the human annotator veri\ufb01es the\ncandidate labels nominated by a series of automatic methods\n(e.g., by using metadata). Also, the list of candidate labels is\nlimited to ten labels per clip. Since the automatic methods for\nnomination are not perfect, some existing sound events fail to\nbe nominated, or are nominated but ranked below the top ten,\nthus leading to missing labels [14], [3].\nAs seen in the speech class example, annotation error can\nimpact performance, but has not received much attention. To\nthe best of our knowledge, only a few efforts have covered\nthe missing label issue. In [45], [50], a synthetic error is\nstudied, however, the real-world noisy labels are believed\nto be much harder to deal with than the synthetic labels.\nIn [14], the authors propose a loss masking based teacher-\nstudent model. In this section, we propose an ontology-based\nlabel enhancement method to alleviate the noisy label problem.\nOur approach differs from previous work in three aspects:\nFirst, we work on real-world noisy labels rather than synthetic\ncorrupted labels; Second, we explicitly modify the labels of the\ntraining data rather than using loss masking during training.\nThus the enhanced label set can be used in the exact same\nway as the original set (no need to modify the model and\ntraining pipeline). We plan to release the enhanced label set\nto facilitate future research. Third, we leverage the AudioSet\nontology to constrain label modi\ufb01cation, which reduces the\nchance of incorrect modi\ufb01cations. For example, for an audio\nclip labeled as \u201cSpeech\u201d, we only consider adding child or\nparent labels in the speci\ufb01c \u201dSpeech\u201d branch of the ontology.\nAs shown in Algorithm 2, the proposed approach consists of\nthe following steps. First, we train a teacher model using the\nfull AudioSet with the original label set.", "mimetype": "text/plain", "start_char_idx": 3823, "end_char_idx": 5658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25d71e38-5247-4d10-bf4b-5afa4f0a1656": {"__data__": {"id_": "25d71e38-5247-4d10-bf4b-5afa4f0a1656", "embedding": null, "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "57d824bfeaea741dae404f3c8cfb3604a2feed6e01f3e0662ceb63cc94433322", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "6a8cf7de5f984cb4f2bbf8c039c4c8655885b07399baaa9eaeaaf55f12d0375f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, for an audio\nclip labeled as \u201cSpeech\u201d, we only consider adding child or\nparent labels in the speci\ufb01c \u201dSpeech\u201d branch of the ontology.\nAs shown in Algorithm 2, the proposed approach consists of\nthe following steps. First, we train a teacher model using the\nfull AudioSet with the original label set. Second, we set a label\nmodi\ufb01cation threshold for each audio tagging, speci\ufb01cally,", "mimetype": "text/plain", "start_char_idx": 5347, "end_char_idx": 5740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea0f125e-91af-4ca3-a8c4-ce765d19dcb0": {"__data__": {"id_": "ea0f125e-91af-4ca3-a8c4-ce765d19dcb0", "embedding": null, "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cefd5f33d228a429be12ccd53d0a09214b9cbbc1dd12c5cf896555ce4d6b8597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314", "node_type": "1", "metadata": {}, "hash": "09514b6304ce4a5f654cd8a3ce13fac7fed6635acf0348442bc3d29cb31bb4b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 9\nTABLE VIII\nRESULT OF LABEL ENHANCEMENT ON THE BALANCED SET (NOTE THE\nMAP WITHOUT LABEL ENHANCEMENT IS 0.3108 \u00b10.0013).\nType I Type II Type I and II\n# Impact Classes 212 93 274\nLabel Added (%) 3.7% 3.9% 7.2%\nImpacted Class Improvement 4.5% 3.8% 4.5%\nNon-impacted Class Improvement 1.9% 2.1% 1.3%\nMean Class-wise Relative Improv. 3.0% 2.4% 2.9%\nmAP Improvement 1.9% 1.5% 1.7%\nmAP 0.3166\n\u00b10.0016\n0.3156\n\u00b10.0007\n0.3162\n\u00b10.0005\nwe set the threshold of a class as the teacher model\u2019s mean\nprediction score of all audio clips originally labeled as that\nclass (lines 1-2). The threshold can also be set as other values\nsuch as the 5th, 10th, or 25th percentile of the teacher model\u2019s\nprediction score. The lower the threshold, the more labels are\nadded. We then identify all samples that need to be relabeled.\nFor each sample, we compile all child (Type I) and/or parent\n(Type II) labels of all original labels as the candidate set\naccording to the AudioSet ontology (line 6). For each label\nin the candidate set, if the teacher model\u2019s prediction score of\nthe class is greater than the corresponding label modi\ufb01cation\nthreshold, we add it to the labels of the sample (line 7-8).\nFinally, we retrain the model from scratch with the enhanced\nlabel set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1309, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314": {"__data__": {"id_": "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314", "embedding": null, "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cefd5f33d228a429be12ccd53d0a09214b9cbbc1dd12c5cf896555ce4d6b8597", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea0f125e-91af-4ca3-a8c4-ce765d19dcb0", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "da4d5bdfc1d748851cc31aaccda481a53c8a8a8789824bae8ce3f91ad0097e46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2129011b-7d24-403b-baa1-b4b26de12f0f", "node_type": "1", "metadata": {}, "hash": "0a2064de9aa28a6423a710302c7d92fcc03d34327e70089da6fd680baaf27f7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each label\nin the candidate set, if the teacher model\u2019s prediction score of\nthe class is greater than the corresponding label modi\ufb01cation\nthreshold, we add it to the labels of the sample (line 7-8).\nFinally, we retrain the model from scratch with the enhanced\nlabel set.\nWe apply the proposed label enhancement method (with the\nteacher model\u2019s mean prediction score as the label modi\ufb01ca-\ntion threshold) on the balanced training set and show the re-\nsults in Table VIII. Note the model without label enhancement\nhas an mAP of 0.3108 \u00b10.0013 (the model from the previous\nsection). The key \ufb01ndings are as follows: First, a noticeable\nnumber of labels are added, and over half of the classes are\nimpacted, which further indicates that the missing label issue\nis prevalent in AudioSet. Second, enhancing the label improves\nthe performance of both impacted and non-impacted classes,\nbut the impacted classes have a larger relative improvement.\nThird, the mean class-wise relative AP improvement is larger\nthan the relative mean AP (mAP) improvement, indicating\nthat more of the classes that improved originally had below-\naverage performance. This supports our hypothesis that the\nmissing label problem lowers the performance of a sound\nclass. Fourth, we evaluate the performance of \ufb01xing Type\nI errors, Type II errors, and \ufb01xing both. The improvement\nachieved by \ufb01xing Type I errors is larger than \ufb01xing Type II\nerrors. Fixing both cannot further improve the performance.\nFifth, since the performance improvement is relatively minor,\nwe run all experiments three times with different random seeds\nand report both the mean and standard deviation. As shown\nin the table, the results verify the statistical signi\ufb01cance of the\nimprovement.", "mimetype": "text/plain", "start_char_idx": 1035, "end_char_idx": 2770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2129011b-7d24-403b-baa1-b4b26de12f0f": {"__data__": {"id_": "2129011b-7d24-403b-baa1-b4b26de12f0f", "embedding": null, "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cefd5f33d228a429be12ccd53d0a09214b9cbbc1dd12c5cf896555ce4d6b8597", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "03b1dfc59b4041eca15a027127786b04b9e506d03b6e8b3466fbf36a1f69f266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e70d9cdb-c1ca-4814-acd9-60a404b92b30", "node_type": "1", "metadata": {}, "hash": "d926f07efda9d3949b30a744784e82e19375f3a7a51319150275db1f23498940", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The improvement\nachieved by \ufb01xing Type I errors is larger than \ufb01xing Type II\nerrors. Fixing both cannot further improve the performance.\nFifth, since the performance improvement is relatively minor,\nwe run all experiments three times with different random seeds\nand report both the mean and standard deviation. As shown\nin the table, the results verify the statistical signi\ufb01cance of the\nimprovement. Finally, we also applied the label enhancement\nmethod on the full AudioSet, however, we did not observe a\nperformance improvement. Fixing Type I, Type II, and both\nerrors leads to mAPs of 0.4400, 0.4387, and 0.4386, respec-\ntively, while the model without label enhancement achieves an\nmAP of 0.4397 \u00b10.0007. We believe the main reason for the\nrelatively small improvement achieved by label enhancement\nis that the same label noise exists consistently in both the\ntraining set and evaluation set. Therefore, merely applying\nlabel enhancement on the training set leads to a mismatch\nbetween the training and evaluation sets. The performance\nresults do not therefore fully re\ufb02ect the actual improvement.\nIn addition, it is possible that the label modi\ufb01cation threshold\nis not appropriate for the full AudioSet.\nIn order to verify these hypotheses, we evaluate our model\non existing datasets with more accurate annotation including\nESC-50 [1] and FSD50K [11], and also test various label\nmodi\ufb01cation thresholds. ESC-50 contains 2,000 audio samples\nof 50 sound classes, among which 40 classes are overlapped\nwith the AudioSet. Therefore, we evaluate our model trained\nwith AudioSet on the 1,600 samples that are labeled as these\n40 overlapped classes.", "mimetype": "text/plain", "start_char_idx": 2370, "end_char_idx": 4018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e70d9cdb-c1ca-4814-acd9-60a404b92b30": {"__data__": {"id_": "e70d9cdb-c1ca-4814-acd9-60a404b92b30", "embedding": null, "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cefd5f33d228a429be12ccd53d0a09214b9cbbc1dd12c5cf896555ce4d6b8597", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2129011b-7d24-403b-baa1-b4b26de12f0f", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "05b7ca14570e070715f63f9d4bc3c978d39ad3e00410e4326461dfc304f11bf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e4f0a5d-cdc0-4d2e-aba7-653f1013c0a6", "node_type": "1", "metadata": {}, "hash": "25a33139b480ce55c47f51b2426ed593b10dd8555222cd83984ff5b88d3c4d4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ESC-50 contains 2,000 audio samples\nof 50 sound classes, among which 40 classes are overlapped\nwith the AudioSet. Therefore, we evaluate our model trained\nwith AudioSet on the 1,600 samples that are labeled as these\n40 overlapped classes. FSD50K is a recently collected data\nset of sound event audio clips with 200 classes drawn from\nthe AudioSet ontology. The FSD50K evaluation set is more\ncarefully annotated compared with the training and validation\nset and can be used as fair references. Since the length of\nAudioSet model input is 10s while a small portion of FSD50K\naudio clips are longer than 10s, we cut all FSD50K audio clips\nto 10s for testing. In addition, we also apply the proposed label\nenhancement algorithm on the AudioSet evaluation set and\ngenerate enhanced evaluation sets. We include the enhanced\nAudioSet evaluation sets as additional evaluation sets.\nWe evaluate various label modi\ufb01cation thresholds including\nthe mean, 25th percentile (25P), 10th percentile (10P), and\n5th percentile (5P) of the teacher model\u2019s prediction score of\nall audio clips originally labeled as that class. The lower the\nthreshold, the more labels are modi\ufb01ed, e.g., using the 5th\npercentile of the prediction score as the threshold changes\nthe largest number of labels. We then train models with\nthe four enhanced label sets and compare their results on\nseven evaluation sets (ESC-50, FSD50K, original AudioSet\nevaluation set, and four enhanced AudioSet evaluation set with\ndifferent label modi\ufb01cation thresholds).", "mimetype": "text/plain", "start_char_idx": 3780, "end_char_idx": 5294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e4f0a5d-cdc0-4d2e-aba7-653f1013c0a6": {"__data__": {"id_": "4e4f0a5d-cdc0-4d2e-aba7-653f1013c0a6", "embedding": null, "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cefd5f33d228a429be12ccd53d0a09214b9cbbc1dd12c5cf896555ce4d6b8597", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e70d9cdb-c1ca-4814-acd9-60a404b92b30", "node_type": "1", "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "d9b07b4789a318cb7cee1cc47eb448ea8cfeee85c20554ec183b1d5f56bb226e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lower the\nthreshold, the more labels are modi\ufb01ed, e.g., using the 5th\npercentile of the prediction score as the threshold changes\nthe largest number of labels. We then train models with\nthe four enhanced label sets and compare their results on\nseven evaluation sets (ESC-50, FSD50K, original AudioSet\nevaluation set, and four enhanced AudioSet evaluation set with\ndifferent label modi\ufb01cation thresholds).\nAs shown in Table IX, we \ufb01nd that models trained with\nenhanced AudioSet label sets consistently outperforms the\nmodel trained with the original AudioSet label set on all\nevaluation sets except the original AudioSet evaluation set,\ndemonstrating that the proposed label enhancement algorithm\nis able to improves the model performance, the reason why we\ncannot observe the improvement on the AudioSet evaluation\nset is that the evaluation set itself contains annotation errors.\nWhile there is no threshold that is optimal for all evaluation\nsets, for both balanced and full AudioSet experiments, we \ufb01nd\nthe mean and 25th percentile of the teacher model\u2019s prediction\nscore are the most appropriate label modi\ufb01cation thresholds.\nWe believe it is an important and non-negligible topic for\nfuture AudioSet and general audio tagging research because\nnoisy labels are inevitable for a large-scale dataset and errors\nwill impact model performance. In the following section, we\nuse models trained with the enhanced label set as default for\nall balanced set experiments.", "mimetype": "text/plain", "start_char_idx": 4886, "end_char_idx": 6354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97ac92bf-f69e-4d9e-bbe7-e0db72980337": {"__data__": {"id_": "97ac92bf-f69e-4d9e-bbe7-e0db72980337", "embedding": null, "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2a4af19bf7bb36175d405e86790183ba68a9e771270d89556bcefcb7c19fa1cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a27d1161-6737-4ac4-b6fa-44e2f3b31db1", "node_type": "1", "metadata": {}, "hash": "c0b544562691fe826042b2bb798000fb625bb4e256225fdd6bd221e2dfea46e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 10\nTABLE IX\nAUDIO SET LABEL ENHANCEMENT (LE) EXPERIMENT RESULTS . WE USE THE MEAN , 25 TH PERCENTILE (25P), 10 TH PERCENTILE (10P), AND 5TH\nPERCENTILE (5P) OF THE PREDICTION SCORE AS THE LABEL MODIFICATION THRESHOLDS AND GENERATE 4 ENHANCED AUDIO SET TRAINING LABEL\nSETS AND EVALUATION LABEL SETS . WE THEN TRAIN THE MODEL WITH THE ENHANCED TRAINING SETS AND EVALUATE IT ON VARIOUS EVALUATION\nSETS . THE RESULTS SHOW THAT THE MODEL TRAINED WITH ENHANCED LABEL SETS CONSISTENTLY OUTPERFORMS THE MODEL TRAINED WITH\nORIGINAL LABEL SETS ON ALL EVALUATION SETS EXCEPT THE ORIGINAL AUDIO SET EVALUATION SET .\nLabel\nAdded (%)\nESC-50\n40 Classes\nFSD50k\nEval\nAudioSet\nEval Ori\nAudioSet\nEval Mean\nAudioSet\nEval 25P\nAudioSet\nEval 10P\nAudioSet\nEval 5P\nAudioSet Balanced Training Set\nNo LE 0.0 0.7320 0.3443 0.3123 0.3485 0.3632 0.3540 0.3417\nLE, Mean 7.2 0.7573 0.3549 0.3162 0.3591 0.3739 0.3630 0.3500\nLE, 25P 22.8 0.7639 0.3680 0.3165 0.3632 0.3855 0.3760 0.3628\nLE, 10P 44.5 0.7551 0.3639 0.3078 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a27d1161-6737-4ac4-b6fa-44e2f3b31db1": {"__data__": {"id_": "a27d1161-6737-4ac4-b6fa-44e2f3b31db1", "embedding": null, "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2a4af19bf7bb36175d405e86790183ba68a9e771270d89556bcefcb7c19fa1cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97ac92bf-f69e-4d9e-bbe7-e0db72980337", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "f7fd143760ea7da174274b06f80fd08fd94ff101d08b7b88ac9dd70711bda30f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b349b525-3127-4814-a68a-088b10e9ad63", "node_type": "1", "metadata": {}, "hash": "3e38ab1000891881e5fc8796bb939a570075bc24e7f5fdebee07ac7e966095e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3549 0.3162 0.3591 0.3739 0.3630 0.3500\nLE, 25P 22.8 0.7639 0.3680 0.3165 0.3632 0.3855 0.3760 0.3628\nLE, 10P 44.5 0.7551 0.3639 0.3078 0.3527 0.3840 0.3811 0.3699\nLE, 5P 60.2 0.7548 0.3766 0.3078 0.3518 0.3862 0.3880 0.3790\nAudioSet Full Training Set\nNo LE 0.0 0.8587 0.4977 0.4397 0.5053 0.5143 0.4930 0.4723\nLE, Mean 11.1 0.8772 0.5079 0.4386 0.5075 0.5190 0.4977 0.4769\nLE, 25P 37.3 0.8736 0.5097 0.4296 0.4999 0.5267 0.5093 0.4891\nLE, 10P 77.7 0.8608 0.5078 0.4094 0.4752 0.5178 0.5121 0.4969\nLE, 5P 111.9 0.8534 0.4988 0.3936 0.4560 0.5047 0.5088 0.", "mimetype": "text/plain", "start_char_idx": 915, "end_char_idx": 1470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b349b525-3127-4814-a68a-088b10e9ad63": {"__data__": {"id_": "b349b525-3127-4814-a68a-088b10e9ad63", "embedding": null, "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2a4af19bf7bb36175d405e86790183ba68a9e771270d89556bcefcb7c19fa1cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a27d1161-6737-4ac4-b6fa-44e2f3b31db1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "5ebfbb57a418b59d1c756e021412c0d2f026f2e32fda1923fc143478810b3abc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423c2908-9082-4753-850c-d644dfd7af54", "node_type": "1", "metadata": {}, "hash": "472858de6ace119775b6c9d88411f8d68940d9723fd589d30c39d37b331d633f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5267 0.5093 0.4891\nLE, 10P 77.7 0.8608 0.5078 0.4094 0.4752 0.5178 0.5121 0.4969\nLE, 5P 111.9 0.8534 0.4988 0.3936 0.4560 0.5047 0.5088 0.4987\nTABLE X\nPERFORMANCE IMPACT ON M AP D UE TO WEIGHT AVERAGING\nBalanced Set Full Set\nWithout Weight Averaging 0.3162 \u00b10.0005 0.4397 \u00b10.0007\nWith Weight Averaging 0.3192 \u00b10.0015 0.4435 \u00b10.0008\nVI. W EIGHT AVERAGING AND ENSEMBLE\nA. Model Weight Averaging\nIn this section, we explore improving model performance\nby aggregating multiple models. The \ufb01rst strategy we explore\nis weight averaging [51]. Weight averaging performs an equal\naverage of the weights traversed by the optimizer, which makes\nthe solution fall in the center, rather than the boundary, of a\nwide \ufb02at low-loss region and thus lead to better generalization\nthan conventional training. Empirically, weight averaging has\nbeen shown to improve the performance of various models\nsuch as VGG [52], ResNets [20], and DenseNets [53] on a\nvariety of tasks [51], [54]. While weight averaging is usually\napplied with a high constant or cyclical learning rate, we \ufb01nd\nit is helpful even when used together with a weight decay\nstrategy.\nIn this work, we simply average all weights of the model\ncheckpoints at multiple epochs.", "mimetype": "text/plain", "start_char_idx": 1332, "end_char_idx": 2550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "423c2908-9082-4753-850c-d644dfd7af54": {"__data__": {"id_": "423c2908-9082-4753-850c-d644dfd7af54", "embedding": null, "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2a4af19bf7bb36175d405e86790183ba68a9e771270d89556bcefcb7c19fa1cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b349b525-3127-4814-a68a-088b10e9ad63", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "c4d1e84357863c9891d912d0d715113745ed4d2dc02c52a0a88d950b313d62d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d26239fa-4f54-4bd7-88c0-cb9c1000a81a", "node_type": "1", "metadata": {}, "hash": "7eaf540f77c886548b78cab049b3284cddea316b5c49dd770f897e4346204c2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While weight averaging is usually\napplied with a high constant or cyclical learning rate, we \ufb01nd\nit is helpful even when used together with a weight decay\nstrategy.\nIn this work, we simply average all weights of the model\ncheckpoints at multiple epochs. For both balanced set and full\nset experiments, we start averaging model checkpoints of every\nepoch after the learning rate is decreased to 1/4 of the initial\nlearning rate (i.e., the 41st and the 16th epochs, respectively)\nuntil the end of the training. As shown in Table X, weight\naveraging leads to a 0.9% improvement for both balanced\nset and full set experiment. We further \ufb01nd the improvement\nis not sensitive to exactly when weight averaging begins. As\nshown in Figure 8, starting averaging at any epoch after the\n10th epochs (until the last epoch) can outperform any single\ncheckpoint model for the full set experiment.\nIn summary, weight averaging is easy to implement, adds no\nadditional cost to training and inference, but can consistently\nimprove model performance. By applying weight averaging\nFig. 8. Relationship of the performance of averaging models with the epoch\nstarts to average. For both weight and prediction averaging, we average all\ncheckpoints from the starting epoch to the last epoch, i.e., the earlier to start\naveraging, the more checkpoints are averaged. Note that the improvement of\nmodel averaging is not sensitive to exactly when weight averaging begins.\nFor weight averaging, the optimal starting epoch is around the 15th epoch\nwhile starting averaging at any epoch after the 10th epochs can outperform\nany single checkpoint. For prediction averaging, starting averaging from the\n\ufb01rst epoch leads to the highest mAP, indicating averaging all checkpoints\nis optimal, while starting averaging at any epoch can outperform any single\ncheckpoint.", "mimetype": "text/plain", "start_char_idx": 2297, "end_char_idx": 4127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d26239fa-4f54-4bd7-88c0-cb9c1000a81a": {"__data__": {"id_": "d26239fa-4f54-4bd7-88c0-cb9c1000a81a", "embedding": null, "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db", "node_type": "4", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2a4af19bf7bb36175d405e86790183ba68a9e771270d89556bcefcb7c19fa1cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423c2908-9082-4753-850c-d644dfd7af54", "node_type": "1", "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "04622d9860417570dba6a3c8a491bed96a83766d25b4833d84d77378832c7a29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the improvement of\nmodel averaging is not sensitive to exactly when weight averaging begins.\nFor weight averaging, the optimal starting epoch is around the 15th epoch\nwhile starting averaging at any epoch after the 10th epochs can outperform\nany single checkpoint. For prediction averaging, starting averaging from the\n\ufb01rst epoch leads to the highest mAP, indicating averaging all checkpoints\nis optimal, while starting averaging at any epoch can outperform any single\ncheckpoint. However, averaging the predictions of the last few checkpoints\nbarely outperforms single checkpoints, indicating the importance of diversity.\nto our models, we get our best single model with an mAP of\n0.3192 and 0.4435 for balanced and full AudioSet experiment,\nrespectively.\nB. Ensemble\nFinally, we explore a series of ensemble strategies. The goal\nof ensemble methods is to combine the predictions of several\nmodels to improve generalizability and robustness over any\nsingle model. Previously, ensemble of audio tagging models\nhas been studied in in [15], [55], [56], [57], [16], [13], [4], but\ntypically only one strategy is covered in each of these previous", "mimetype": "text/plain", "start_char_idx": 3637, "end_char_idx": 4789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc0d8d1d-5d9a-4af7-ab29-a8fde5c3837a": {"__data__": {"id_": "cc0d8d1d-5d9a-4af7-ab29-a8fde5c3837a", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17b8883c-b686-419e-8d8e-0cbd44d4f27f", "node_type": "1", "metadata": {}, "hash": "ec8456230e1cf50a4ad6a2e952d78b1d572d6a5e89cc89688d336598b228d0ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 11\nTABLE XI\nRESULTS OF MODEL ENSEMBLE . FOR EACH EXPERIMENT , WE SHOW THE\nNUMBER OF THE MODELS IN THE COMMITTEE (# M ODELS ), THE AVERAGE\nMAP OF MODELS IN THE COMMITTEE (AVG M AP), THE M AP OF THE BEST\nMODEL IN THE COMMITTEE (BEST M AP), AND THE M AP OF THE\nENSEMBLE MODEL (ENSEMBLE M AP). N OTE THAT FOR ALL\nEXPERIMENTS , THE ENSEMBLE M AP IS HIGHER THAN THE BEST M AP.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17b8883c-b686-419e-8d8e-0cbd44d4f27f": {"__data__": {"id_": "17b8883c-b686-419e-8d8e-0cbd44d4f27f", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc0d8d1d-5d9a-4af7-ab29-a8fde5c3837a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ec695d552410ea291dbbcdf5f26ddf08a4493eed0eda4930a6b9f5935bedf6d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40a10066-c6ab-40d2-b2c1-0434d256200e", "node_type": "1", "metadata": {}, "hash": "0a36c106060d49eafabcd13b3452b5c281538f6185cfef34d12eb5e573cb3c1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "N OTE THAT FOR ALL\nEXPERIMENTS , THE ENSEMBLE M AP IS HIGHER THAN THE BEST M AP.\n# Models Avg mAP Best mAP Ensemble mAP\nCheckpoints of a Single Run\nBalanced 60 0.2369 0.3169 0.3280\nFull 30 0.4236 0.4406 0.4518\nMultiple Runs with Same Setting\nBalanced 3 0.3162 0.3167 0.3446\nFull 3 0.4397 0.4405 0.4641\nModels Trained with Different Settings\nBal-pretrain 2 0.1978 0.2385 0.2410\nBal-mixup rate 5 0.3009 0.3123 0.3476\nBal-mixup-\u03b1 3 0.3071 0.3123 0.3418\nBal-augment 3 0.2775 0.3123 0.3281\nBal-label 4 0.3146 0.3169 0.3503\nBal-top5 5 0.3168 0.3180 0.3527\nBal-all 20 0.2987 0.3180 0.3620\nFull-pretrain 2 0.3831 0.3939 0.4006\nFull-augment 4 0.4080 0.4396 0.4578\nFull-label 4 0.4397 0.4400 0.4653\nFull-top5 5 0.4396 0.4405 0.4690\nFull-all 10 0.4201 0.4405 0.4744\nefforts.", "mimetype": "text/plain", "start_char_idx": 354, "end_char_idx": 1117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40a10066-c6ab-40d2-b2c1-0434d256200e": {"__data__": {"id_": "40a10066-c6ab-40d2-b2c1-0434d256200e", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b8883c-b686-419e-8d8e-0cbd44d4f27f", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "33e4f12ebd374a0797008b3202d765f0b9aff749b63e10b33c3e92f0a652aa40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3594f721-d576-4d64-8f2e-589ef8f59690", "node_type": "1", "metadata": {}, "hash": "a7784691034099ccf06e47d356bdb283929590cddc219801cb553de0344c845b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this work, we use the simple voting algorithm, but\ncompare multiple ways of building the model committee. The\nreason why we do not use iterative ensemble methods (e.g.,\nBoosting) is because AudioSet training is expensive making\niterative training computationally unreasonable for this work.\n1) Checkpoint Averaging: The \ufb01rst strategy investigated\nis checkpoint averaging, whereby the output of checkpoint\nmodels at multiple epochs are averaged together. The imple-\nmentation is similar to weight averaging, but is conducted\nin the model space rather than the weight space. Since we\nconduct random sampling with replacement during full set\ntraining, the combination with checkpoint averaging is the\nsame as bootstrap aggregating (i.e., Bagging) [39]. In our\nexperiment, we average the output of all checkpoint models\n(i.e., 60 and 30 checkpoint models for the balanced set and\nfull set, respectively). As shown in the upper part of Table XI,\nthis approach works well. Speci\ufb01cally, the ensembled model\nnoticeably outperforms the best checkpoint model in the com-\nmittee. In addition, as shown in Figure 8, starting averaging\nfrom the \ufb01rst epoch leads to the highest mAP, indicating\naveraging all checkpoints is optimal. Averaging from any\nepoch can outperform the best single checkpoint model, which\ncan be a simple alternative. However, this approach greatly\nincreases the computational overhead of inference, which\nmakes it less practical in deployment.\n2) Averaging Models Trained with Different Random Seeds:\nPrevious work suggests that ensembles generalize better when\nthey constitute members that form a diverse and accurate\nset [58]. As shown in Figure 8, starting averaging the check-\npoint predictions from the last few epochs can only slightly\noutperform the best single checkpoint model, even though\nthese checkpoint models are quite accurate, indicating the im-\nportance of diversity.", "mimetype": "text/plain", "start_char_idx": 1118, "end_char_idx": 3015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3594f721-d576-4d64-8f2e-589ef8f59690": {"__data__": {"id_": "3594f721-d576-4d64-8f2e-589ef8f59690", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40a10066-c6ab-40d2-b2c1-0434d256200e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "e97fc521f248d35b5737a6587b4f9eca757923d224df0f5bfe0300ae296759fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e05d602c-8c2d-476c-b180-cd06b58596af", "node_type": "1", "metadata": {}, "hash": "1811f715c88cacb6389d4aac9030d850f26a76f0b1386c1816734d531c3bc131", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) Averaging Models Trained with Different Random Seeds:\nPrevious work suggests that ensembles generalize better when\nthey constitute members that form a diverse and accurate\nset [58]. As shown in Figure 8, starting averaging the check-\npoint predictions from the last few epochs can only slightly\noutperform the best single checkpoint model, even though\nthese checkpoint models are quite accurate, indicating the im-\nportance of diversity. Therefore, we run the experiment three\ntimes with the exact same setting, but with a different random\nseed. We then average the output of the last checkpoint model\nof each run. As shown in the middle part of Table XI, this\napproach leads to an even larger improvement than checkpoint\naveraging with only three models in the committee. Therefore,\naveraging models trained with different random seeds, while\nincreasing the training cost (due to the repeat runs), is more\npractical for deployment and offers better performance.\n3) Averaging Models Trained with Different Settings:\nFinally, we explore averaging more models with greater diver-\nsity. Speci\ufb01cally, we ensemble models trained with all different\nsettings tested in this paper, including whether pretraining is\nused (pretrain), different mix-up rates (mixup rate), different\nmix-up \u03b1 (mix-up-\u03b1), different augmentation settings (aug-\nment), and different label enhancement strategies (label). As\nshown in the lower part of Table XI, no matter how the model\ncommittee is built, ensemble always improves the performance\nand outperforms the best model in the committee. In the\nliterature, diversity is usually introduced with an intuitive\nmotivation. For example, in [15], the authors ensemble models\nuse different scale inputs because they believe the optimal\ninput scale varies with the target audio events, and ensembles\nallows the model to extract relevant information from inputs\nwith various scales.", "mimetype": "text/plain", "start_char_idx": 2575, "end_char_idx": 4476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e05d602c-8c2d-476c-b180-cd06b58596af": {"__data__": {"id_": "e05d602c-8c2d-476c-b180-cd06b58596af", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3594f721-d576-4d64-8f2e-589ef8f59690", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "d1d3a41548bbc01504864e7b9bb882adb7af0a59647ad32ef48c6f84fd861f63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc254142-af5e-4ceb-8172-6157fa17e2cf", "node_type": "1", "metadata": {}, "hash": "9c2a2a26f3a58007e91f9f1c7361a7cbb2fe30e1bb96595a3f000d4bac79fb8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the\nliterature, diversity is usually introduced with an intuitive\nmotivation. For example, in [15], the authors ensemble models\nuse different scale inputs because they believe the optimal\ninput scale varies with the target audio events, and ensembles\nallows the model to extract relevant information from inputs\nwith various scales. But according to our experimental results,\nthe source of the diversity seems to be less important, i.e., the\ndiversity caused by any factor is helpful for an ensemble.\nIn addition, we \ufb01nd the performance of the ensemble model\nis positively correlated with the accuracy of the models in the\ncommittee as well as the number of the models. For both\nthe balanced set and full set experiments, our best model is\nachieved when all available models form an ensemble.\nVII. S UPPLEMENTARY EXPERIMENTS\nTABLE XII\nABLATION STUDY RESULTS ON AUDIO SET.\nBalanced AudioSet Full AudioSet\nPSLA Model 0.3280 0.4518\nPSLA Model - Pretrain 0.2379 0.4302\nPSLA Model - Balanced Sampling - 0.3688\nPSLA Model - Masking 0.3154 0.4430\nPSLA Model - Mixup 0.3181 0.4493\nPSLA Model - Label Enhancement 0.3229 -\nPSLA Model - Ensemble 0.3162 0.4397\nPSLA Model - Ensemble + W A 0.3192 0.4435\nA. Ablation Study\nFrom Section III to Section VI, we incrementally improve\nmodel performance from the baseline by incorporating a\nnew technique with other techniques that have been found\nto be effective. In order to clearly identify the contribution\nof each technique and verify that all are necessary for the\nbest model, we conduct an ablation study on balanced and\nfull AudioSet.", "mimetype": "text/plain", "start_char_idx": 4141, "end_char_idx": 5716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc254142-af5e-4ceb-8172-6157fa17e2cf": {"__data__": {"id_": "cc254142-af5e-4ceb-8172-6157fa17e2cf", "embedding": null, "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "cb259fece2fc763ab4480f8b6bbe399252c8f85a7caa3524ea9e5256646e1a22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e05d602c-8c2d-476c-b180-cd06b58596af", "node_type": "1", "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1e5e45a0aa7aada8ffac19ce4f4b591f2bc89b9b0f2b7bc34076b526873f12f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to clearly identify the contribution\nof each technique and verify that all are necessary for the\nbest model, we conduct an ablation study on balanced and\nfull AudioSet. Speci\ufb01cally, we set the PSLA model with\ncheckpoints ensemble as the baseline (the best model for", "mimetype": "text/plain", "start_char_idx": 5539, "end_char_idx": 5813, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7e541e3-7ee6-441d-b24b-a3e8048f6384": {"__data__": {"id_": "e7e541e3-7ee6-441d-b24b-a3e8048f6384", "embedding": null, "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "df39ba84cf88ed8322cfc9d7372cad4f32561a2f5f09c192bb322968f43d181f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b97ba877-7824-4acc-8f9f-465e1472ea18", "node_type": "1", "metadata": {}, "hash": "f9e95ac7d2402c3d6b6512127fca2a587b7cdb20a6f7baa6003bdad4f0771a6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 12\na single training run), and then remove techniques from\nPSLA one by one, and check the performance. As shown\nin Table XII, removing any technique from PSLA leads to a\nperformance drop, demonstrating that all proposed techniques\nare useful. It is worth mentioning that removing balanced\nsampling leads to a signi\ufb01cant performance drop for AudioSet,\nthe performance of the model is worse than the model only\nwith pretraining (0.3939 mAP, in Table IV), indicating that\nother techniques (e.g., masking, mixup, and ensemble) should\nbe used together with balanced sampling for AudioSet. Besides\nbalanced sampling, removing pretraining leads to the largest\nperformance drop, followed by ensemble, time and frequency\nmasking, and mixup training for the full AudioSet.\nB. Experiment with Various Audio Tagging Models\nIn the previous sections, we focus on the Ef\ufb01cientNet-B2\nwith a 4-headed attention model described in Section II-C. In\norder to identify if the proposed PSLA framework is model-\nagnostic and explore the model size-performance trade-offs, in\nthis section, we evaluate the PSLA framework using 6 different\nmodels. All models take the same input and are trained with\nthe same setting as mentioned in Section II-B.\n1) MobileNet V2 [21]. The MobileNet model does not have\nan attention module. We use a fully connected layer as\nthe classi\ufb01cation layer.\n2) Ef\ufb01cientNet-B0 with single-headed attention model. The\nmodel architecture is the same as the model described\nin Section II-C except that it is based on a smaller\nEf\ufb01cientNet-B0 and only has one attention module.\n3) Ef\ufb01cientNet-B2 with mean pooling model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b97ba877-7824-4acc-8f9f-465e1472ea18": {"__data__": {"id_": "b97ba877-7824-4acc-8f9f-465e1472ea18", "embedding": null, "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "df39ba84cf88ed8322cfc9d7372cad4f32561a2f5f09c192bb322968f43d181f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7e541e3-7ee6-441d-b24b-a3e8048f6384", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "db6d2440949d564da6d587f5b32f236d9799a7a9d696ef8e652dbfb4da29f799", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76ab8edb-6470-4688-99d5-833556bef64a", "node_type": "1", "metadata": {}, "hash": "146048dbb6d3b8ab2b0fcbd24611c70e9820a54a48086e0a6198329e472b36c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The MobileNet model does not have\nan attention module. We use a fully connected layer as\nthe classi\ufb01cation layer.\n2) Ef\ufb01cientNet-B0 with single-headed attention model. The\nmodel architecture is the same as the model described\nin Section II-C except that it is based on a smaller\nEf\ufb01cientNet-B0 and only has one attention module.\n3) Ef\ufb01cientNet-B2 with mean pooling model. The model\narchitecture is the same as the model described in\nSection II-C except that it uses mean pooling rather than\nattention pooling.\n4) Ef\ufb01cientNet-B2 with single-headed attention model. The\nmodel architecture is the same as the model described in\nSection II-C except that it only has one attention module.\n5) Ef\ufb01cientNet-B2 with 4-headed attention model. This is\nthe model we use in from Section III to Section VI and\nis described in Section II-C.\n6) ResNet50 with single-headed attention module. This is\nthe model proposed in [4].\nTo save compute, for all PSLA models, we use the check-\npoint averaging ensemble that only requires a single training\nprocess, we also report the single model with weight averaging\nfor all full AudioSet experiments. As shown in Table XIII,\nwhen trained with PSLA techniques, all models can achieve\na noticeable performance improvement. This justi\ufb01es that the\nproposed PSLA framework is model-agnostic.\nComparing the Ef\ufb01cientNet-B2 models with 4-headed atten-\ntion, single-headed attention, and mean pooling, we \ufb01nd while\nthe single 4-headed attention model performs best (0.4435\nmAP), the single-headed attention model and the mean pooling\nmodel only perform slightly worse.", "mimetype": "text/plain", "start_char_idx": 1308, "end_char_idx": 2892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76ab8edb-6470-4688-99d5-833556bef64a": {"__data__": {"id_": "76ab8edb-6470-4688-99d5-833556bef64a", "embedding": null, "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "df39ba84cf88ed8322cfc9d7372cad4f32561a2f5f09c192bb322968f43d181f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b97ba877-7824-4acc-8f9f-465e1472ea18", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "e8a5742b506655ef2fd0a9129e568dd4483d8e8fac34c19645c47b6575dc08ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5833bc4-80b0-4710-b1fb-16a5da2cd633", "node_type": "1", "metadata": {}, "hash": "444417ad5557ae8fbaf0a8e0455d61a7962763fd6ce228db2bb66ee38b086a3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This justi\ufb01es that the\nproposed PSLA framework is model-agnostic.\nComparing the Ef\ufb01cientNet-B2 models with 4-headed atten-\ntion, single-headed attention, and mean pooling, we \ufb01nd while\nthe single 4-headed attention model performs best (0.4435\nmAP), the single-headed attention model and the mean pooling\nmodel only perform slightly worse. The Ef\ufb01cientNet-B0 model\nwith single-headed attention that has 5.36M parameters also\nachieves a comparable performance with the best existing\nmodel that has 81M parameters [5]. The choice of the model\ndepends on the application, e.g., attention-based models can\nbe used for frame-level tagging; models with mean pooling\ncan be used for streaming applications; smaller models are\npreferable for resource-constrained devices.\nWe also compute the Pearson correlation of class-wise APs\nbetween these models and \ufb01nd that the correlation of class-\nwise APs are high (over 0.95), this con\ufb01rms that the poor\nperformance of some class is not due to model architecture,\nbut due to the data.\nC. Experiment on FSD50K\nIn the previous sections, we focus on AudioSet. To check\nthe generalizability of the proposed PSLA techniques, we also\nconduct a set of experiments on FSD50K [11]. Speci\ufb01cally,\nwe train the Ef\ufb01cientNet-B2 model with a 4-headed attention\nmodule with an initial learning rate of 5e-4 and a batch size\nof 24 for 40 epochs. The learning rate is cut in half every 5\nepochs after the 10th epoch. Since the maximum input audio\nlength of FSD50K is 30s, we pad all input audio clips to 30s.", "mimetype": "text/plain", "start_char_idx": 2554, "end_char_idx": 4079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5833bc4-80b0-4710-b1fb-16a5da2cd633": {"__data__": {"id_": "b5833bc4-80b0-4710-b1fb-16a5da2cd633", "embedding": null, "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "df39ba84cf88ed8322cfc9d7372cad4f32561a2f5f09c192bb322968f43d181f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76ab8edb-6470-4688-99d5-833556bef64a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "d1166c252b9765a24e3338996ee330d2b844398bb9c7f377afa32100aa2cb9b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62b74a8b-cd70-4c50-b298-223e5a6881c3", "node_type": "1", "metadata": {}, "hash": "5cca7580c9e096a283771cccf1c88b792bdf95dfab21baf1e7c2caac6802ed4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Speci\ufb01cally,\nwe train the Ef\ufb01cientNet-B2 model with a 4-headed attention\nmodule with an initial learning rate of 5e-4 and a batch size\nof 24 for 40 epochs. The learning rate is cut in half every 5\nepochs after the 10th epoch. Since the maximum input audio\nlength of FSD50K is 30s, we pad all input audio clips to 30s.\nFor the single model, we train it with the FSD50K training\nset, validate it on the FSD50K validation set, and evaluate\nit on the FSD50K evaluation set. We use the same weight\naveraging and checkpoint averaging ensemble setting as the\nAudioSet experiments. We also conduct an ablation study on\nFSD50K.\nAs shown in Table XIV, our single model, weight averaging\nmodel, and ensemble model achieve an mAP of 0.5535,\n0.5571, and 0.5671 on the FSD50K evaluation set, respec-\ntively, all outperform the best existing model [59]. Removing\nany technique from PSLA leads to a performance drop,\ndemonstrating that all proposed techniques can be generalized\nto the FSD50K dataset.\nD. Learning Curve of PSLA models\nFig. 9. The learning curve of our experiments. Each experiment is run three\ntimes, and the stand deviation is shown in the shade.\nWe show the learning curve of our best single Ef\ufb01cientNet\nB2 with 4-headed attention model (without weight averaging)\nin Figure 9. For both the balanced set and full set experiment,\nwe repeat the training process three times with different\nrandom seeds and show the standard deviation in the plot.", "mimetype": "text/plain", "start_char_idx": 3762, "end_char_idx": 5208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62b74a8b-cd70-4c50-b298-223e5a6881c3": {"__data__": {"id_": "62b74a8b-cd70-4c50-b298-223e5a6881c3", "embedding": null, "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "df39ba84cf88ed8322cfc9d7372cad4f32561a2f5f09c192bb322968f43d181f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5833bc4-80b0-4710-b1fb-16a5da2cd633", "node_type": "1", "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "8b5b7fcca151f4c2993cdec8e0c06b9c63dbf884719e60dd47dcea55d8e7391d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9. The learning curve of our experiments. Each experiment is run three\ntimes, and the stand deviation is shown in the shade.\nWe show the learning curve of our best single Ef\ufb01cientNet\nB2 with 4-headed attention model (without weight averaging)\nin Figure 9. For both the balanced set and full set experiment,\nwe repeat the training process three times with different\nrandom seeds and show the standard deviation in the plot.\nAs we can see, the training converges, and the performance\nof the model barely varies with the random seed, i.e., the three\nruns achieve almost the same result.", "mimetype": "text/plain", "start_char_idx": 4786, "end_char_idx": 5369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee0f177b-9942-404f-8ed7-6758c5a5dfea": {"__data__": {"id_": "ee0f177b-9942-404f-8ed7-6758c5a5dfea", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b0cce89-d72f-4e82-9d1a-9535ddbd0822", "node_type": "1", "metadata": {}, "hash": "49df6779af37f04a4a33382f933dba2bfab0f46e8693a0c8754a77d413ce8d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 13\nTABLE XIII\nCOMPARISON OF THE PERFORMANCE ON M AP OF VARIOUS MODELS TRAINED WITH PSLA AND WITHOUT PSLA ON THE BALANCED AND FULL\nAUDIO SET.\n# Params Balanced AudioSet Full AudioSet\nNo PSLA PSLA Imp.(%) No PSLA PSLA Imp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b0cce89-d72f-4e82-9d1a-9535ddbd0822": {"__data__": {"id_": "6b0cce89-d72f-4e82-9d1a-9535ddbd0822", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee0f177b-9942-404f-8ed7-6758c5a5dfea", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "3dbab6eb4cebc56d76119bee40f34107ce1eb62c4047dbd7a7ab450eb39ac108", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6769ff1-0f97-4709-9dda-4afd6a1f094c", "node_type": "1", "metadata": {}, "hash": "a5448937f4223c561c885cd58ed2ad9d3fa70e0bff0fde6847fc77f9bb0f129c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 13\nTABLE XIII\nCOMPARISON OF THE PERFORMANCE ON M AP OF VARIOUS MODELS TRAINED WITH PSLA AND WITHOUT PSLA ON THE BALANCED AND FULL\nAUDIO SET.\n# Params Balanced AudioSet Full AudioSet\nNo PSLA PSLA Imp.(%) No PSLA PSLA Imp.(%)\nMobileNet V2 2.90M 0.1612 0.2650 64.4 0.3032 0.4058\n(Single: 0.3940) 33.8\nEf\ufb01cientNet-B0, Single-headed Attention 5.36M 0.1529 0.3350 119.1 0.3789 0.4493\n(Single: 0.4391) 18.6\nEf\ufb01cientNet-B2, Mean Pooling 8.44M 0.1903 0.3317 74.3 0.3325 0.4455\n(Single: 0.4382) 34.0\nEf\ufb01cientNet-B2, Single-headed Attention 9.19M 0.1478 0.3406 130.4 0.3818 0.4556\n(Single: 0.4414) 19.3\nEf\ufb01cientNet-B2, 4-headed Attention 13.64M 0.1570 0.3280 108.9 0.3723 0.4518\n(Single: 0.4435) 21.4\nResNet-50, Single-headed Attention 25.66M 0.1635 0.3180 94.5 0.3790 0.4477\n(Single: 0.4042) 18.1\nTABLE XIV\nEXPERIMENT RESULT ON FSD50K DATASET.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6769ff1-0f97-4709-9dda-4afd6a1f094c": {"__data__": {"id_": "e6769ff1-0f97-4709-9dda-4afd6a1f094c", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b0cce89-d72f-4e82-9d1a-9535ddbd0822", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "69336c5088f8c68aedcc9d0a1530da7d62d1a6e7ae4c3ded3a8b5bdc6909f152", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "033463eb-6bfa-43b5-906c-ad1fb49c4b3f", "node_type": "1", "metadata": {}, "hash": "6a5595fabfd11902823b4907766dad9ef523d37f35f236aede2474d4162919c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "FSD50K Eval\nFSD50K Baseline [11] 0.434\nAudio Transformers [59] 0.537\nPSLA Model 0.5671\nPSLA Model - Pretrain 0.4524\nPSLA Model - Balanced Sampling 0.5626\nPSLA Model - Masking 0.5617\nPSLA Model - Mixup 0.5164\nPSLA Model - Label Enhancement 0.5583\nPSLA Model - Ensemble 0.5535\nPSLA Model - Ensemble + W A 0.5571\nVIII. C ONCLUSION\nIn this paper, we describe several techniques that improve\nthe performance of a CNN-based neural model for audio\ntagging. First, we show an ImageNet-pretrained CNN can\nnoticeably improve performance. While it is straightforward to\nimplement for CNN-based models it has seldom been used in\naudio tagging research. Second, due to an imbalance in sound\nclass samples in Audioset, we describe several data balancing\nand augmentation strategies that alleviate the data imbalance\nissue and help improve performance. We argue that balanced\nsampling and data augmentation should be a standard compo-\nnent for AudioSet modeling. Third, by observing variation in\nclass-speci\ufb01c performance, we identi\ufb01ed a missing label issue\nwith Audioset and proposed a label enhancement method that\nshows improvement on the balanced training set. The enhanced\nlabel set can be used in the same way as the original label set\nin future research. We were not able to observe a performance\nimprovement by enhancing the full set labels, possibly due to\nsimilar missing labels in the evaluation set. Due to its impact\non performance, we believe addressing the noisy label issue\nis an important research topic for audio tagging. Finally, we\ndescribe weight averaging and ensemble strategies that are\nboth simple and effective for audio tagging.", "mimetype": "text/plain", "start_char_idx": 898, "end_char_idx": 2538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "033463eb-6bfa-43b5-906c-ad1fb49c4b3f": {"__data__": {"id_": "033463eb-6bfa-43b5-906c-ad1fb49c4b3f", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6769ff1-0f97-4709-9dda-4afd6a1f094c", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "4fcec467937c88a7f7c555e294b3f7ee2e76a70b82a947cf834bfbf7b5dd2b02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d94f2189-770d-4665-827b-3735f71a4d50", "node_type": "1", "metadata": {}, "hash": "672e178cf0c308e1c9a0b0a5bd3b94bc885daec64aa19ac5636a41889cec813d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We were not able to observe a performance\nimprovement by enhancing the full set labels, possibly due to\nsimilar missing labels in the evaluation set. Due to its impact\non performance, we believe addressing the noisy label issue\nis an important research topic for audio tagging. Finally, we\ndescribe weight averaging and ensemble strategies that are\nboth simple and effective for audio tagging.\nBy combining all these training techniques, we are able to\nimprove the performance of a normal Ef\ufb01cientNet model by\n130.6% and 28.2% without modifying the model architecture\nfor the balanced and full AudioSet experiment, respectively.\nTABLE XV\nCOMPARISON WITH PREVIOUS METHODS (UPPER : BALANCED AUDIO SET\nEXPERIMENTS , LOWER : FULL AUDIO SET EXPERIMENTS ).\n#Params mAP AUC d\u2032\nWu-minimal [60], 2018 2.6M - 0.916 1.950\nKumar [61], 2018 - 0.213 0.927 2.056\nWu-best [60], 2018 56M - 0.927 2.056\nKong [8], 2019 - 0.274 0.949 2.316\nPANNs [5], 2020 81M 0.278 0.905 1.853\nOur Baseline 13.6M 0.1570 0.9108 1.903\nProposed Single Model 13.6M 0.3192\n\u00b10.0015\n0.9534\n\u00b10.0005\n2.374\n\u00b10.007\nProposed 68M Model 13.6M \u00d75 0.3527 0.9602 2.479\nProposed Full Model 13.6M \u00d720 0.3620 0.9638 2.", "mimetype": "text/plain", "start_char_idx": 2145, "end_char_idx": 3307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d94f2189-770d-4665-827b-3735f71a4d50": {"__data__": {"id_": "d94f2189-770d-4665-827b-3735f71a4d50", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "033463eb-6bfa-43b5-906c-ad1fb49c4b3f", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "2d1a6f87df4675121ffeb5d879f5f433c4139890c7c16c098f2dcd60102eb04a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6efd1c36-a551-4874-984b-a8a83ec83db7", "node_type": "1", "metadata": {}, "hash": "c6d907c297843f871e784e7884dacbff7bbb5496beb5557d90ef33a5ef4b71f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9108 1.903\nProposed Single Model 13.6M 0.3192\n\u00b10.0015\n0.9534\n\u00b10.0005\n2.374\n\u00b10.007\nProposed 68M Model 13.6M \u00d75 0.3527 0.9602 2.479\nProposed Full Model 13.6M \u00d720 0.3620 0.9638 2.541\nAudioSet Baseline [3] - 0.314 0.959 2.452\nKong [6], 2018 - 0.327 0.965 2.558\nYu [7], 2018 - 0.360 0.970 2.660\nTALNet [62], 2019 - 0.362 0.965 2.554\nKong [8], 2019 - 0.369 0.969 2.639\nDeepRes [4], 2019 26M 0.392 0.971 2.682\nPANNs [5], 2020 81M 0.439 0.973 2.725\nOur Baseline 13.6M 0.3723 0.9706 2.672\nProposed Single Model 13.6M 0.4435\n\u00b10.0008\n0.9753\n\u00b10.0003\n2.778\n\u00b10.007\nProposed 68M Model 13.6M \u00d75 0.4690 0.9789 2.872\nProposed Full Model 13.6M \u00d710 0.4744 0.9810 2.936\nThis magnitude of improvement is larger than was achieved\nby many previous model architecture or attention module\ndevelopment efforts, indicating that appropriate training tech-\nniques are equally important.", "mimetype": "text/plain", "start_char_idx": 3131, "end_char_idx": 3987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6efd1c36-a551-4874-984b-a8a83ec83db7": {"__data__": {"id_": "6efd1c36-a551-4874-984b-a8a83ec83db7", "embedding": null, "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c", "node_type": "4", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "84c8ef9abc651d869ef1392bb62feb60fb70f384b8858424296cf839bb05c917", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d94f2189-770d-4665-827b-3735f71a4d50", "node_type": "1", "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "224a60a87d1d1758d385f8b08a94ecd9e5459d622e3797859fc19e1281f6b182", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "778\n\u00b10.007\nProposed 68M Model 13.6M \u00d75 0.4690 0.9789 2.872\nProposed Full Model 13.6M \u00d710 0.4744 0.9810 2.936\nThis magnitude of improvement is larger than was achieved\nby many previous model architecture or attention module\ndevelopment efforts, indicating that appropriate training tech-\nniques are equally important. As a consequence, by training an\nEf\ufb01cientNet with these techniques, we obtain a single model\n(with 13.6M parameters) and an ensemble model that achieve\nmean average precision (mAP) scores of 0.444 and 0.474 on\nAudioSet, respectively, outperforming the previous best system\nof 0.439 with 81M parameters [5]. Our best model trained with\nonly the balanced AudioSet (\u223c1% of the full set) outperforms\nour baseline and many previous models trained with the full\nset. We show the AUC and d-prime of our models and compare\nthem with previous efforts in Table XV. The proposed model\noutperform previous models for all evaluation metrics.\nThe work in this paper can serve as a recipe for AudioSet\ntraining. Most of the proposed methods are model agnostic and", "mimetype": "text/plain", "start_char_idx": 3671, "end_char_idx": 4736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de2b97ff-76ca-4418-aca7-9d06049ab391": {"__data__": {"id_": "de2b97ff-76ca-4418-aca7-9d06049ab391", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70289da0-b495-4391-bd6f-5c7aa86b6217", "node_type": "1", "metadata": {}, "hash": "717b18acaf63bf554ca11cf3a3c351b2924289d5d81a674145fc79ed3da5293e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 14\ncan be combined together with various model architectures and\nattention modules. As we showed in the paper, the same model\ncan perform much better when it is trained with appropriate\ntechniques. We hope this work can facilitate future audio\ntagging research by documenting a set of strong and useful\ntraining techniques.\nIX. A CKNOWLEDGMENT\nThis work was supported in part by Signify.\nREFERENCES\n[1] K. J. Piczak, \u201cEsc: Dataset for environmental sound classi\ufb01cation,\u201d in\nProceedings of the 23rd ACM international conference on Multimedia ,\n2015, pp. 1015\u20131018.\n[2] P. Foster, S. Sigtia, S. Krstulovic, J. Barker, and M. D. Plumbley,\n\u201cChime-home: A dataset for sound source recognition in a domestic\nenvironment,\u201d in 2015 IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics (WASPAA) , 2015, pp. 1\u20135.\n[3] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.\nMoore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-\nlabeled dataset for audio events,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2017, pp. 776\u2013780.\n[4] L. Ford, H. Tang, F. Grondin, and J. R. Glass, \u201cA deep residual network\nfor large-scale acoustic scene analysis.\u201d in INTERSPEECH, 2019, pp.\n2568\u20132572.\n[5] Q. Kong, Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70289da0-b495-4391-bd6f-5c7aa86b6217": {"__data__": {"id_": "70289da0-b495-4391-bd6f-5c7aa86b6217", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de2b97ff-76ca-4418-aca7-9d06049ab391", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ba7685133a1629b250e329306be3e22c6b49f475924afdae5c32f3bae37b0378", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14f089fa-3fb0-40ce-8faf-297a185f267a", "node_type": "1", "metadata": {}, "hash": "7127aba13df53ef0e4a080f69a5305395827732dca74ed3434d9c973fcc2d8dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "776\u2013780.\n[4] L. Ford, H. Tang, F. Grondin, and J. R. Glass, \u201cA deep residual network\nfor large-scale acoustic scene analysis.\u201d in INTERSPEECH, 2019, pp.\n2568\u20132572.\n[5] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumbley,\n\u201cPanns: Large-scale pretrained audio neural networks for audio pattern\nrecognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 28, pp. 2880\u20132894, 2020.\n[6] Q. Kong, Y . Xu, W. Wang, and M. D. Plumbley, \u201cAudio set classi\ufb01cation\nwith attention model: A probabilistic perspective,\u201d in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2018,\npp. 316\u2013320.\n[7] C. Yu, K. S. Barsim, Q. Kong, and B. Yang, \u201cMulti-level attention model\nfor weakly supervised audio classi\ufb01cation,\u201d in DCASE Workshop, 2018.\n[8] Q. Kong, C. Yu, Y . Xu, T. Iqbal, W. Wang, and M. D. Plumbley, \u201cWeakly\nlabelled audioset tagging with attention neural networks,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing , vol. 27,\nno. 11, pp. 1791\u20131802, 2019.\n[9] S. Chen, J. Chen, Q. Jin, and A. Hauptmann, \u201cClass-aware self-attention\nfor audio event recognition,\u201d in Proceedings of the 2018 ACM on\nInternational Conference on Multimedia Retrieval , 2018, pp.", "mimetype": "text/plain", "start_char_idx": 1165, "end_char_idx": 2395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14f089fa-3fb0-40ce-8faf-297a185f267a": {"__data__": {"id_": "14f089fa-3fb0-40ce-8faf-297a185f267a", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70289da0-b495-4391-bd6f-5c7aa86b6217", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "d879ddc45b60acb022a0d0584bfbc2cef6b74d479b6dcced43047ff9a9668f92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9154366f-887e-47aa-b204-00f941f6e7f0", "node_type": "1", "metadata": {}, "hash": "aadb10a000d2885ee0b3a713b193079dca0cee8d561ccdd8fa6499d5d46bbda7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "27,\nno. 11, pp. 1791\u20131802, 2019.\n[9] S. Chen, J. Chen, Q. Jin, and A. Hauptmann, \u201cClass-aware self-attention\nfor audio event recognition,\u201d in Proceedings of the 2018 ACM on\nInternational Conference on Multimedia Retrieval , 2018, pp. 28\u201336.\n[10] M. Tan and Q. Le, \u201cEf\ufb01cientnet: Rethinking model scaling for con-\nvolutional neural networks,\u201d in International Conference on Machine\nLearning, 2019, pp. 6105\u20136114.\n[11] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, \u201cFsd50k:\nan open dataset of human-labeled sound events,\u201d arXiv preprint\narXiv:2010.00475, 2020.\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet:\nA large-scale hierarchical image database,\u201d in 2009 IEEE conference on\ncomputer vision and pattern recognition , 2009, pp. 248\u2013255.\n[13] K. Palanisamy, D. Singhania, and A. Yao, \u201cRethinking cnn models for\naudio classi\ufb01cation,\u201d arXiv preprint arXiv:2007.11154 , 2020.\n[14] E. Fonseca, S. Hershey, M. Plakal, D. P. Ellis, A. Jansen, and R. C.\nMoore, \u201cAddressing missing labels in large-scale sound event recogni-\ntion using a teacher-student framework with loss masking,\u201d IEEE Signal\nProcessing Letters, vol. 27, pp. 1235\u20131239, 2020.", "mimetype": "text/plain", "start_char_idx": 2162, "end_char_idx": 3338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9154366f-887e-47aa-b204-00f941f6e7f0": {"__data__": {"id_": "9154366f-887e-47aa-b204-00f941f6e7f0", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14f089fa-3fb0-40ce-8faf-297a185f267a", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1b80491f1a8d05842918d448c69da02a22e2a55411484e17aa5358186245b484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7", "node_type": "1", "metadata": {}, "hash": "eaf345ab7fa4f76da6962c1a9daff294d8537e24e82c2a749f170c19bc826204", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] E. Fonseca, S. Hershey, M. Plakal, D. P. Ellis, A. Jansen, and R. C.\nMoore, \u201cAddressing missing labels in large-scale sound event recogni-\ntion using a teacher-student framework with loss masking,\u201d IEEE Signal\nProcessing Letters, vol. 27, pp. 1235\u20131239, 2020.\n[15] D. Lee, S. Lee, Y . Han, and K. Lee, \u201cEnsemble of convolutional neural\nnetworks for weakly-supervised sound event detection using multiple\nscale input,\u201d in DCASE Workshop, 2017.\n[16] P. Lopez-Meyer, J. A. del Hoyo Ontiveros, G. Stemmer, L. Nach-\nman, and J. Huang, \u201cEnsemble of convolutional neural networks for\nthe DCASE 2020 acoustic scene classi\ufb01cation challenge,\u201d in DCASE\nWorkshop, 2020.\n[17] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\nin International Conference on Learning Representations , 2015.\n[18] A. P. Bradley, \u201cThe use of the area under the roc curve in the evaluation\nof machine learning algorithms,\u201d Pattern recognition, vol. 30, no. 7, pp.\n1145\u20131159, 1997.\n[19] J. Davis and M. Goadrich, \u201cThe relationship between precision-recall\nand roc curves,\u201d in Proceedings of the 23rd international conference on\nMachine learning, 2006.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770\u2013778.", "mimetype": "text/plain", "start_char_idx": 3074, "end_char_idx": 4407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7": {"__data__": {"id_": "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9154366f-887e-47aa-b204-00f941f6e7f0", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "1240159b61dc3a43781aca158318bb15f186b9ee68a6ad6e63cb03b98900651f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "207c77b5-9d1f-4bb0-a419-5b72314f561b", "node_type": "1", "metadata": {}, "hash": "9ae13a719176a6e682df91fdff10c4adcdced52e69cf44290a7dbd4b2731e1e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] J. Davis and M. Goadrich, \u201cThe relationship between precision-recall\nand roc curves,\u201d in Proceedings of the 23rd international conference on\nMachine learning, 2006.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770\u2013778.\n[21] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n\u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\n2018, pp. 4510\u20134520.\n[22] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard,\nand Q. V . Le, \u201cMnasnet: Platform-aware neural architecture search for\nmobile,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2019, pp. 2820\u20132828.\n[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of\ndeep bidirectional transformers for language understanding,\u201d in NAACL-\nHLT, 2019, pp. 4171\u20134186.\n[24] K. He, R. Girshick, and P. Doll \u00b4ar, \u201cRethinking imagenet pre-training,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 4918\u20134927.\n[25] Y .-A. Chung, W.-N.", "mimetype": "text/plain", "start_char_idx": 4046, "end_char_idx": 5267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "207c77b5-9d1f-4bb0-a419-5b72314f561b": {"__data__": {"id_": "207c77b5-9d1f-4bb0-a419-5b72314f561b", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "7985cc273072906796dddfd0d017712d70a87ca602da502188c80889d4686088", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17e7193b-cfff-48dc-bc30-99b0c64743d4", "node_type": "1", "metadata": {}, "hash": "ef66570ed3022585910416e8be514269ad4c01aee25c33c5cadc0aeb8d51a316", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4171\u20134186.\n[24] K. He, R. Girshick, and P. Doll \u00b4ar, \u201cRethinking imagenet pre-training,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 4918\u20134927.\n[25] Y .-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised\nautoregressive model for speech representation learning,\u201d Interspeech,\npp. 146\u2013150, 2019.\n[26] A. Saeed, D. Grangier, and N. Zeghidour, \u201cContrastive learning\nof general-purpose audio representations,\u201d arXiv preprint\narXiv:2010.10915, 2020.\n[27] J. Shor, A. Jansen, R. Maor, O. Lang, O. Tuval, F. de Chaumont Quitry,\nM. Tagliasacchi, I. Shavitt, D. Emanuel, and Y . Haviv, \u201cTowards learning\na universal non-semantic representation of speech,\u201d Interspeech, pp.\n140\u2013144, 2020.\n[28] M. Tagliasacchi, B. Gfeller, F. de Chaumont Quitry, and D. Roblek,\n\u201cPre-training audio representations with self-supervision,\u201d IEEE Signal\nProcessing Letters, vol. 27, pp. 600\u2013604, 2020.\n[29] A. Jansen, M. Plakal, R. Pandya, D. P. Ellis, S. Hershey, J. Liu, R. C.\nMoore, and R. A. Saurous, \u201cUnsupervised learning of semantic audio\nrepresentations,\u201d in IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2018, pp.", "mimetype": "text/plain", "start_char_idx": 5057, "end_char_idx": 6237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17e7193b-cfff-48dc-bc30-99b0c64743d4": {"__data__": {"id_": "17e7193b-cfff-48dc-bc30-99b0c64743d4", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "207c77b5-9d1f-4bb0-a419-5b72314f561b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "910186b5620387f44965dbdeb9ee05806e2964066fb727639870d662d64ca83c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8aa194b-9d72-4b7b-9c4a-e760442d85b1", "node_type": "1", "metadata": {}, "hash": "1a2e9fe45fef5d971d025d83717ba067a5e2a48b76b59e820724b0ce1f09b84e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "600\u2013604, 2020.\n[29] A. Jansen, M. Plakal, R. Pandya, D. P. Ellis, S. Hershey, J. Liu, R. C.\nMoore, and R. A. Saurous, \u201cUnsupervised learning of semantic audio\nrepresentations,\u201d in IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2018, pp. 126\u2013130.\n[30] L. Wang, K. Kawakami, and A. van den Oord, \u201cContrastive predictive\ncoding of audio with an adversary,\u201d Interspeech, pp. 826\u2013830, 2020.\n[31] J. Salamon, C. Jacoby, and J. P. Bello, \u201cA dataset and taxonomy for\nurban sound research,\u201d in Proceedings of the 22nd ACM international\nconference on Multimedia , 2014, pp. 1041\u20131044.\n[32] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation,\u201d in\nProceedings of the IEEE international conference on computer vision ,\n2015, pp. 1026\u20131034.\n[33] G. Gwardys and D. M. Grzywczak, \u201cDeep image features in music\ninformation retrieval,\u201dInternational Journal of Electronics and Telecom-\nmunications, vol. 60, no. 4, pp. 321\u2013326, 2014.\n[34] A. Guzhov, F. Raue, J. Hees, and A. Dengel, \u201cEsresnet: Environmental\nsound classi\ufb01cation based on visual domain models,\u201d in 2020 25th\nInternational Conference on Pattern Recognition (ICPR) , 2021, pp.\n4933\u20134940.", "mimetype": "text/plain", "start_char_idx": 5964, "end_char_idx": 7209, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8aa194b-9d72-4b7b-9c4a-e760442d85b1": {"__data__": {"id_": "a8aa194b-9d72-4b7b-9c4a-e760442d85b1", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17e7193b-cfff-48dc-bc30-99b0c64743d4", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "64a2626ef21990b3997036c4ff94200c7cba16f4b3363c72f0ea003344006ad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec687e40-ddd1-4202-a8d3-ab0e13274db8", "node_type": "1", "metadata": {}, "hash": "26e65aaa00341211d527938fb2e8c5b9d66bc8f31dea95ada706104fa97d01b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "60, no. 4, pp. 321\u2013326, 2014.\n[34] A. Guzhov, F. Raue, J. Hees, and A. Dengel, \u201cEsresnet: Environmental\nsound classi\ufb01cation based on visual domain models,\u201d in 2020 25th\nInternational Conference on Pattern Recognition (ICPR) , 2021, pp.\n4933\u20134940.\n[35] S. Adapa, \u201cUrban sound tagging using convolutional neural networks,\u201d\nin DCASE Workshop, 2019, p. 5.\n[36] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1\u20139.\n[37] H. He and E. A. Garcia, \u201cLearning from imbalanced data,\u201d IEEE\nTransactions on knowledge and data engineering , vol. 21, no. 9, pp.\n1263\u20131284, 2009.\n[38] B. Efron and R. J. Tibshirani, An introduction to the bootstrap . CRC\npress, 1994.\n[39] L. Breiman, \u201cBagging predictors,\u201d Machine Learning, vol. 24, no. 2, pp.\n123\u2013140, 1996.\n[40] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, \u201cSpecAugment: A simple data augmentation method for\nautomatic speech recognition,\u201d Interspeech, pp. 2613\u20132617, 2019.", "mimetype": "text/plain", "start_char_idx": 6963, "end_char_idx": 8094, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec687e40-ddd1-4202-a8d3-ab0e13274db8": {"__data__": {"id_": "ec687e40-ddd1-4202-a8d3-ab0e13274db8", "embedding": null, "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95385bde-ed41-4a53-962b-559aeedd3586", "node_type": "4", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "ebc6693aa0082c607bbac1c3d9b0b249a48d43316ee85f9a81248766531af831", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8aa194b-9d72-4b7b-9c4a-e760442d85b1", "node_type": "1", "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "79de35e8963baa6234f05ee788ccce5fa0fd32a37805e7bf7b6818234c464a5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24, no. 2, pp.\n123\u2013140, 1996.\n[40] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, \u201cSpecAugment: A simple data augmentation method for\nautomatic speech recognition,\u201d Interspeech, pp. 2613\u20132617, 2019.\n[41] Y . Tokozume, Y . Ushiku, and T. Harada, \u201cBetween-class learning\nfor image classi\ufb01cation,\u201d in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2018, pp. 5486\u20135494.\n[42] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, \u201cmixup: Beyond\nempirical risk minimization,\u201d in International Conference on Learning\nRepresentations, 2018.\n[43] Y . Tokozume, Y . Ushiku, and T. Harada, \u201cLearning from between-class\nexamples for deep sound recognition,\u201d in International Conference on\nLearning Representations, 2018.", "mimetype": "text/plain", "start_char_idx": 7862, "end_char_idx": 8637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8dc98971-ff36-407f-baf4-857e4f7a0812": {"__data__": {"id_": "8dc98971-ff36-407f-baf4-857e4f7a0812", "embedding": null, "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "a13b77106ed2362654dbd94563d07af3bd38a15d5fe70a79dcd38673d49b7a56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a08a900-13ed-4fc1-ad37-ebda5e215ec0", "node_type": "1", "metadata": {}, "hash": "9587ee1898eababb65ad5b768ed9544f5487a2e132956413900d4b1ab4a856c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 15\n[44] A. Mesaros, T. Heittola, A. Diment, B. Elizalde, A. Shah, E. Vincent,\nB. Raj, and T. Virtanen, \u201cDcase 2017 challenge setup: Tasks, datasets\nand baseline system,\u201d in DCASE Workshop, 2017.\n[45] A. Shah, A. Kumar, A. G. Hauptmann, and B. Raj, \u201cA closer look at\nweak label learning for audio events,\u201d arXiv preprint arXiv:1804.09288,\n2018.\n[46] L. H. Ford, \u201cLarge-scale acoustic scene analysis with deep residual\nnetworks,\u201d Master\u2019s thesis, Massachusetts Institute of Technology, 2019.\n[47] K. Wu and D. G. Childers, \u201cGender recognition from speech. part i:\nCoarse analysis,\u201d JASA, vol. 90, no. 4, pp. 1828\u20131840, 1991.\n[48] D. G. Childers and K. Wu, \u201cGender recognition from speech. part ii:\nFine analysis,\u201d JASA, vol. 90, no. 4, pp. 1841\u20131856, 1991.\n[49] Z.-Q. Wang and I. Tashev, \u201cLearning utterance-level representations for\nspeech emotion and age/gender recognition using deep neural networks,\u201d\nin IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2017, pp. 5150\u20135154.\n[50] M. Meire, L. Vuegen, and P. Karsmakers, \u201cThe impact of missing labels\nand overlapping sound events on multi-label multi-instance learning for\nsound event classi\ufb01cation,\u201d in DCASE Workshop, 2019, pp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a08a900-13ed-4fc1-ad37-ebda5e215ec0": {"__data__": {"id_": "3a08a900-13ed-4fc1-ad37-ebda5e215ec0", "embedding": null, "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "a13b77106ed2362654dbd94563d07af3bd38a15d5fe70a79dcd38673d49b7a56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dc98971-ff36-407f-baf4-857e4f7a0812", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "83eebff1b2059f9e841c87d88391ddd012908e21ee13d7e7e805e419830068e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d0634a1-d312-49c9-8f8d-7fbc31d8f614", "node_type": "1", "metadata": {}, "hash": "7caecbf30a957551b0eb251ba3202ff030d14e043d1ea61be717bda83d3dc37b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5150\u20135154.\n[50] M. Meire, L. Vuegen, and P. Karsmakers, \u201cThe impact of missing labels\nand overlapping sound events on multi-label multi-instance learning for\nsound event classi\ufb01cation,\u201d in DCASE Workshop, 2019, pp. 159\u2013163.\n[51] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson,\n\u201cAveraging weights leads to wider optima and better generalization,\u201d\nin 34th Conference on Uncertainty in Arti\ufb01cial Intelligence 2018, UAI\n2018, 2018, pp. 876\u2013885.\n[52] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for\nlarge-scale image recognition,\u201d 2015.\n[53] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \u201cDensely\nconnected convolutional networks,\u201d in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , 2017, pp. 4700\u20134708.\n[54] B. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson, \u201cThere\nare many consistent explanations of unlabeled data: Why you should\naverage,\u201d in International Conference on Learning Representations ,\n2019.\n[55] Z. Shi, L. Liu, H. Lin, R. Liu, A. Shi, and S. First, \u201cHodgepodge:\nSound event detection based on ensemble of semi-supervised learning\nmethods,\u201d in DCASE Workshop, 2019, p. 224.\n[56] Y .", "mimetype": "text/plain", "start_char_idx": 1063, "end_char_idx": 2253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d0634a1-d312-49c9-8f8d-7fbc31d8f614": {"__data__": {"id_": "9d0634a1-d312-49c9-8f8d-7fbc31d8f614", "embedding": null, "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "a13b77106ed2362654dbd94563d07af3bd38a15d5fe70a79dcd38673d49b7a56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a08a900-13ed-4fc1-ad37-ebda5e215ec0", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "b04e4506e0bd2a2924a023e4ad6426150d2c8bb313380a36657d1e37ecf81d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ac3e06e-fa9b-4ec5-a06a-fd548856600d", "node_type": "1", "metadata": {}, "hash": "423f961fbb22b60585c567fbf06de92d90205e8de0b55734c6d61ca8541120eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[55] Z. Shi, L. Liu, H. Lin, R. Liu, A. Shi, and S. First, \u201cHodgepodge:\nSound event detection based on ensemble of semi-supervised learning\nmethods,\u201d in DCASE Workshop, 2019, p. 224.\n[56] Y . Guo, M. Xu, Z. Wu, J. Wu, and B. Su, \u201cMulti-scale convolutional\nrecurrent neural network with ensemble method for weakly labeled\nsound event detection,\u201d in 2019 8th International Conference on Af-\nfective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW), 2019, pp. 1\u20135.\n[57] W. Lim, S. Suh, S. Park, and Y . Jeong, \u201cSound event detection in\ndomestic environments using ensemble of convolutional recurrent neural\nnetworks,\u201d in DCASE Workshop, 2019.\n[58] A. Chandra, H. Chen, and X. Yao, \u201cTrade-off between diversity and\naccuracy in ensemble generation,\u201d in Multi-objective machine learning .\nSpringer, 2006, pp. 429\u2013464.\n[59] P. Verma and J. Berger, \u201cAudio transformers: Transformer architectures\nfor large scale audio understanding. adieu convolutions,\u201d arXiv preprint\narXiv:2105.00335, 2021.\n[60] Y . Wu and T. Lee, \u201cReducing model complexity for dnn based large-\nscale audio classi\ufb01cation,\u201d in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , 2018, pp. 331\u2013335.", "mimetype": "text/plain", "start_char_idx": 2062, "end_char_idx": 3271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ac3e06e-fa9b-4ec5-a06a-fd548856600d": {"__data__": {"id_": "3ac3e06e-fa9b-4ec5-a06a-fd548856600d", "embedding": null, "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "a13b77106ed2362654dbd94563d07af3bd38a15d5fe70a79dcd38673d49b7a56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d0634a1-d312-49c9-8f8d-7fbc31d8f614", "node_type": "1", "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}, "hash": "3b17d90397d9321953cc3581169cadcc1e712240beea66c2ac07347e75e99f36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "adieu convolutions,\u201d arXiv preprint\narXiv:2105.00335, 2021.\n[60] Y . Wu and T. Lee, \u201cReducing model complexity for dnn based large-\nscale audio classi\ufb01cation,\u201d in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , 2018, pp. 331\u2013335.\n[61] A. Kumar, M. Khadkevich, and C. F \u00a8ugen, \u201cKnowledge transfer from\nweakly labeled audio using convolutional neural network for sound\nevents and scenes,\u201d in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2018, pp. 326\u2013330.\n[62] Y . Wang, J. Li, and F. Metze, \u201cA comparison of \ufb01ve multiple instance\nlearning pooling functions for sound event detection with weak labeling,\u201d\nin IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2019, pp. 31\u201335.", "mimetype": "text/plain", "start_char_idx": 3004, "end_char_idx": 3786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5461f41f-ac13-41b1-90d2-d5e64bc5e33c": {"__data__": {"id_": "5461f41f-ac13-41b1-90d2-d5e64bc5e33c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4e31095-9605-4685-afdd-66b105bcdd09", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3117b754733d8102dfbb469d7f640c0853bdcd76806c7cdb39535cd574b9cc0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc579127-3253-49da-aeb3-062e737be11c", "node_type": "1", "metadata": {}, "hash": "c6f6fb1f98860b3a60465b3d9e4461f6c243bfefe17e568129f7397fb212da07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2450 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\nSound Event Detection of Weakly Labelled Data\nWith CNN-Transformer and Automatic\nThreshold Optimization\nQiuqiang Kong , Student Member , IEEE, Yong Xu , Member , IEEE,W e n w uW a n g , Senior Member , IEEE,\nand Mark D. Plumbley , Fellow, IEEE\nAbstract\u2014Sound event detection (SED) is a task to detect sound\nevents in an audio recording. One challenge of the SED task is that\nmany datasets such as the Detection and Classi\ufb01cation of Acoustic\nScenes and Events (DCASE) datasets are weakly labelled. That is,\nthere are only audio tags for each audio clip without the onset and\noffset times of sound events. We compare segment-wise and clip-\nwise training for SED that is lacking in previous works. We propose\na convolutional neural network transformer (CNN-Transfomer)\nfor audio tagging and SED, and show that CNN-Transformer\nperforms similarly to a convolutional recurrent neural network\n(CRNN). Another challenge of SED is that thresholds are required\nfor detecting sound events. Previous works set thresholds empir-\nically, and are not an optimal approaches. To solve this problem,\nwe propose an automatic threshold optimization method. The \ufb01rst\nstage is to optimize the system with respect to metrics that do not\ndepend on thresholds, such as mean average precision (mAP). The\nsecond stage is to optimize the thresholds with respect to metrics\nthat depends on those thresholds. Our proposed automatic thresh-\nold optimization system achieves a state-of-the-art audio tagging\nF1 of 0.646, outperforming that without threshold optimization of\n0.629, and a sound event detection F1 of 0.584, outperforming that\nwithout threshold optimization of 0.564.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc579127-3253-49da-aeb3-062e737be11c": {"__data__": {"id_": "dc579127-3253-49da-aeb3-062e737be11c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4e31095-9605-4685-afdd-66b105bcdd09", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3117b754733d8102dfbb469d7f640c0853bdcd76806c7cdb39535cd574b9cc0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5461f41f-ac13-41b1-90d2-d5e64bc5e33c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5a5f8265a5cb68e6d432f4e4b77ec5964e82c5ce27a32416401b18daa7d35757", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118", "node_type": "1", "metadata": {}, "hash": "cd53ff70acbb8b17f7cfa55fac680b9732427d6664ff028da5749210522f4004", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The\nsecond stage is to optimize the thresholds with respect to metrics\nthat depends on those thresholds. Our proposed automatic thresh-\nold optimization system achieves a state-of-the-art audio tagging\nF1 of 0.646, outperforming that without threshold optimization of\n0.629, and a sound event detection F1 of 0.584, outperforming that\nwithout threshold optimization of 0.564.\nIndex Terms\u2014Sound event detection (SED), weakly labelled\ndata, automatic threshold optimization.\nI. I NTRODUCTION\nS\nOUND event detection (SED) is an important research topic\nwhich can be used in smart home, self-driving cars and\nsmart cities. For example, a SED system can detect an ambulance\nsiren even if the ambulance is far away. In this situation, it is\nManuscript received December 12, 2019; revised May 28, 2020; accepted\nJuly 14, 2020. Date of publication August 12, 2020; date of current version\nAugust 25, 2020. This work was supported in part by the EPSRC under Grant\nEP/N014111/1 \u201cMaking Sense of Sounds,\u201d in part by the Research Scholarship\nfrom the China Scholarship Council under Grant 201406150082, and in part\nby a studentship (Reference: 1976218) from the EPSRC Doctoral Training\nPartnership under Grant EP/N509772/1. The associate editor coordinating the\nreview of this manuscript and approving it for publication was Prof. Isabel\nBarbancho. (Corresponding author: Qiuqiang Kong.)\nQiuqiang Kong and Mark D. Plumbley are with the Centre for Vision, Speech\nand Signal Processing, University of Surrey, Guildford GU2 7XH, U.K.", "mimetype": "text/plain", "start_char_idx": 1358, "end_char_idx": 2876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118": {"__data__": {"id_": "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4e31095-9605-4685-afdd-66b105bcdd09", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3117b754733d8102dfbb469d7f640c0853bdcd76806c7cdb39535cd574b9cc0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc579127-3253-49da-aeb3-062e737be11c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "01035c48417e99f1f496d2aa210aaedbdae0c9ec2d51fb7aab45a6d183f1ad77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c063cdd-32a6-415f-8862-92d5a4c888c4", "node_type": "1", "metadata": {}, "hash": "6820d8c1e8080c11326db0966672bf7b0073c222a6dd807719bfb25348a0365a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The associate editor coordinating the\nreview of this manuscript and approving it for publication was Prof. Isabel\nBarbancho. (Corresponding author: Qiuqiang Kong.)\nQiuqiang Kong and Mark D. Plumbley are with the Centre for Vision, Speech\nand Signal Processing, University of Surrey, Guildford GU2 7XH, U.K. (e-mail:\nqiuqiangkong@gmail.com; m.plumbley@surrey.ac.uk).\nYong Xu is with the Tencent AI Lab, Bellevue, WA 98004 USA (e-mail:\nlucayongxu@tencent.com).\nWenwu Wang is with the Centre for Vision, Speech and Signal Processing,\nUniversity of Surrey, Guildford GU2 7XH, U.K., and also with the Qing-\ndao University of Science and Technology, Qingdao 266071, China (e-mail:\nw.wang@surrey.ac.uk).\nDigital Object Identi\ufb01er 10.1109/TASLP.2020.3014737\ndif\ufb01cult to detect the siren with cameras because of the distance\nand obstructions. Different from audio tagging (AT) which only\nrequires to detect the presence or absence of sound events in an\naudio recording, SED requires to predict the onsets and offsets\nof sound events. SED has attracted many researches since the\nintroduction of the Detection and Classi\ufb01cation of Acoustic\nScenes and Events (DCASE) challenges [1]\u2013[5].\nOne challenge of the SED task is that audio recordings are\nusually weakly labelled. That is, in the training data, we only\nknow the presence or absence of sound events, without knowing\ntheir onset and offset times. We call this kind of data weakly\nlabelled data (WLD).", "mimetype": "text/plain", "start_char_idx": 2570, "end_char_idx": 4012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c063cdd-32a6-415f-8862-92d5a4c888c4": {"__data__": {"id_": "9c063cdd-32a6-415f-8862-92d5a4c888c4", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4e31095-9605-4685-afdd-66b105bcdd09", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3117b754733d8102dfbb469d7f640c0853bdcd76806c7cdb39535cd574b9cc0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "b6f423f716a193c727cacd3730371747a06fcbd3fc188a0f46ca3f91b5c44b57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a7251a5-bb5f-45b9-9465-cb4a43938c08", "node_type": "1", "metadata": {}, "hash": "a846a629dc4dd18203c3c98d10423af416f1b63223efd9048dc0e02d0d4ee165", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One challenge of the SED task is that audio recordings are\nusually weakly labelled. That is, in the training data, we only\nknow the presence or absence of sound events, without knowing\ntheir onset and offset times. We call this kind of data weakly\nlabelled data (WLD). In this paper, we focus on the large-scale\nweakly supervised sound event detection task for smart cars\ndataset from the DCASE 2017 challenge Task 4 [6]. The audio\nrecordings from this task is a subset of the AudioSet dataset [7].\nThis task includes both AT and SED. All audio clips for training\nare weakly labelled without time information of sound events.\nIn previous research of AT, several CNN-based methods have\nbeen applied [8]\u2013[19]. Those approaches show that a robust\nfeature extractor is important for AT and SED. Usually CNNs are\napplied to the log mel spectrogram of audio recordings followed\nby a sigmoid non-linearity to predict the presence probabilities\nof sound events.\nGeneral SED tasks can be divided into two categories ac-\ncording to the availability of frame-level or clip-level labels:\nstrongly supervised SED, when frame-level labels are provided;\nor weakly supervised SED, when only clip-level tags are pro-\nvided. Several deep learning based methods [20]\u2013[27] have\nbeen proposed for the strongly supervised SED task. However,\nframe-level sound event labels are time consuming to obtain.\nRecently, the DCASE 2017 Task 4 provides a large-scale dataset\ndesigned for AT and SED with only weakly labelled data pro-\nvided. To train with weakly labelled data, segment-wise based\nmethods [10], [28] split audio clips into segments, and assign\neach segment with weak labels. On the other hand, clip-wise\ntraining methods [12] apply entire audio clips for training.", "mimetype": "text/plain", "start_char_idx": 3744, "end_char_idx": 5492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a7251a5-bb5f-45b9-9465-cb4a43938c08": {"__data__": {"id_": "7a7251a5-bb5f-45b9-9465-cb4a43938c08", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4e31095-9605-4685-afdd-66b105bcdd09", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3117b754733d8102dfbb469d7f640c0853bdcd76806c7cdb39535cd574b9cc0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c063cdd-32a6-415f-8862-92d5a4c888c4", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "48c1e3da4b1b26196f8ae36e8aeec008b0d2a74ba2f0833bc4ff53de31b9f995", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recently, the DCASE 2017 Task 4 provides a large-scale dataset\ndesigned for AT and SED with only weakly labelled data pro-\nvided. To train with weakly labelled data, segment-wise based\nmethods [10], [28] split audio clips into segments, and assign\neach segment with weak labels. On the other hand, clip-wise\ntraining methods [12] apply entire audio clips for training. There\nis a lack of research comparing the segment-wise and clip-wise\nbased methods for SED.\nAlthough previous CNNs based methods have been successful\nin audio tagging and SED tasks, CNNs do not capture the long\ntime dependency in an audio clip well. For example, the receptive\n\ufb01eld of a CNN can be limited to a short duration with a \ufb01xed\nlength that does not take long history information into account in\n2329-9290 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 5124, "end_char_idx": 6243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e170b4b-3014-4c0a-93be-f9654d7a30f4": {"__data__": {"id_": "0e170b4b-3014-4c0a-93be-f9654d7a30f4", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5ad9052d2acc1e4666f5b9de0b773734ba7f65dbfa8f76abfe1ced122706aa93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb848190-f685-4699-9262-6cbda309dca9", "node_type": "1", "metadata": {}, "hash": "8780155a9318cb268c96aea9b895409665d157c313b21e5a3be9a4ee330c844a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "KONG et al.: SOUND EVENT DETECTION OF WEAKLY LABELLED DATA 2451\nthe system. To solve this problem, convolutional recurrent neural\nnetworks (CRNNs) including [18], [29], [30] and bidirectional\nlong short term memory (BLSTM) systems [22] were used to\nconsider the long temporal information for audio tagging and\nsound event detection. One disadvantage of CRNNs is that the\nhidden states of a CRNN have to be calculated one by one, and\ncan not be calculated in parallel. Recently, transformers [31]\u2013\n[33] have been proposed to consider the long time dependency\nof sequences. A transformer consists of several attention layers.\nEach state of a layer takes the information from all states\nof the previous layer. Therefore, each state retains the global\ninformation of the input sequence.\nAnother challenging problem of AT and SED is the selection\nof thresholds for post-processing [27], [34]. For example, in\nthe AT subtask, if the predicted probability of a sound class\nis over a threshold in an audio clip, then the audio clip is\nregarded as containing this sound class. The thresholds selection\nis an important part of AT and SED. Usually, the thresholds\nare selected empirically. For example, in the winning system of\nthe AT subtask in the DCASE 2017 [12], thresholds of 0.3 are\nused for all the sound classes. However, those thresholds are\nselected by experience and may not be optimal. In this work, we\npropose an automatic threshold optimization method to solve\nthis problem.\nThis work contributes in the following aspects. First, we\ninvestigate segment-wise training and clip-wise training for AT\nand SED. We found that different systems perform differently\nfor the AT and SED subtask.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb848190-f685-4699-9262-6cbda309dca9": {"__data__": {"id_": "fb848190-f685-4699-9262-6cbda309dca9", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5ad9052d2acc1e4666f5b9de0b773734ba7f65dbfa8f76abfe1ced122706aa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e170b4b-3014-4c0a-93be-f9654d7a30f4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "41cfb289cf3803facabe677c9e506fcbb18a777aefc550cc0fa7e274065021db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae05de58-4ee5-46b5-ae54-af30c173d2ba", "node_type": "1", "metadata": {}, "hash": "35fc4bec9d8ccf3bf099005072884287a0c7053096cac39eb410c211137ff702", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, those thresholds are\nselected by experience and may not be optimal. In this work, we\npropose an automatic threshold optimization method to solve\nthis problem.\nThis work contributes in the following aspects. First, we\ninvestigate segment-wise training and clip-wise training for AT\nand SED. We found that different systems perform differently\nfor the AT and SED subtask. Second, we propose a CNN-\nTransformer system, and achieves competitive results to the\nCNN-GRU system. Third, we propose an automatic threshold\noptimization method for the AT and SED subtask. Our proposed\nsystems outperform the best systems in the DCASE 2017 Task\n4 challenge. This paper is organized as follows: Section II intro-\nduces CNN and CRNN for AT and SED. Section III introduces a\nCNN-Transformer system. Section IV introduces segment-wise\nand clip-wise training. Section V proposes an automatic thresh-\nold optimization method for AT and SED. Section VI shows\nexperimental results. Section VII concludes this work.\nII. C ONVOLUTIONAL NEURAL NETWORKS (CNNS)\nA. Conventional CNNs\nCNNs were originally designed for image classi\ufb01cation [35],\nand have been recently used for audio related tasks such as\nspeech recognition [36] and AT [37], [38]. A conventional\nCNN consists of convolution layers, pooling layers and fully\nconnected layers. The input to each convolutional layer is a\ntensor with a shape (N,C,W,H ) representing the number of\ninput samples, channels, width and height. For AT, the input\nwidth and height represent the number of time frames and\nfrequency bins. Each convolutional layer consists of a set of\nlearnable kernels. The output of a convolutional layer is a tensor\ncalled feature maps.", "mimetype": "text/plain", "start_char_idx": 1310, "end_char_idx": 3002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae05de58-4ee5-46b5-ae54-af30c173d2ba": {"__data__": {"id_": "ae05de58-4ee5-46b5-ae54-af30c173d2ba", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5ad9052d2acc1e4666f5b9de0b773734ba7f65dbfa8f76abfe1ced122706aa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb848190-f685-4699-9262-6cbda309dca9", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9e3f9b52442c0b7a780f0e0ea5f86374eaac9e9ce60e507cb4b1ecda1cd7924d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fe777df-c74e-4f6e-a56a-45f4ffedfc82", "node_type": "1", "metadata": {}, "hash": "0e4c64ac5b033b6f1fd7d6335b80bb8bbc5ed66db5fe4545f7764a9350e9a50d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The input to each convolutional layer is a\ntensor with a shape (N,C,W,H ) representing the number of\ninput samples, channels, width and height. For AT, the input\nwidth and height represent the number of time frames and\nfrequency bins. Each convolutional layer consists of a set of\nlearnable kernels. The output of a convolutional layer is a tensor\ncalled feature maps. The kernels in a convolutional layer can\nlearn local time-frequency patterns in the spectrogram of an\naudio clip. In audio processing, low level features [39] can be\nwaveforms or time-frequency representations such as spectro-\ngram. High level features are those extracted from low level\nfeatures by convolutional layers. Recent CNN architectures\napply batch normalization [40] after convolutional layers to\nspeed up and stabilise training. Nonlinear activation functions\nsuch as ReLU [41] are applied after each batch normalization.\nFor AT and SED, pooling layers are applied along both time\nand frequency axes. A time distributed fully connected layer is\napplied on the output of the last convolutional layer to predict\nthe presence probability of sound events along the time axis.\nThen the predicted probabilities are aggregated over the time\naxis to obtain the clip-wise sound event presence probability.\nThe aggregation can be, for example, maximum or average\noperations over the time axis.\nB. Convolutional Recurrent Neural Network (CRNNs)\nThe receptive \ufb01eld of CNNs have limited sizes. That is, CNNs\ncan not capture long time dependency in an audio clip. However,\nsome sound events have long time dependencies. For example,\nan ambulance siren may last for tens of seconds, and the temporal\ninformation is useful for AT and SED. Designing a system that is\nable to capture the temporal dependency is bene\ufb01cial for AT and\nSED.", "mimetype": "text/plain", "start_char_idx": 2634, "end_char_idx": 4433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fe777df-c74e-4f6e-a56a-45f4ffedfc82": {"__data__": {"id_": "2fe777df-c74e-4f6e-a56a-45f4ffedfc82", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5ad9052d2acc1e4666f5b9de0b773734ba7f65dbfa8f76abfe1ced122706aa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae05de58-4ee5-46b5-ae54-af30c173d2ba", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "e2ff3b8161da0155046b4da2e306ed1d2b40dc537764edcc667b55e72ab9eeb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "114af520-8d01-4b74-b314-ffb9b9e29353", "node_type": "1", "metadata": {}, "hash": "c94718e4214eefafb802ef179e2ea0230eda13d5fa08bab0ed0d51b9a4a10367", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That is, CNNs\ncan not capture long time dependency in an audio clip. However,\nsome sound events have long time dependencies. For example,\nan ambulance siren may last for tens of seconds, and the temporal\ninformation is useful for AT and SED. Designing a system that is\nable to capture the temporal dependency is bene\ufb01cial for AT and\nSED. Recurrent neural networks (RNNs) [42] are kinds of neural\nnetworks that can store history information in their hidden states,\nand thus capture long term dependency of sequential data. RNNs\nhave been applied to language processing tasks such as [42]. The\npotential problem of a conventional RNN is that the gradient\nof weights may vanish or explode in training. Long short term\nmemory (LSTM) [43] is a variation of RNN that introduces\nconstant error carousel units, input gate, output gate and forget\ngate to avoid the gradient exploding and vanishing problem.\nAn improved architecture of LSTM called gated recurrent units\n(GRU) [44] is proposed to reduce the parameters of LSTMs and\nsimplify the gates to a reset gate and a forget gate. A GRU can\nbe in both directions which we call bidirectional GRU (biGRU),\nwhich is applied in our AT and SED systems.\nIII. CNN-T RANSFORMER\nCRNN can capture long time-dependency of sound events.\nOn the other hand, the sequential nature of CRNNs also makes\nit more dif\ufb01cult to take advantage of modern fast computing\ndevices such as GPUs. Recently, transformer [31] is proposed\nto learn correlations of time steps in a sequence such as natural\nlanguage processing tasks [32]. Compared with RNNs which\nrequire to compute the hidden states in a sequence, a trans-\nformer can parallelize the computation which only requires\nmatrix multiplication in the forward pass.", "mimetype": "text/plain", "start_char_idx": 4096, "end_char_idx": 5832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "114af520-8d01-4b74-b314-ffb9b9e29353": {"__data__": {"id_": "114af520-8d01-4b74-b314-ffb9b9e29353", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5ad9052d2acc1e4666f5b9de0b773734ba7f65dbfa8f76abfe1ced122706aa93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fe777df-c74e-4f6e-a56a-45f4ffedfc82", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "cacf760ecba69006767bcc5124f942ff7bdd44e4a9b00a0e941e7cb5358f532f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recently, transformer [31] is proposed\nto learn correlations of time steps in a sequence such as natural\nlanguage processing tasks [32]. Compared with RNNs which\nrequire to compute the hidden states in a sequence, a trans-\nformer can parallelize the computation which only requires\nmatrix multiplication in the forward pass. Transformer applies\na self-attention mechanism which directly models relationships\nbetween all time steps in a sequence. In an audio clip, a sound\nclass may contain several sound events over time. For example,\nthe speech of a human may appear in any time in an audio clip.\nA transformer can capture the correlation of speeches appearing\nin different part of an audio clip.\nA. Transformer\nTransformer was originally proposed in [31]. The motivation\nfor the design of the transformer is to allow modeling of depen-\ndencies without regard to their distance in the input sequence. In\naddition, a transformer allows for more parallel computing than\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 5508, "end_char_idx": 6635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a29f4449-2334-42ba-9faa-f4afaa7a408c": {"__data__": {"id_": "a29f4449-2334-42ba-9faa-f4afaa7a408c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ce873490ff4ffcdf0e378a3d3342599dbcbfed1f4f2048beab8e14604990d314", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd6a53d3-a669-4546-9f1a-6b75bea6b480", "node_type": "1", "metadata": {}, "hash": "07cd59e4130b5c7abee98d34c9992a9da1de2835634762d6d38a454ef21c9b59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2452 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\nRNNs by removing the recurrent connections. A transformer\nconsists of several encoder and decoder layers. The encoder\ntransforms an input to a high level embedding, and the decoder\ntransforms an embedding to output. In a classi\ufb01cation task such\nas AT or SED, we only need the encoder. Each encoder consists\nof several encoder layers. For each encoder layer, we denote the\ninput to the encoder layer as a tensor x with a shape of T \u00d7C,\nwhere T and C represent the number of time steps and channels.\nWe follow the symbols used in [31]. An encoder layer consists\nof a query transform matrix WQ, a key transform matrix WK\nand a value transform matrix WV . The matrices WQ and WK\nhave a shape of C \u00d7dk, and WV has a shape of C \u00d7dv where\ndk and dv are integers. Then the query Q,k e y K and value V\ncan be obtained by:\nQ = xWQ\nK = xWK\nV = xWV .\n(1)\nThe query Q and key K have a shape of T \u00d7dk, and the value\nV has a shape of T \u00d7dv. The output of an encoder layer can be\nwritten as:\nh = softmax\n(QKT\n\u221adk\n)\nV, (2)\nwhere the output hhas a shape of T \u00d7H. Equation (2) computes\nthe dot product of the query with all keys, divide each by \u221adk,\nand apply a softmax function to obtain the weights on the values\nV [31]. The division of square root of dk is a normalization\nterm [31]. In (2), the inner product of Q and KT has a shape\nof T \u00d7T, representing the feature correlation of different time\nsteps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd6a53d3-a669-4546-9f1a-6b75bea6b480": {"__data__": {"id_": "cd6a53d3-a669-4546-9f1a-6b75bea6b480", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ce873490ff4ffcdf0e378a3d3342599dbcbfed1f4f2048beab8e14604990d314", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a29f4449-2334-42ba-9faa-f4afaa7a408c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "a9513ae3d8403e76285f97d0d78091f8d3e94f68bf97a1b40259c49f0b706fa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03418609-31dd-4a95-bd2e-b6ebb7169b7c", "node_type": "1", "metadata": {}, "hash": "61d7ad9d02547c01e5b7a277e2e9d28a7c3522e5421b7cc344fbb11aa129d9bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Equation (2) computes\nthe dot product of the query with all keys, divide each by \u221adk,\nand apply a softmax function to obtain the weights on the values\nV [31]. The division of square root of dk is a normalization\nterm [31]. In (2), the inner product of Q and KT has a shape\nof T \u00d7T, representing the feature correlation of different time\nsteps. The softmax operation converts the correlation value to\nprobabilities along the time steps indicating how much the value\nV in a time step should be attended.\nB. CNN-Transformer\nFor audio tagging and SED, the input is usually a time-\nfrequency representation such as a log mel spectrogram. Log mel\nspectrogram is a low level feature and CNNs have been proposed\nto apply on the log mel spectrogram to extract high level fea-\ntures [37]. To build the CNN-Transformer system, we \ufb01rst apply\na CNN described in Section II on the log mel spectrogram of an\naudio clip. Convolutional layers in the CNN are used to extract\nhigh level features of the input log mel spectrogram. We use the\nfeature maps of the last convolutional layer to obtain embedding\nvectors along time axis. The embedding can be viewed as xwith\na shape of the number of time frames by the number of channels.\nThe output of the transformer has a shape of T \u00d7dv. A fully\nconnected layer followed by a sigmoid non-linearity is applied\non this output to predict the presence probabilities of sound\nclasses over time steps. An aggregation function such as average\naggregation can be applied to average out those probabilities\nalong time domain to obtain the audio tagging result.\nIV . S EGMENT-WISE V. S . CLIP-WISE SED\nSections II and III introduce CNN, CNN-biGRU and CNN-\nTransformer architectures.", "mimetype": "text/plain", "start_char_idx": 1129, "end_char_idx": 2828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03418609-31dd-4a95-bd2e-b6ebb7169b7c": {"__data__": {"id_": "03418609-31dd-4a95-bd2e-b6ebb7169b7c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ce873490ff4ffcdf0e378a3d3342599dbcbfed1f4f2048beab8e14604990d314", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd6a53d3-a669-4546-9f1a-6b75bea6b480", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "4f4a6a9e066fc619a4c1a69af433a927527cb881c3cb2d7fd82e5193f1b3b401", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bc45822-d595-4fa4-91ee-9d600f6e9c92", "node_type": "1", "metadata": {}, "hash": "e00f382c9014a546d8c156789ded13cd7307f19a79f40c3fe1a01bce247e344a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A fully\nconnected layer followed by a sigmoid non-linearity is applied\non this output to predict the presence probabilities of sound\nclasses over time steps. An aggregation function such as average\naggregation can be applied to average out those probabilities\nalong time domain to obtain the audio tagging result.\nIV . S EGMENT-WISE V. S . CLIP-WISE SED\nSections II and III introduce CNN, CNN-biGRU and CNN-\nTransformer architectures. In this section, we introduce how\nwe apply the aforementioned architectures for AT and SED\ntraining with weakly labelled data. Conventional SED methods\nutilise strongly labelled data for supervised learning. However,\ncollecting strongly labelled data is time consuming. The amount\nof strongly labelled data is therefore limited. To solve this\nproblem, we propose to use weakly labelled dataset for SED. The\nSED systems with weakly labelled data can be categorized into\nsegment-wise training [10], [16], and our previously proposed\nclip-wise training [12] methods. This section aims to investigate\nthe comparison of the segment-wise and clip-wise training for\nAT and SED.\nA. Segment-Wise Training\nWe denote the waveform of an audio clip as X. For an X\nlasting for several seconds, we split it into several segments\n{xm}M\nm=1 where M is the number of segments. Each segment\ninherits the tags of the audio clip. We denote the tags of each\nsegment as y \u2208{ 0,1}K where Kis the number of sound classes.\nThe SED problem is converted to an audio tagging problem\non those segments. In training, a classi\ufb01er f is trained on the\nsegments.", "mimetype": "text/plain", "start_char_idx": 2394, "end_char_idx": 3956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bc45822-d595-4fa4-91ee-9d600f6e9c92": {"__data__": {"id_": "3bc45822-d595-4fa4-91ee-9d600f6e9c92", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ce873490ff4ffcdf0e378a3d3342599dbcbfed1f4f2048beab8e14604990d314", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03418609-31dd-4a95-bd2e-b6ebb7169b7c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "b057624955aac639ac1c1e11fb54427775d249b86254195ec7948193bb44b8fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a00ccb-5e42-4139-b870-bcb41596224c", "node_type": "1", "metadata": {}, "hash": "0ae8ee66cf0f99377596af719d9179faac2ecedd7ebfd2523509b2e449768215", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each segment\ninherits the tags of the audio clip. We denote the tags of each\nsegment as y \u2208{ 0,1}K where Kis the number of sound classes.\nThe SED problem is converted to an audio tagging problem\non those segments. In training, a classi\ufb01er f is trained on the\nsegments. The loss function can be written as:\nE = \u2212\nM\u2211\nm=1\nK\u2211\nk=1\n[yklogf(xm)k +(1 \u2212yk)log(1\u2212f(xm)k)].\n(3)\nIn inference, an audio clip is split into segments {xm}M\nm=1, and\nthe SED result on each segment can be calculated by f(xm).\nThe AT result can be obtained by aggregating f(xm) over all\nsegments:\nF(X)= agg({f(xm)}M\nm=1). (4)\nThe aggregation can be, for example, maximum or average\noperation over all the segments. The segment-wise classi\ufb01er f\ncan be CNN, CNN-biGRU or CNN-Transformer followed by\na sigmoid non-linearity to predict the presence probability of\nsound events of each segment. We investigate the performance\nof choosing different duration of segments on SED and AT in\nSection VI.\nB. Clip-Wise Training\nIn the segment-wise training, all segments xm inherit the tags\nof an audio clip X. The problem of segment-wise training is\nthat many sound events may only last for a short time in the\naudio clip. Therefore, the tags of xm may be incorrect because\nthe segment may not contain the sound event. To solve this\nproblem, our previous work proposed attention neural network\nbased clip-wise training [12]. The clip-wise training method\ndoes not explicitly assign tags for each segment xm.", "mimetype": "text/plain", "start_char_idx": 3688, "end_char_idx": 5148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2a00ccb-5e42-4139-b870-bcb41596224c": {"__data__": {"id_": "e2a00ccb-5e42-4139-b870-bcb41596224c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ce873490ff4ffcdf0e378a3d3342599dbcbfed1f4f2048beab8e14604990d314", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bc45822-d595-4fa4-91ee-9d600f6e9c92", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "d7e3fa25e64570ca17503773cc30cb926cb3295c40d1fe969e2446c1b2112c41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the tags of xm may be incorrect because\nthe segment may not contain the sound event. To solve this\nproblem, our previous work proposed attention neural network\nbased clip-wise training [12]. The clip-wise training method\ndoes not explicitly assign tags for each segment xm. Instead,\nthe systems are designed to learn the tags of xm implicitly, that\nis, from the hidden layer of a neural network. We denote the\nsegment-wise prediction of a segment xm to be f(xm). Then\nthe prediction on the clip X can be obtained by aggregating the\nsegment-wise predictions. For example, the aggregation can be\na max, average or attention function over the prediction of all\nsegments of each sound class. The max function can be de\ufb01ned\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4864, "end_char_idx": 5752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f05dd48b-f840-437a-a67f-52a7a83d28a6": {"__data__": {"id_": "f05dd48b-f840-437a-a67f-52a7a83d28a6", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "c95439eb175125097bc54720d3876c85e0b3e288166440595c5d4a34e4902b7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f19c4011-fad2-497a-9c01-a65ad2344ee2", "node_type": "1", "metadata": {}, "hash": "cca831e028456bd18d1075972e097fdd75d1cfd56c64acfdac71dc564c87a4ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "KONG et al.: SOUND EVENT DETECTION OF WEAKLY LABELLED DATA 2453\nAlgorithm 1: Audio Tagging.\n1: Inputs: predicted presence probability of sound events\nin an audio clip F(X). AT thresholds {\u03bc1,...,\u03bc K}.\n2: Outputs: Predicted audio tags.\n3: for k =1 ,...,K do\n4: if F(X)k <\u03bc k then\n5: return 0 for the k-th sound event.\n6: else\n7: return 1 for the k-th sound event.\nas:\nF(X)k = max\nk\nf(xm)k. (5)\nFor example, the average function can be de\ufb01ned as:\nF(X)k =\nM\u2211\nm=1\nf(xm)k. (6)\nThe decision-level function can be de\ufb01ned as [45]:\nF(X)k =\nM\u2211\nm=1\nf(xm)kp(xm)k, (7)\nwhere p(xm)= exp(w(xm)k)\u2211M\nj=1 exp(w(xj)k), and w(\u00b7) is a linear transfor-\nmation. In training, we calculate the categorical binary crossen-\ntropy loss between the clip-level prediction F(X) and the\nground truth label of X:\nE = \u2212\nK\u2211\nk=1\n[yklogF(X)k +(1 \u2212yk)log(1\u2212F(X)k)]. (8)\nThe difference between the clip-wise training (8) and the\nsegment-wise training (3) is that the clip-wise training directly\noutputs F(X), and can be trained in an end-to-end way with\nweakly labelled data. The f(xm)k are latent representations\nlearnt by the neural network.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f19c4011-fad2-497a-9c01-a65ad2344ee2": {"__data__": {"id_": "f19c4011-fad2-497a-9c01-a65ad2344ee2", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "c95439eb175125097bc54720d3876c85e0b3e288166440595c5d4a34e4902b7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f05dd48b-f840-437a-a67f-52a7a83d28a6", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "92a0fc700c26b295f58f44b1ca1a6e8a1de4179a8f2dabb9f7e68a4fdafb7b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af9a9926-097e-4c2f-956a-b9e1a9a5dc14", "node_type": "1", "metadata": {}, "hash": "6350d7eba315e453c956ab11f74a04fa5fd87281cd3b54d210646740a2bc219a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(8)\nThe difference between the clip-wise training (8) and the\nsegment-wise training (3) is that the clip-wise training directly\noutputs F(X), and can be trained in an end-to-end way with\nweakly labelled data. The f(xm)k are latent representations\nlearnt by the neural network.\nV. A UTOMATIC THRESHOLD OPTIMIZATION\nTo obtain the presence or absence of sound events in an\naudio clip, AT systems need to apply thresholds to the system\noutputs. A sound class is predicted as presence if the AT output\nis larger than its corresponding threshold. The thresholds for\nAT are denoted as \u0398AT = {\u03bc1,...,\u03bc K}. Algorithm 1 shows the\nalgorithm to obtain AT result from the AT system outputs.\nSED requires to predict not only the presence or absence but\nalso the onset and offset times of sound events. Similar to AT, we\n\ufb01rst apply thresholds {\u03bc1,...,\u03bc K}on F(X)to predict the pres-\nence or absence of K classes of sound events in an audio clip X.\nIf the k-th sound class is predicted to be present, then we apply a\nthreshold \u03c4high\nk to the segment-wise predictions f(xm)to detect\nsound events. In addition, to reduce the number of missed detec-\ntion, a second threshold \u03c4low\nk is used. To begin with, we denote\nthe neighbouring segments of an active segment as x\u2032. Then, a\nlower threshold \u03c4low\nk is applied on f(xm)to obtain the calibrated\nsound event detection result. All thresholds for SED are denoted\nAlgorithm 2: Sound Event Detection.", "mimetype": "text/plain", "start_char_idx": 828, "end_char_idx": 2254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af9a9926-097e-4c2f-956a-b9e1a9a5dc14": {"__data__": {"id_": "af9a9926-097e-4c2f-956a-b9e1a9a5dc14", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "c95439eb175125097bc54720d3876c85e0b3e288166440595c5d4a34e4902b7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f19c4011-fad2-497a-9c01-a65ad2344ee2", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "27a80b78abdb33a1c010e4cfc9a1d3904b171950839ec925ca8fa02f2eda2fc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c93ee1c4-2a46-4deb-b086-dd53e1688198", "node_type": "1", "metadata": {}, "hash": "cb78893b59b4e08d2100664accb013ef48267c04bf3c532024a662639c136e6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, to reduce the number of missed detec-\ntion, a second threshold \u03c4low\nk is used. To begin with, we denote\nthe neighbouring segments of an active segment as x\u2032. Then, a\nlower threshold \u03c4low\nk is applied on f(xm)to obtain the calibrated\nsound event detection result. All thresholds for SED are denoted\nAlgorithm 2: Sound Event Detection.\n1: Inputs: clip-wise prediction F(X), segment-wise\nprediction f(xm), AT thresholds {\u03bc1,...,\u03bc K},S E D\nhigh thresholds {\u03c4high\n1 ,...,\u03c4 high\nK }, SED low thresholds\n{\u03c4low\n1 ,...,\u03c4 low\nK }.\n2: Outputs: Detected sound events.\n3: for k =1 ,...,K do\n4: if F(X)k <\u03bc k then\n5: return 0 for the k-th sound event.\n6: else\n7: for m =1 ,...,M do\n8: if f(xm)k >\u03c4 high\nk then\n9: Return 1 for the neighbouring\nsegments x\u2032of xm if f(x\u2032)k <\u03c4 low\nk .\nas \u0398SED = {\u03bc1,...,\u03bc K,\u03c4 high\n1 ,...,\u03c4 high\nK ,\u03c4 low\n1 ,...,\u03c4 low\nK }. Algo-\nrithm 2 summarizes obtaining the SED results from the clip-wise\nand segment-wise predictions.\nThe winning system of the AT subtask in DCASE 2017 Task\n4 [12] applies constant thresholds for both the AT and SED\nsubtask. Setting those thresholds requires a lot of experience, and\nthe manually selected thresholds are often not optimal. In addi-\ntion, each sound class may have different thresholds. Therefore,\nsweeping over all combinations of thresholds is intractable.", "mimetype": "text/plain", "start_char_idx": 1908, "end_char_idx": 3231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c93ee1c4-2a46-4deb-b086-dd53e1688198": {"__data__": {"id_": "c93ee1c4-2a46-4deb-b086-dd53e1688198", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "c95439eb175125097bc54720d3876c85e0b3e288166440595c5d4a34e4902b7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af9a9926-097e-4c2f-956a-b9e1a9a5dc14", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "80ffccd4ab888bd80c571b65483c9e4d273d8c66d484b5939fdaf3bab2e3e0bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39ed0dbb-b52d-4367-900a-ecb2830721b6", "node_type": "1", "metadata": {}, "hash": "94ce45f611bc29b8e690b8fe7297f261988fc00138eee0857b28b28135758a54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The winning system of the AT subtask in DCASE 2017 Task\n4 [12] applies constant thresholds for both the AT and SED\nsubtask. Setting those thresholds requires a lot of experience, and\nthe manually selected thresholds are often not optimal. In addi-\ntion, each sound class may have different thresholds. Therefore,\nsweeping over all combinations of thresholds is intractable. We\npropose an automatic threshold optimization method to solve\nthis problem. In the \ufb01rst stage, we optimize the systems and\nevaluate the systems based on the metrics that do not depend\non the thresholds such as mean average precision (mAP). In the\nsecond stage, for a trained system, we optimize the thresholds\nover a speci\ufb01c metric such as F1 score or error rate (ER) to\noptimize the thresholds.\nFor an audio clip X, the AT result rAT can be obtained\nby alg AT(F(X),\u0398AT) where alg AT is the AT algorithm shown\nin Algorithm 1. The SED result rSED can be obtained by\nalgSED(F(X),{f(xm)}M\nm=1,\u0398SED) where alg SED is the SED\nalgorithm shown in Algorithm 2. The goal of AT or SED\nis to minimize some loss J(\u0398), for example, ER JER(\u0398) or\nnegative F1 score JF1(\u0398). The reason of using negative F1 is\nthat minimizing JF1(\u0398) is equivalent to maximizing F1 score.\nThe optimization of thresholds becomes solving the following\nproblem:\n\u02c6\u0398= argmin\n\u0398\nJ(\u0398). (9)\nThe dif\ufb01culty of solving (9) is that \u0398 consists of several pa-\nrameters to be optimized. So applying grid search over all\nthresholds is inef\ufb01cient.", "mimetype": "text/plain", "start_char_idx": 2858, "end_char_idx": 4327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39ed0dbb-b52d-4367-900a-ecb2830721b6": {"__data__": {"id_": "39ed0dbb-b52d-4367-900a-ecb2830721b6", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "c95439eb175125097bc54720d3876c85e0b3e288166440595c5d4a34e4902b7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c93ee1c4-2a46-4deb-b086-dd53e1688198", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "e72a7efa441ac22bc16159ea4311301a45626f8511baac3e50751c3595665ede", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The optimization of thresholds becomes solving the following\nproblem:\n\u02c6\u0398= argmin\n\u0398\nJ(\u0398). (9)\nThe dif\ufb01culty of solving (9) is that \u0398 consists of several pa-\nrameters to be optimized. So applying grid search over all\nthresholds is inef\ufb01cient. Another way is to use gradient based\nmethods to iteratively optimize those thresholds. However, equa-\ntion (9) is a non-differentiable function, so we can not calculate\nthe gradient over the thresholds in an analytical way. This\nis because both the AT and SED algorithms in Algorithm 1\nand Algorithm 2 contain non-differentiable operations such as\nthresholding. In addition, the evaluation metrics ER and F1 score\nare also non-differentiable. To solve this problem, we propose\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4087, "end_char_idx": 4963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e8a4bfa-0d20-49cd-899c-013297293e15": {"__data__": {"id_": "7e8a4bfa-0d20-49cd-899c-013297293e15", "embedding": null, "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "67cf9b215be06e2f5ed8ee1e51b1b1536bbfc1db1ab5d385417093c44fa321b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff", "node_type": "1", "metadata": {}, "hash": "d7aa531893c4b0bf059d365a4f3ef87a8ea6cf9a9af44e069581d6d08d84a180", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2454 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\nAlgorithm 3: Adam Optimization. Symbol g2\nt Indicates the\nElementwise Square gt \u2299 gt. Learning Rate is Denoted as\n\u03b1. Hyper-Parameters are Set to \u03b21 =0 .9, \u03b22 =0 .999 and\n\u03f5=1 0\u22128 Following [46].\n1: Inputs: parameters \u0398.\n2: Init \u03980,m0 =0 ,v0 =0 ,t =0\n3: while \u0398 not converged do\n4: t \u2190 t+1\n5: gt = \u25bd \u0398J\n6: mt \u2190 \u03b21mt\u22121 +(1 \u2212\u03b21)gt\n7: vt \u2190 \u03b22vt\u22121 +(1 \u2212\u03b22)g2\nt\n8: \u02c6mt \u2190 mt/(1\u2212\u03b2t\n1)\n9: \u02c6vt \u2190 vt/(1\u2212\u03b2t\n2)\n10: \u0398t \u2190 \u0398t\u22121 \u2212\u03b1\u00b7 \u02c6mt/(\u221a\u02c6vt +\u03f5)\nAlgorithm 4: Automatic Thresholds Optimization.\n1: Inputs: Validation dataset D = {X(n),y(n)}N\nn=1,\ntrained AT system F(\u00b7), trained SED system f(\u00b7).\n2: Outputs: Optimized thresholds \u0398.\n3: Initialize \u0398.\n4: for i =1 ,..., ITER do\n5: for n =1 ,...,N do\n6: \u02c6y(n) = alg(F(X(n)),f (x(n)\nm ),\u0398).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff": {"__data__": {"id_": "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff", "embedding": null, "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "67cf9b215be06e2f5ed8ee1e51b1b1536bbfc1db1ab5d385417093c44fa321b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e8a4bfa-0d20-49cd-899c-013297293e15", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ade11c41833eb0c6cb85eb413c9edc281a10a8a8fd0c9cb9ade4759a93b45cd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "123ceb7d-069e-4260-a6cf-829b27c2e414", "node_type": "1", "metadata": {}, "hash": "c594e8c6e90aeb15a30506a27b0bb099ccb93796a3896a606dcac5e6e1c5fb4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1: Inputs: Validation dataset D = {X(n),y(n)}N\nn=1,\ntrained AT system F(\u00b7), trained SED system f(\u00b7).\n2: Outputs: Optimized thresholds \u0398.\n3: Initialize \u0398.\n4: for i =1 ,..., ITER do\n5: for n =1 ,...,N do\n6: \u02c6y(n) = alg(F(X(n)),f (x(n)\nm ),\u0398).\n7: J = metric({\u02c6y(n)}n=N\nn=1 ,{y(n)}n=N\nn=1 )\n8: for \u03b8 in \u0398 do\n9: \u25bd \u03b8J = J(\u0398+\u25b3 \u0398)\u2212J(\u03b8)\n\u25b3 \u03b8\n10: \u25bd \u0398J = {\u25bd \u03b8J}\u03b8\u2208\u0398\n11: \u0398 \u2190 opt(\u0398,\u25bd \u0398J)\nto calculate the gradients over the thresholds in a numerical\nway. That is, for each parameter \u03b8, we calculate the gradient\nas:\n\u25bd \u03b8J(\u0398) =J(\u0398+ \u25b3 \u0398)\u2212J(\u0398)\n\u25b3 \u03b8 , (10)\nwhere \u25b3 \u03b8 is a small constant number, and \u25b3 \u0398 is a vector\nwith all zero values the position of \u03b8 which has a value of\n\u25b3 \u03b8. After calculating the numerical gradient for all parameters\n\u25bd \u0398J = {\u25bd \u03b8J}\u03b8\u2208\u0398, the optimized thresholds can be obtained\nby applying gradient based optimization methods iteratively:\n\u0398 \u2190 opt(\u0398,\u25bd \u0398J), where opt denotes an optimization algo-\nrithm such as gradient descent (GD). GD optimization can be\nwritten by \u0398 \u2190 \u0398\u2212\u03b1\u25bd \u0398 J where \u03b1 is a learning rate.", "mimetype": "text/plain", "start_char_idx": 561, "end_char_idx": 1568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "123ceb7d-069e-4260-a6cf-829b27c2e414": {"__data__": {"id_": "123ceb7d-069e-4260-a6cf-829b27c2e414", "embedding": null, "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "67cf9b215be06e2f5ed8ee1e51b1b1536bbfc1db1ab5d385417093c44fa321b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "7728cef3676aa259d1b7c04539e0c2f47c739c1f2aaf917be7645afcc96629b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98ba43b0-c79e-458a-8173-92f853e2f865", "node_type": "1", "metadata": {}, "hash": "4832f1290c18cbbdb46691bc736dfd3a81e89273fed8c0fed02d8509c7f344dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GD optimization can be\nwritten by \u0398 \u2190 \u0398\u2212\u03b1\u25bd \u0398 J where \u03b1 is a learning rate. We\nuse Adam optimizer [46] to optimize J(\u0398) due to its fast con-\nvergence. We describe Adam optimizer in Algorithm 3 to show\nhow it is used in our method. Overall, the automatic threshold\noptimization algorithm is described in Algorithm 4. We have\nreleased our proposed automatic threshold optimization toolbox\ncalled autoth.1\n1[Online]. Available: https://github.com/qiuqiangkong/autoth\nTABLE I\nSOUND EVENTS IN THE DCASE 2017 T ASK 4\u201c LARGE-SCALE WEAKLY\nSUPERVISED SOUND EVENT DETECTION FOR SMART CARS\u201d\nVI. E XPERIMENTS\nA. Experimental Setup\nThere are several SED datasets including the DCASE 2017\nTask 4 [6], the DCASE 2018 Task 4 [47] and the DCASE 2019\nTask 4 [48]. We evaluate our SED system on the DCASE 2017\nTask 4 \u201clarge-scale weakly supervised sound event detection\nfor smart cars\u201d. The reason we choose this dataset is because\nit is a large-scale dataset containing over 140 hours of weakly\nlabelled audio clips for training. The audio recordings of the\nDCASE 2017 Task 4 are from a subset of AudioSet [7] where\neach audio clip is extracted from YouTube video. DCASE 2017\nTask 4 consists of 17 sound events divided into two categories:\n\u201cwarning\u201d and \u201cvehicle\u201d. Most of those audio clips have duration\nof 10 seconds. The audio clips shorter than 10 seconds are\npadded with silence to 10 seconds. Table I lists the sound events\nand their statistics.", "mimetype": "text/plain", "start_char_idx": 1494, "end_char_idx": 2926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98ba43b0-c79e-458a-8173-92f853e2f865": {"__data__": {"id_": "98ba43b0-c79e-458a-8173-92f853e2f865", "embedding": null, "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "67cf9b215be06e2f5ed8ee1e51b1b1536bbfc1db1ab5d385417093c44fa321b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "123ceb7d-069e-4260-a6cf-829b27c2e414", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "cfd397736b37a0cc8e9ac75a7243140c3a73892fd06568566a890e572acb43a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DCASE 2017\nTask 4 consists of 17 sound events divided into two categories:\n\u201cwarning\u201d and \u201cvehicle\u201d. Most of those audio clips have duration\nof 10 seconds. The audio clips shorter than 10 seconds are\npadded with silence to 10 seconds. Table I lists the sound events\nand their statistics. The DCASE 2017 Task 4 dataset consists of\na training subset with 51172 audio clips, a validation subset with\n488 audio clips, and an evaluation set with 1103 audio clips. The\ntraining subset is weakly labelled. The validation and evaluation\nsubsets are both weakly and strongly labelled for evaluation. The\nsource code of this work is released. 2\nB. Feature\nWe use log mel spectrogram as input feature following pre-\nvious work on audio tagging [10], [37], [49]. To begin with,\nall audio clips are converted to monophonic and resampled\nto 32 kHz. The short time Fourier transform with a Hanning\nwindow of 1024 samples and a hop size of 320 samples is used\nto extract spectrogram which leads to 100 frames in a second.\nWe apply 64 mel \ufb01lter banks on the spectrogram followed by\nlogarithmic operation to calculate log mel spectrogram. The\nnumber 64 is chosen so that it can be evenly divided by a power\nof 2 in the down-sampling layers of CNNs. The mel \ufb01lter banks\n2[Online]. Available: https://github.com/qiuqiangkong/sound_event_\ndetection_dcase2017_task4\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 2640, "end_char_idx": 4141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9581539-9c27-4b14-b87a-b1f9453b595e": {"__data__": {"id_": "b9581539-9c27-4b14-b87a-b1f9453b595e", "embedding": null, "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "0afe29e3e966d50b421c8b37b6360cf439e83a2e462a1676c329243388a2d1db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83", "node_type": "1", "metadata": {}, "hash": "80d798d818574475235fb3a8c0a37aa714b69bde2fc2bf009ab16dd494db430a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "KONG et al.: SOUND EVENT DETECTION OF WEAKLY LABELLED DATA 2455\nTABLE II\nCNN ARCHITECTURE\nhave a lower cut-off frequency of 50 Hz and a higher cut-off\nfrequency of 14 kHz to avoid aliasing caused by resampling.\nC. Model\nThe segment-wise training systems are described in equation\n(4), and are modeled by a 9-layer CNN which has shown to per-\nform well on a variety of audio tagging tasks [49]. Table II shows\nthat the 9-layer CNN consists of 4 convolutional blocks, where\neach convolutional block consists of 2 convolutional layers\nwith kernel sizes of 3\u00d73. Batch normalization [40] and ReLU\nnon-linearity [50] is applied after each convolutional layer. The\nconvolutional block consists of 64, 128, 256 and 512 feature\nmaps, respectively. A 2\u00d72average pooling is applied after each\nconvolutional block to extract high level features. We did not\napply residual connections in our CNNs as gradient vanishing\nis not a problem with 8 convolutional layers. In Table II, the\nnumber following @ represents the number of feature maps.\nThe second column shows the number of batch size (bs), feature\nmaps, frames and frequency bins. We average out the frequency\naxis of the output from the last convolutional layer. Then time\ndistributed fully connected layer with sigmoid non-linearity is\napplied to predict the presence probability of sound events of\neach time frame. To obtain the AT result for supervised learn-\ning, aggregation functions including max, average and attention\nalong time frames are applied. Adam [46] optimizer with a\nlearning rate of 0.001 is applied, and is reduced to 0.0001 after\nthe performance is plateaued on validation data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83": {"__data__": {"id_": "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83", "embedding": null, "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "0afe29e3e966d50b421c8b37b6360cf439e83a2e462a1676c329243388a2d1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9581539-9c27-4b14-b87a-b1f9453b595e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "90c8518c472735c90902011da3c20ba42bec45e3c3a0b0a87ff0b0c9caddc9c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb03027e-2cb1-4acf-acfa-bd091b17925a", "node_type": "1", "metadata": {}, "hash": "ab3e3ced4d4db1c374bd685436b9c1a3f95fa2f34e482add69b54075b00715e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then time\ndistributed fully connected layer with sigmoid non-linearity is\napplied to predict the presence probability of sound events of\neach time frame. To obtain the AT result for supervised learn-\ning, aggregation functions including max, average and attention\nalong time frames are applied. Adam [46] optimizer with a\nlearning rate of 0.001 is applied, and is reduced to 0.0001 after\nthe performance is plateaued on validation data. Mixup [51]\nwith alpha of 1.0 is used in all experiments to prevent training\nfrom over\ufb01tting. The training is stopped at 60,000 iterations by\nobserving the performance on the validation set.\nD. Evaluation Metrics\nTo evaluate the systems performance, we use the precision,\nrecall and F1 score which are described in [52]:\nP = TP\nTP +FP (11)\nR = TP\nTP +FN (12)\nF1 = 2P \u00b7R\nP +R, (13)\nwhere TP, FP, FN are the number of true positive, false positive\nand false negative samples, respectively. The higher precision,\nrecall and F1 score indicate better performance. Usually thresh-\nolds need to be manually selected and applied on the system\noutputs to calculate TP, FP and FN. We use average precision\n(AP) metric [7] to compare the performance of different sys-\ntems because the AP does not depend on thresholds. AP is\nde\ufb01ned as the area under the precision-recall curve calculated\nat multiple thresholds. Mean average precision (mAP) is the\naveraged AP over all sound classes. The higher mAP indicates\nbetter performance. Random guess has an mAP of 0.06 for the\nDCASE 2017 Task 4 containing 17 sound classes. The mAP is a\nmacro-averaging statistic because it is calculated independently\nwithin a sound class.", "mimetype": "text/plain", "start_char_idx": 1206, "end_char_idx": 2846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb03027e-2cb1-4acf-acfa-bd091b17925a": {"__data__": {"id_": "bb03027e-2cb1-4acf-acfa-bd091b17925a", "embedding": null, "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "0afe29e3e966d50b421c8b37b6360cf439e83a2e462a1676c329243388a2d1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "8d3c7382e70543d00f1739f8c0efc4556971e02fc1b59fd09def5886993f4a9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c09bbfad-90f5-4042-8348-efe554b42b10", "node_type": "1", "metadata": {}, "hash": "096987dc48cfd40fdb83538cb1a3d4ebe93623d1b7341d14daf656b6ed1f03c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean average precision (mAP) is the\naveraged AP over all sound classes. The higher mAP indicates\nbetter performance. Random guess has an mAP of 0.06 for the\nDCASE 2017 Task 4 containing 17 sound classes. The mAP is a\nmacro-averaging statistic because it is calculated independently\nwithin a sound class. Then, the statistics are averaged across\nall sound classes. On the other hand, micro-averaging statistic\nis calculated from outputs and ground truths \ufb02attened from all\nclasses.\nFor the AT subtask, systems are ranked based on macro-\naveraging F1 score. For the SED subtask, systems are ranked\nbased on micro-averaging F1 score and Error rate (ER) evaluated\non 1-second segments [4]. Error rate measures the amount of\nerrors in terms of insertions (I), deletions (D) and substitutions\n(S), and is de\ufb01ned as follows [52]:\nER =\n\u2211\nm S(m)+ \u2211\nm D(m)+ \u2211\nm I(m)\u2211\nm N(m) , (14)\nwhere I(m),D(m),S(m),N (m) are the number of inserted,\ndeleted, substituted, and ground truth sound events in the m-th\nsegment. Lower ER indicates better performance. The segment\nbased evaluation is calculated in a \ufb01xed time grid, using seg-\nments of one second length to compare the ground truth and\nthe system output [6]. Similarly, segment based F1-score are\ncalculated in the same way.\nE. Segment-Wise AT and SED\nThere is a lack of research comparing segment-wise [10] and\nclip-wise [12] training for AT and SED. We \ufb01rst investigate\nthe segment-wise training method. To begin with, an audio clip\nis split into segments.", "mimetype": "text/plain", "start_char_idx": 2543, "end_char_idx": 4038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c09bbfad-90f5-4042-8348-efe554b42b10": {"__data__": {"id_": "c09bbfad-90f5-4042-8348-efe554b42b10", "embedding": null, "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "0afe29e3e966d50b421c8b37b6360cf439e83a2e462a1676c329243388a2d1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb03027e-2cb1-4acf-acfa-bd091b17925a", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "8fd2269dc3add54af61718d051034c43706565daa094e090432f7f247e388675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, segment based F1-score are\ncalculated in the same way.\nE. Segment-Wise AT and SED\nThere is a lack of research comparing segment-wise [10] and\nclip-wise [12] training for AT and SED. We \ufb01rst investigate\nthe segment-wise training method. To begin with, an audio clip\nis split into segments. Then SED predictions are calculated by\nrunning audio tagging system on segments. Because the audio\nclips are weakly labelled, there is no information when a sound\nevent occurs and how long they last. This can affect the label\naccuracy of segment-wise training because all segments inherit\nthe tags from the audio clip. In inference, the SED result is\nobtained by predicting audio tags on segments. Table III shows\nthe average percentage of time frames in an audio clip containing\ndifferent sound events from the validation set of DCASE 2017\nTask 4. Sound events such as civil defense siren has a presence\npercentage of 0.930 which indicates segment-wise labels are\nmore likely to be correct. Sound events such as train horn has\na presence percentage of 0.400 which indicates segment-wise\nlabel is less likely to be correct.\nThe 10-second audio clips are split into segments with differ-\nent lengths from 1 to 10 seconds. Each segment inherit the tags\nfrom the audio clip. The minimum 1-second setting follows [10].\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 3739, "end_char_idx": 5212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8c719d6-765d-450f-8802-d3ddf3999ee4": {"__data__": {"id_": "a8c719d6-765d-450f-8802-d3ddf3999ee4", "embedding": null, "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80593f87-d286-44aa-8dad-b7b1ccd9284b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ca8b2572e3b9387a1d700682fbf2ca1054e43340b128542c7cfbe515ac646577", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd223082-e5a3-46bf-b9e8-71b22a82d318", "node_type": "1", "metadata": {}, "hash": "0ab39a231f460f7a9f5299a26e67cf09a070fb633df8f0894a8ef805805cbcdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2456 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\nTABLE III\nTHE PERCENTAGE OF TIME CONTAINING SOUND EVENTS IN AN AUDIO CLIP LABELLED AS CONTAINING THE SOUND EVENT\nFig. 1. Segment-wise training result trained with different durations of audio segments. From left to right: Audio tagging macro mAP; SED macro mAP; S ED\nmicro error rate.\nFig. 2. Clip-wise training result with different systems. Audio tagging macro mAP; SED macro mAP; SED micro error rate.\nA 9-layer CNN is applied to build the segment-wise training sys-\ntems. Fig. 1 shows the mAP and ER of AT and SED with different\nsegment durations. Training with 2-second segments achieves\nan mAP of 0.64 in audio tagging, slightly outperforming other\nsegment duration in AT. This indicates that the prediction of\nlong segments does not perform well when no attention or\ntemporal dependency is used. The second column of Fig. 1 shows\nthat training with 1-second segments achieves an SED mAP of\n0.44, outperforming other segment duration. This indicates that\nshorter segments achieve better SED result than longer segments\nwhen using segment-wise training systems. One explanation\nis that SED is obtained by AT on segments, so AT on shorter\nsegments can provide higher SED resolution. To calculate the\nER, we use constant AT thresholds of \u03bck =0 .5,k =1 ,...,K ,\nand SED thresholds of \u03c4high\nk =0 .3,\u03c4 low\nk =0 .1,k =1 ,...,K for\nall sound class following [12]. The third column of Fig.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd223082-e5a3-46bf-b9e8-71b22a82d318": {"__data__": {"id_": "bd223082-e5a3-46bf-b9e8-71b22a82d318", "embedding": null, "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80593f87-d286-44aa-8dad-b7b1ccd9284b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "ca8b2572e3b9387a1d700682fbf2ca1054e43340b128542c7cfbe515ac646577", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8c719d6-765d-450f-8802-d3ddf3999ee4", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "19c2786648d75384bbff8e31269f55a870ef15840e711278fa5adccfe68c397c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One explanation\nis that SED is obtained by AT on segments, so AT on shorter\nsegments can provide higher SED resolution. To calculate the\nER, we use constant AT thresholds of \u03bck =0 .5,k =1 ,...,K ,\nand SED thresholds of \u03c4high\nk =0 .3,\u03c4 low\nk =0 .1,k =1 ,...,K for\nall sound class following [12]. The third column of Fig. 1 shows\nthat the 1-second and 2-second segment duration achieve an ER\nof 0.74, outperforming other segment durations.\nF . Clip-Wise AT and SED\nWe investigate the clip-wise training systems in this section.\nThe difference of the clip-wise training and the segment-wise\ntraining is that with clip-wise training, the SED result can be\nobtained from the intermediate layer of a neural network. Then,\nthe AT predictions can be calculated by the aggregation functions\nsuch as (5, 6, 7). Fig. 2 shows the AT and SED performance of the\nclip-wise CNN systems. For the AT subtask, the decision-level\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 1151, "end_char_idx": 2219, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12d40e66-3d9c-4784-8e82-2081fdbdcf40": {"__data__": {"id_": "12d40e66-3d9c-4784-8e82-2081fdbdcf40", "embedding": null, "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbf356b3-ba72-400a-a12e-d464101bcf72", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "7b04535b83ea3e68a9f34d72531afa97437a1096b9eef7b79c63eed90bb6d861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f14048be-df00-407c-b1e3-72ae8def3d15", "node_type": "1", "metadata": {}, "hash": "b313f5802cb0fc89777f3b568b74fb73e4f6f03c5696b7e9f87e98ee9d3cc917", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "KONG et al.: SOUND EVENT DETECTION OF WEAKLY LABELLED DATA 2457\nFig. 3. From left to right: Class-wise AT average precision; Class-wise SED average precision; Class-wise SED error rate with the CNN-biGRU-Att syst em.\nFig. 4. Precision-recall curves of sound classes at different thresholds with the CNN-biGRU-Att system.\nmaximum (CNN-Max) or decision-level average (CNN-Avg)\nsystems achieve an mAP of 0.60. The decision-level attention\n(CNN-Att) improves the mAP to 0.64, indicating the atten-\ntion plays an important role in AT. The CNN-biGRU systems\nand the CNN-Transformer system further improve the mAP\nperformance to 0.65, indicating that the temporal dependency\ninformation is helpful for AT. For the SED subtask, the CNN-\nTransformer system achieves an SED mAP of 0.45, slightly\noutperforming the CNN-biGRU systems of 0.44 and other CNN\nsystems of 0.36 to 0.39, respectively. On the other hand, CNN-\nbiGRU achieves an ER of 0.66, outperforming other systems\nranging from 0.69 to 0.86. To calculate ER we applies thresholds\nthat are the same as the segment-wise training systems. Fig. 1 and\nFig. 2 show that the segment-wise training achieves better mAP\non the SED task, while the clip-wise training achieves better ER\non the SED task. One explanation is that mAP is evaluated in\nframe-wise, while ER is evaluated in 1-second segments.\nFig. 3 shows the class-wise performance of the CNN-biGRU-\nAtt system over training iterations. The performance on different\nsound classes varies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f14048be-df00-407c-b1e3-72ae8def3d15": {"__data__": {"id_": "f14048be-df00-407c-b1e3-72ae8def3d15", "embedding": null, "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbf356b3-ba72-400a-a12e-d464101bcf72", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "7b04535b83ea3e68a9f34d72531afa97437a1096b9eef7b79c63eed90bb6d861", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12d40e66-3d9c-4784-8e82-2081fdbdcf40", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "850e5670534ca945438c21c5139d0fd98f8b0524297901701acd1b9d4b6ed457", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 1 and\nFig. 2 show that the segment-wise training achieves better mAP\non the SED task, while the clip-wise training achieves better ER\non the SED task. One explanation is that mAP is evaluated in\nframe-wise, while ER is evaluated in 1-second segments.\nFig. 3 shows the class-wise performance of the CNN-biGRU-\nAtt system over training iterations. The performance on different\nsound classes varies. The prediction of screaming achieves the\nhighest AT mAP of 0.94. On the other hand, the prediction of car\npassing by achieves the lowest mAP of 0.20. One explanation\nof the underperformance of sound classes such as car passing\nby is that they are dif\ufb01cult to perceive even by human in audio\nrecordings. For SED, some sound classes achieve better mAP\nthan others, for example, civil defense siren achieves the highest\nmAP of 0.80, indicating the system is performing well on\nthese sound classes. The ER curve of different sound classes\nis different. Civil defense siren has an ER of 0.26 while other\nsound classes such as bicycle has ER over 1. The class-wise\nresults show that both the AT and SED performance vary from\nsound class to sound class.\nG. Automatic Thresholds Optimization\nPrevious subsection shows that the performance of different\nsound classes can be different. It can be useful to observe\ntheir precision-recall curves under various thresholds. Fig. 4\nshows the AT precision-recall curve of sound classes with\nthe CNN-biGRU-Att system under different thresholds ranging\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 1086, "end_char_idx": 2731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45fd1fd2-357e-41d0-8964-eeb46efd782e": {"__data__": {"id_": "45fd1fd2-357e-41d0-8964-eeb46efd782e", "embedding": null, "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37df766f-7beb-4a8c-8cce-46c85d739594", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "1d537fd2f33f52d045388a034581178735bd15603ff04a9bcf4171b50725d2ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a068120-2c9e-4eb1-992c-cd851b2c880a", "node_type": "1", "metadata": {}, "hash": "ffbdbfc33ac8890ce3a296c36b2e554052129bdb96d970730f96a7e689fe2ece", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2458 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\nTABLE IV\nPERFORMANCE OF THE PROPOSED SYSTEMS ON THE VALIDATION (VAL.) AND EVA L UAT I O N(EVA L.) SUBSET\nTABLE V\nAT OF DIFFERENT SYSTEMS\nfrom 0 and 1. The blue and red curve show the validation and\nevaluation precision-recall curve, respectively. Fig. 4 shows that\nthe validation and the evaluation curve have similar trend but are\nnot overlapped, indicating that the data distribution of validation\nand evaluation data can be slightly different. Some sound classes\nsuch as screaming have high precision at a variety of thresholds.\nOn the other hand, the precision drops rapidly with the increase\nof recall for some sound classes such as car passing by. Fig. 4\nshows that different sound classes have different thresholds to\nachieve optimal metrics such as F1 score.\nTable IV shows the AT and SED performance with the clip-\nwise training systems. We \ufb01rst apply constant thresholds for both\nAT and SED systems. The constant thresholds are the same as the\nthresholds applied in previous sections. In addition to applying\nthe constant thresholds, we apply thresholds \u0398AT and \u0398SED for\nAT and SED obtained by using the automatic thresholds opti-\nmization algorithm described in Algorithm 4. Table IV shows\nthat the proposed automatic thresholds optimization improved\nboth the AT and SED performance. For example, the CNN-\nTransformer AT F1 score improves from 0.557 to 0.599, and\n0.629 to 0.646 in the validation and evaluation set, respectively.\nThe CNN-biGRU SED ER is reduced from 0.80 to 0.65, and\n0.78 to 0.68 in the validation and evaluation set, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a068120-2c9e-4eb1-992c-cd851b2c880a": {"__data__": {"id_": "2a068120-2c9e-4eb1-992c-cd851b2c880a", "embedding": null, "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37df766f-7beb-4a8c-8cce-46c85d739594", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "1d537fd2f33f52d045388a034581178735bd15603ff04a9bcf4171b50725d2ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45fd1fd2-357e-41d0-8964-eeb46efd782e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "813c9043350428a42bb8e824efa57eed157ff86a5cb9fd9f05e2a328118b751c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "601e0d9e-562f-45f0-b7ea-ce42a33f8c29", "node_type": "1", "metadata": {}, "hash": "285d3cbcc724586238ea143fc338dc3cf40b4ef1a7d51767e68a17bc97150ba2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, the CNN-\nTransformer AT F1 score improves from 0.557 to 0.599, and\n0.629 to 0.646 in the validation and evaluation set, respectively.\nThe CNN-biGRU SED ER is reduced from 0.80 to 0.65, and\n0.78 to 0.68 in the validation and evaluation set, respectively.\nThose results show the effectiveness of the proposed automatic\nthreshold optimization method.\nTable V shows the precision, recall and F1-score of differ-\nent methods for the AT on the validation and evaluation sets,\nrespectively. The of\ufb01cial DCASE2017 baseline is give in [4]\nTABLE VI\nSED RESULTS OF DIFFERENT SYSTEMS\nby using a multilayer perceptron (MLP) classi\ufb01er, denoted as\n\u201cDCASE2017 Baseline\u201d. The MIL-NN is a multiple instance\nlearning based neural network system proposed in [53]. The\nCNN-ensemble system is proposed by [16] and ranked the 1st\nin the SED subtask in Task 4 of the DCASE 2017 challenge.\nOur proposed systems achieve an F1 score of 0.646 on the\nevaluation set, outperforming the other methods in AT. The\nCNN-biGRU and the CNN-Transformer systems achieve similar\nperformance. Table VI shows the SED result with different\nmethods. On the evaluation set, our proposed CNN-biGRU-Att\nwith automatic thresholds optimization achieves an F1 score of\n0.584, outperforming other systems. The system achieves an\nER of 0.68 which is comparable with the ensemble based CNN\nsystem [16].\nVII. C ONCLUSION\nThis paper investigates sound event detection (SED) with\nweakly labelled data.", "mimetype": "text/plain", "start_char_idx": 1379, "end_char_idx": 2837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "601e0d9e-562f-45f0-b7ea-ce42a33f8c29": {"__data__": {"id_": "601e0d9e-562f-45f0-b7ea-ce42a33f8c29", "embedding": null, "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37df766f-7beb-4a8c-8cce-46c85d739594", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "1d537fd2f33f52d045388a034581178735bd15603ff04a9bcf4171b50725d2ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a068120-2c9e-4eb1-992c-cd851b2c880a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "59ed52540dd79c368d8cf0cb001f5cb5bcf2352ad1cf5ab28813ad9808e32dba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table VI shows the SED result with different\nmethods. On the evaluation set, our proposed CNN-biGRU-Att\nwith automatic thresholds optimization achieves an F1 score of\n0.584, outperforming other systems. The system achieves an\nER of 0.68 which is comparable with the ensemble based CNN\nsystem [16].\nVII. C ONCLUSION\nThis paper investigates sound event detection (SED) with\nweakly labelled data. The variants of convolutional neural net-\nworks (CNNs) and CNN-Transformer systems were proposed\nfor the audio tagging and sound event detection. The segment-\nwise training and clip-wise training systems were investigated.\nThe clip-wise training achieves an mAP of 0.650 in audio\ntagging and an ER of 0.68 in SED. A novel automatic thresh-\nolds optimization method is proposed to approach the thresh-\nolds selection problem. The automatic thresholds optimization\nmethod improves the AT F1 score from 0.629 to 0.646, and\nreduces the ER from 0.78 to 0.68. We show that the CNN-\nTransformer performs similarly to the CRNN system, while\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 2444, "end_char_idx": 3629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "655bdd10-e7f2-420f-b69d-714523e83cfe": {"__data__": {"id_": "655bdd10-e7f2-420f-b69d-714523e83cfe", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95b38490-f17e-4157-8114-901692b8c2f0", "node_type": "1", "metadata": {}, "hash": "8b54772237bf53705780a4aa24e34e24a6961655aba95214459c25716cfabb1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "KONG et al.: SOUND EVENT DETECTION OF WEAKLY LABELLED DATA 2459\nthe CNN-Transformer has the advantage of being computed in\nparallel. In addition, the improvements of audio tagging and\nSED are mainly from the automatic threshold optimization. In\nfuture, we will focus on extending the sound event detection\nsystems to large-scale training data such as AudioSet.\nREFERENCES\n[1] D. Giannoulis, E. Benetos, D. Stowell, M. Rossignol, M. Lagrange, and M.\nD. Plumbley, \u201cDetection and classi\ufb01cation of acoustic scenes and events:\nAn IEEE AASP challenge,\u201d in Proc. IEEE Workshop Appl. Signal Process.\nAudio Acoust., 2013, pp. 1\u20134.\n[2] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D. Plumbley,\n\u201cDetection and classi\ufb01cation of acoustic scenes and events,\u201d IEEE Trans.\nMultimedia, vol. 17, no. 10, pp. 1733\u20131746, Oct. 2015.\n[3] A. Mesaros, T. Heittola, and T. Virtanen, \u201cTUT database for acoustic\nscene classi\ufb01cation and sound event detection,\u201d in Proc. IEEE Eur . Signal\nProcess. Conf., 2016, pp. 1128\u20131132.\n[4] A. Mesaros et al., \u201cDCASE 2017 challenge setup: Tasks, datasets and\nbaseline system,\u201d in Proc. Workshop Detect. Classi\ufb01c. Acoust. Scenes\nEvents, 2017, pp. 85\u201392.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95b38490-f17e-4157-8114-901692b8c2f0": {"__data__": {"id_": "95b38490-f17e-4157-8114-901692b8c2f0", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "655bdd10-e7f2-420f-b69d-714523e83cfe", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9aebbaaa6c6669e46f5e1aa5dc132bd09918753b7122d7deedef74dd1dd6ab53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3223bc50-70e9-4965-93b0-f7ff73316f88", "node_type": "1", "metadata": {}, "hash": "7a22aaac047cf6c54e832666bbb128170014139582c42f82e6678109ddf67a51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Eur . Signal\nProcess. Conf., 2016, pp. 1128\u20131132.\n[4] A. Mesaros et al., \u201cDCASE 2017 challenge setup: Tasks, datasets and\nbaseline system,\u201d in Proc. Workshop Detect. Classi\ufb01c. Acoust. Scenes\nEvents, 2017, pp. 85\u201392.\n[5] A. Mesaros, T. Heittola, and T. Virtanen, \u201cA multi-device dataset for urban\nacoustic scene classi\ufb01cation,\u201d Workshop Detect. Classi\ufb01c. Acoust. Scenes\nEvents, 2018.\n[6] \u201cDCASE 2017 Task 4,\u201d 2017. [Online]. Available: http://www.cs.tut.\ufb01/\nsgn/arg/dcase2017/challenge/task-large-scale-sound-event-detection\n[7] J. F. Gemmeke et al., \u201cAudio Set: An ontology and human-labeled dataset\nfor audio events,\u201d inIEEE Int. Conf. Acoust., Speech Signal Process., 2017,\npp. 776\u2013780.\n[8] E. Cak\u0131r, T. Heittola, and T. Virtanen, \u201cDomestic audio tagging with con-\nvolutional neural networks,\u201d DCASE2016 Challenge, Tech. Rep., 2016.\n[Online]. Available: http://dcase.community/challenge2016\n[9] Y . Xu, Q. Kong, Q. Huang, W. Wang, and M. D. Plumbley, \u201cCon-\nvolutional gated recurrent neural network incorporating spatial fea-\ntures for audio tagging,\u201d in Proc. Int. Joint Conf. Neural Netw., 2017,\npp. 3461\u20133466.", "mimetype": "text/plain", "start_char_idx": 956, "end_char_idx": 2074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3223bc50-70e9-4965-93b0-f7ff73316f88": {"__data__": {"id_": "3223bc50-70e9-4965-93b0-f7ff73316f88", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95b38490-f17e-4157-8114-901692b8c2f0", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "d5f08d31f5b5b08a9c491b047f4c96334991f8958bb54246aad2dc9199e3c40c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9863ef36-c206-4404-b132-8a58ca2bad44", "node_type": "1", "metadata": {}, "hash": "b19604ea3391d008fc0fa9da0dfda32313eef857e30d61e0090fd6ec7c8bc120", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rep., 2016.\n[Online]. Available: http://dcase.community/challenge2016\n[9] Y . Xu, Q. Kong, Q. Huang, W. Wang, and M. D. Plumbley, \u201cCon-\nvolutional gated recurrent neural network incorporating spatial fea-\ntures for audio tagging,\u201d in Proc. Int. Joint Conf. Neural Netw., 2017,\npp. 3461\u20133466.\n[10] S. Hershey et al., \u201cCNN architectures for large-scale audio classi\ufb01ca-\ntion,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2017,\npp. 131\u2013135.\n[11] A. Kumar and B. Raj, \u201cAudio event detection using weakly labeled data,\u201d\nin Proc. ACM Multimedia Conf., 2016, pp. 1038\u20131047.\n[12] Y . Xu, Q. Kong, W. Wang, and M. D. Plumbley, \u201cLarge-scale weakly\nsupervised audio classi\ufb01cation using gated convolutional neural net-\nwork,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2018,\npp. 121\u2013125.\n[13] T.-W. Su, J.-Y . Liu, and Y .-H. Yang, \u201cWeakly-supervised audio event\ndetection using event-speci\ufb01c gaussian \ufb01lters and fully convolutional\nnetworks,\u201d inProc. IEEE Int. Conf. Acoust., Speech Signal Process., 2017,\npp. 791\u2013795.\n[14] S.-Y . Chou, J.-S. R. Jang, and Y .-H. Yang, \u201cLearning to recognize transient\nsound events using attentional supervision.\u201d in Proc. Int.", "mimetype": "text/plain", "start_char_idx": 1783, "end_char_idx": 2961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9863ef36-c206-4404-b132-8a58ca2bad44": {"__data__": {"id_": "9863ef36-c206-4404-b132-8a58ca2bad44", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3223bc50-70e9-4965-93b0-f7ff73316f88", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "44bb1814ccb19f10f450c5d2be9587ac5d880b69d185f74dd71a3097f4ac2154", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a70a5a57-8aee-4535-bff8-45f2838c8427", "node_type": "1", "metadata": {}, "hash": "822a4a4fdd06aa4c52eb496e108f64cffae9a21fc2cc40cb40cf0a580fc95262", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. Conf. Acoust., Speech Signal Process., 2017,\npp. 791\u2013795.\n[14] S.-Y . Chou, J.-S. R. Jang, and Y .-H. Yang, \u201cLearning to recognize transient\nsound events using attentional supervision.\u201d in Proc. Int. Joint Conf. Artif.\nIntell., 2018, pp. 3336\u20133342.\n[15] J. Salamon, B. McFee, and P. Li, \u201cDCASE 2017 submission: Multiple in-\nstance learning for sound event detection,\u201d DCASE2017 Challenge, Tech.\nRep., 2017. [Online]. Available: http://dcase.community/challenge2017\n[16] K. Lee, D. Lee, S. Lee, and Y . Han, \u201cEnsemble of convolutional neural\nnetworks for weakly-supervised sound event detection using multiple\nscale input,\u201d DCASE2017 Challenge, Tech. Rep., Sep. 2017. [Online].\nAvailable: http://dcase.community/challenge2017\n[17] S. Chou, J. Jang, and Y .-H. Yang, \u201cFrameCNN: A weakly-supervised\nlearning framework for frame-wise acoustic event detection and classi-\n\ufb01cation,\u201d DCASE2017 Challenge, Tech. Rep., 2017. [Online]. Available:\nhttp://dcase.community/challenge2017\n[18] S. Adavanne, P. Pertil\u00e4, and T. Virtanen, \u201cSound event detection using\nspatial features and convolutional recurrent neural network,\u201d in Proc. IEEE\nInt. Conf. Acoust., Speech Signal Process., 2017, pp. 771\u2013775.", "mimetype": "text/plain", "start_char_idx": 2752, "end_char_idx": 3950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a70a5a57-8aee-4535-bff8-45f2838c8427": {"__data__": {"id_": "a70a5a57-8aee-4535-bff8-45f2838c8427", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9863ef36-c206-4404-b132-8a58ca2bad44", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "a9d570113c92e72cf1bb2079bc7be26bbdbbd7df1a7e8cc449d35e151e4fdb6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d", "node_type": "1", "metadata": {}, "hash": "876cc1b31bf7fb7dc5c9068c7f6541adbeebb7c204db46065f74cf6b21b1f2dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rep., 2017. [Online]. Available:\nhttp://dcase.community/challenge2017\n[18] S. Adavanne, P. Pertil\u00e4, and T. Virtanen, \u201cSound event detection using\nspatial features and convolutional recurrent neural network,\u201d in Proc. IEEE\nInt. Conf. Acoust., Speech Signal Process., 2017, pp. 771\u2013775.\n[19] L. Ford, H. Tang, F. Grondin, and J. Glass, \u201cA deep residual network\nfor large-scale acoustic scene analysis,\u201d in Proc. INTERSPEECH, 2019,\npp. 2568\u20132572.\n[20] E. Cakir, T. Heittola, H. Huttunen, and T. Virtanen, \u201cPolyphonic sound\nevent detection using multi label deep neural networks,\u201d in Proc. Int. Joint\nConf. Neural Netw., 2015.\n[21] E. Cakir, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen,\n\u201cConvolutional recurrent neural networks for polyphonic sound event\ndetection,\u201dIEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 25, no. 6,\npp. 1291\u20131303, Jun. 2017.\n[22] T. Hayashi, S. Watanabe, T. Toda, T. Hori, J. Le Roux, and K. Takeda,\n\u201cBLSTM-HMM hybrid system combined with sound activity detection\nnetwork for polyphonic sound event detection,\u201d in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process., 2017, pp. 766\u2013770.", "mimetype": "text/plain", "start_char_idx": 3666, "end_char_idx": 4794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d": {"__data__": {"id_": "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a70a5a57-8aee-4535-bff8-45f2838c8427", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "00d7aae086b7d6c60a9b22f3c990c0eb672a57ddfc4f3ff101eaf9140f337d7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4f132d1-b564-4a5f-9bf9-0f92bed4362b", "node_type": "1", "metadata": {}, "hash": "ca766ad649433251cf228455c7e2ab70cba1f70268eb180f1f5d364c7f5fa4b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1291\u20131303, Jun. 2017.\n[22] T. Hayashi, S. Watanabe, T. Toda, T. Hori, J. Le Roux, and K. Takeda,\n\u201cBLSTM-HMM hybrid system combined with sound activity detection\nnetwork for polyphonic sound event detection,\u201d in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process., 2017, pp. 766\u2013770.\n[23] J. L. Dai Wei, P. Pham, S. Das, S. Qu, and F. Metze, \u201cSound event detection\nfor real life audio DCASE challenge,\u201d DCASE2016 Challenge, Tech. Rep.,\n2016. [Online]. Available: http://dcase.community/challenge2016\n[24] X. Xia, R. Togneri, F. Sohel, and D. Huang, \u201cFrame-wise dynamic thresh-\nold based polyphonic acoustic event detection,\u201d in Proc. INTERSPEECH,\n2017, pp. 474\u2013478.\n[25] H. Phan, L. Hertel, M. Maass, and A. Mertins, \u201cRobust audio event\nrecognition with 1-max pooling convolutional neural networks,\u201d in Proc.\nINTERSPEECH, 2016, pp. 3653\u20133657.\n[26] Y . Wang and F. Metze, \u201cA \ufb01rst attempt at polyphonic sound event detection\nusing connectionist temporal classi\ufb01cation,\u201d in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process., 2017, pp. 2986\u20132990.\n[27] S. Gururani, C. Summers, and A. Lerch, \u201cInstrument activity detection in\npolyphonic music using deep neural networks.\u201d in Proc. Int. Soc. Music\nInf.", "mimetype": "text/plain", "start_char_idx": 4510, "end_char_idx": 5711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4f132d1-b564-4a5f-9bf9-0f92bed4362b": {"__data__": {"id_": "f4f132d1-b564-4a5f-9bf9-0f92bed4362b", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "b046e004fcb0278563f94e08ea0d52dc4ad03f50c70924431eb9178b78cfc04d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b227e76-fb11-4d64-80bb-46e5a369f0b3", "node_type": "1", "metadata": {}, "hash": "86a6c5229c14bffc31409df9fc837d0bfcaf375fc54a42760d21acb800d7ba9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. Conf.\nAcoust., Speech Signal Process., 2017, pp. 2986\u20132990.\n[27] S. Gururani, C. Summers, and A. Lerch, \u201cInstrument activity detection in\npolyphonic music using deep neural networks.\u201d in Proc. Int. Soc. Music\nInf. Retrieval, 2018, pp. 569\u2013576.\n[28] J. Lee, J. Park, S. Kum, Y . Jeong, and J. Nam, \u201cCombining multi-scale\nfeatures using sample-level deep convolutional neural networks for weakly\nsupervised sound event detection,\u201d Proc. Workshop Detect. Classi\ufb01c.\nAcoust. Scenes Events (DCASE), 2017, pp. 69\u201373.\n[29] K. Choi, G. Fazekas, M. Sandler, and K. Cho, \u201cConvolutional recurrent\nneural networks for music classi\ufb01cation,\u201d in Proc. IEEE Int. Conf. Acoust.,\nSpeech Signal Process., 2017, pp. 2392\u20132396.\n[30] E. Cakir, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen,\n\u201cConvolutional recurrent neural networks for polyphonic sound event\ndetection,\u201dIEEE/ACM Trans. Audio, Speech, and Lang. Process., vol. 25,\nno. 6, pp. 1291\u20131303, Jun. 2017.\n[31] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. Advances Neural\nInform. Process. Syst., 2017, pp. 5998\u20136008.\n[32] J. Devlin, M.-W.", "mimetype": "text/plain", "start_char_idx": 5488, "end_char_idx": 6595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b227e76-fb11-4d64-80bb-46e5a369f0b3": {"__data__": {"id_": "7b227e76-fb11-4d64-80bb-46e5a369f0b3", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4f132d1-b564-4a5f-9bf9-0f92bed4362b", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "0071064461574047775cf38be987c8e3afc4ad33138bbff6d33f8a6fbcc2efeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eeeefc92-f126-4be8-924a-9cd54b409613", "node_type": "1", "metadata": {}, "hash": "51179da36e8922771f2e62826078d78f04dedbe292b0501979d960a0dac3abfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Audio, Speech, and Lang. Process., vol. 25,\nno. 6, pp. 1291\u20131303, Jun. 2017.\n[31] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. Advances Neural\nInform. Process. Syst., 2017, pp. 5998\u20136008.\n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-\ntraining of deep bidirectional transformers for language understanding,\u201d\nin Proc. North Amer . Chapter Assoc. Comput. Linguistics (NAACL), 2019,\npp. 4171\u20134186.\n[33] M. Won, S. Chun, and X. Serra, \u201cToward interpretable music tagging with\nself-attention,\u201d 2019, arXiv:1906.04972.\n[34] L. Cances, P. Guyot, and T. Pellegrini, \u201cEvaluation of post-processing\nalgorithms for polyphonic sound event detection,\u201d in Proc. IEEE Workshop\nAppl. Signal Process. Audio Acoust., 2019, pp. 318\u2013322.\n[35] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classi\ufb01cation\nwith deep convolutional neural networks,\u201d in Proc. Adv. Neural Inf. Pro-\ncess. Syst., 2012, pp. 1097\u20131105.\n[36] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D.\nYu, \u201cConvolutional neural networks for speech recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, Lang. Process., vol.", "mimetype": "text/plain", "start_char_idx": 6374, "end_char_idx": 7489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eeeefc92-f126-4be8-924a-9cd54b409613": {"__data__": {"id_": "eeeefc92-f126-4be8-924a-9cd54b409613", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b227e76-fb11-4d64-80bb-46e5a369f0b3", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "83f3e001c65a770c4bfbefdff3b982f1089b74994f82b65fe6fce5aa42e4caf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec604294-391a-4a95-b2c8-af2be54bcd1b", "node_type": "1", "metadata": {}, "hash": "90c191b7ba2719d63a69c7c44395d6e0c4ed0bfc357e255315ad040c3c0b2ca8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Adv. Neural Inf. Pro-\ncess. Syst., 2012, pp. 1097\u20131105.\n[36] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D.\nYu, \u201cConvolutional neural networks for speech recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, Lang. Process., vol. 22, no. 10, pp. 1533\u20131545,\nOct. 2014.\n[37] K. Choi, G. Fazekas, and M. Sandler, \u201cAutomatic tagging using deep\nconvolutional neural networks,\u201d Int. Soc. Music Inf. Retrieval, 2016.\n[38] Q. Kong, T. Iqbal, Y . Xu, W. Wang, and M. D. Plumbley, \u201cDCASE 2018\nchallenge baseline with convolutional neural networks,\u201d inProc. Workshop\nDetect. Classi\ufb01c. Acoust. Scenes Events, 2018, pp. 217\u2013221.\n[39] J. Thickstun, Z. Harchaoui, and S. Kakade, \u201cLearning features of music\nfrom scratch,\u201d in Proc. Int. Conf. Learn. Representations, 2017.\n[40] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network\ntraining by reducing internal covariate shift,\u201d in Proc. Int. Conf. Mach.\nLearn., 2015.\n[41] V . Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted boltz-\nmann machines,\u201d in Proc. Int. Conf. Mach. Learn., 2010, pp. 807\u2013814.", "mimetype": "text/plain", "start_char_idx": 7251, "end_char_idx": 8332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec604294-391a-4a95-b2c8-af2be54bcd1b": {"__data__": {"id_": "ec604294-391a-4a95-b2c8-af2be54bcd1b", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "576b46ff-2bef-427f-888a-3f943bc07c53", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "9798b076aca5bb29423ec319f9d26cff7bdada01b73ef12f8e6abb31db573dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eeeefc92-f126-4be8-924a-9cd54b409613", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "1620d9b577d829a17b2483b6928e111708d002ef425b1768ae1acee5d2382748", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. Conf. Mach.\nLearn., 2015.\n[41] V . Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted boltz-\nmann machines,\u201d in Proc. Int. Conf. Mach. Learn., 2010, pp. 807\u2013814.\n[42] T. Mikolov, M. Kara\ufb01\u00e1t, L. Burget, J. \u02c7Cernock`y, and S. Khudanpur,\n\u201cRecurrent neural network based language model,\u201d in Proc. Conf. Int.\nSpeech Commun. Assoc., 2010, pp. 1045\u20131048.\n[43] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComput., vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[44] K. Cho et al., \u201cLearning phrase representations using RNN encoder-\ndecoder for statistical machine translation,\u201d in Proc. Conf. Empirical\nMethods Natural Lang. Process., 2014, pp. 1724\u20131734.\n[45] Q. Kong, C. Yu, Y . Xu, T. Iqbal, W. Wang, and M. D. Plumbley, \u201cWeakly\nlabelled audioset tagging with attention neural networks,\u201d IEEE/ACM\nTrans. Audio, Speech, Lang. Process., vol. 27, no. 11, pp. 1791\u20131802,\nNov. 2019.\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 8152, "end_char_idx": 9215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f088893-7aa2-49b1-b979-23278a0de780": {"__data__": {"id_": "2f088893-7aa2-49b1-b979-23278a0de780", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3a271db6b1883d1170d2d01fab25cc6312b99364136b5c90784b3c72d80d8e0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3824c77-aa3c-4e01-bae2-ae5291aa8e62", "node_type": "1", "metadata": {}, "hash": "9b779052214fad55e223e549fc4807c7f446f48a293e888b03f1c33739c8eb96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2460 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 28, 2020\n[46] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d Int.\nConf. Learn. Representations, 2014.\n[47] R. Serizel, N. Turpault, H. Eghbal-Zadeh, and A. P. Shah, \u201cLarge-scale\nweakly labeled semi-supervised sound event detection in domestic envi-\nronments,\u201d in Proc. Detect. Classi\ufb01c. Acoust. Scenes Events Workshop,\n2018, pp. 19\u201323.\n[48] N. Turpault, R. Serizel, A. P. Shah, and J. Salamon, \u201cSound event detec-\ntion in domestic environments with weakly labeled data and soundscape\nsynthesis,\u201d in Proc. Detect. Classi\ufb01c. Acoust. Scenes Events Workshop,\n2019, pp. 253\u2013257.\n[49] Q. Kong, Y . Cao, T. Iqbal, Y . Xu, W. Wang, and M. D. Plumbley,\n\u201cCross-task learning for audio tagging, sound event detection and spatial\nlocalization: DCASE 2019 baseline systems,\u201d 2019, arXiv:1904.03476.\n[50] G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural\nnetworks for LVCSR using recti\ufb01ed linear units and dropout,\u201d in Proc.\nInt. Conf. Acoust., Speech, Signal Process., 2013, pp. 8609\u20138613.\n[51] H. Zhang, M. Cisse, Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3824c77-aa3c-4e01-bae2-ae5291aa8e62": {"__data__": {"id_": "e3824c77-aa3c-4e01-bae2-ae5291aa8e62", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3a271db6b1883d1170d2d01fab25cc6312b99364136b5c90784b3c72d80d8e0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f088893-7aa2-49b1-b979-23278a0de780", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "4ddc09cec2d66fde18e0a4e2d10827163795a5a0b2a31a62fb154c238f7fd584", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62b31508-96ab-46ad-b9eb-a7dff8268e55", "node_type": "1", "metadata": {}, "hash": "0ba6fa176c321fdf50eaadd6b3a5d4362349a4d7d1b0f00055e40e6abcaa76ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[50] G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural\nnetworks for LVCSR using recti\ufb01ed linear units and dropout,\u201d in Proc.\nInt. Conf. Acoust., Speech, Signal Process., 2013, pp. 8609\u20138613.\n[51] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, \u201cMixup: Beyond\nempirical risk minimization,\u201d Proc. Int. Conf. Learn. Representations,\n2018.\n[52] A. Mesaros, T. Heittola, and T. Virtanen, \u201cMetrics for polyphonic sound\nevent detection,\u201d Appl. Sci., vol. 6, no. 6, pp. 162\u2013178, 2016.\n[53] S.-Y . Tseng, J. Li, Y . Wang, J. Szurley, F. Metze, and S. Das, \u201cMultiple\ninstance deep learning for weakly supervised audio event detection,\u201d in\nProc. INTERSPEECH, 2018, pp. 3279\u20133283.\nQiuqiang Kong (Student Member, IEEE) received\nthe B.Sc. and M.E. degrees from the South China Uni-\nversity of Technology, Guangzhou, China, in 2012\nand 2015, respectively. He is currently working to-\nward the Ph.D. degree from the University of Sur-\nrey, Guildford, U.K on sound event detection. His\nresearch topic includes sound understanding, audio\nsignal processing and machine learning. He was nom-\ninated as the postgraduate research student of the year\nin University of Surrey, 2019.\nYong Xu (Member, IEEE) received the Ph.D.", "mimetype": "text/plain", "start_char_idx": 881, "end_char_idx": 2105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62b31508-96ab-46ad-b9eb-a7dff8268e55": {"__data__": {"id_": "62b31508-96ab-46ad-b9eb-a7dff8268e55", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3a271db6b1883d1170d2d01fab25cc6312b99364136b5c90784b3c72d80d8e0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3824c77-aa3c-4e01-bae2-ae5291aa8e62", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "5f64c7b00019a5fc4bf91dc963e09e4a03473a8f8a2d8820dc84de6168c8328a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f68b309a-053d-486e-9355-47074dd4cb4e", "node_type": "1", "metadata": {}, "hash": "afc98ba95d521f0dd7b322ee98184edd4278b7a95c7b4cb98ccfca4ee037645c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is currently working to-\nward the Ph.D. degree from the University of Sur-\nrey, Guildford, U.K on sound event detection. His\nresearch topic includes sound understanding, audio\nsignal processing and machine learning. He was nom-\ninated as the postgraduate research student of the year\nin University of Surrey, 2019.\nYong Xu (Member, IEEE) received the Ph.D. degree\nfrom the University of Science and Technology of\nChina (USTC), Hefei, China, in 2015, on the topic\nof DNN-based speech enhancement and recognition.\nCurrently, he is a Senior Research Scientist in Ten-\ncent AI lab, Bellevue, USA. He once worked at the\nUniversity of Surrey, U.K. as a Research Fellow from\n2016 to 2018 working on sound event detection. He\nvisited Prof. Chin-Hui Lee\u2019s lab in Georgia Institute\nof Technology, USA from Sept. 2014 to May 2015. He\nonce also worked in IFLYTEK company from 2015\nto 2016 to develop far-\ufb01eld ASR technologies. His research interests include\ndeep learning, speech enhancement and recognition, sound event detection, etc.\nHe received 2018 IEEE SPS best paper award.\nWenwu Wang (Senior Member, IEEE) was born in\nAnhui, China. He received the B.Sc. degree in 1997,\nthe M.E. degree in 2000, and the Ph.D. degree in 2002,\nall from Harbin Engineering University, China. He\nthen worked in King\u2019s College London, Cardiff Uni-\nversity, Tao Group Ltd. (now Antix Labs Ltd.", "mimetype": "text/plain", "start_char_idx": 1746, "end_char_idx": 3115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f68b309a-053d-486e-9355-47074dd4cb4e": {"__data__": {"id_": "f68b309a-053d-486e-9355-47074dd4cb4e", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3a271db6b1883d1170d2d01fab25cc6312b99364136b5c90784b3c72d80d8e0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62b31508-96ab-46ad-b9eb-a7dff8268e55", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "f4bdcce25147b7aed36347d25a6b8fe26d11ee02d31e100cd7167d276a4c494f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7d918eb-0368-452e-9178-e09c2fc5ec05", "node_type": "1", "metadata": {}, "hash": "3bb8d273f6145de16dfc405b4e74dff87c1fbc18505fc80c6e9339f40ab50ecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wenwu Wang (Senior Member, IEEE) was born in\nAnhui, China. He received the B.Sc. degree in 1997,\nthe M.E. degree in 2000, and the Ph.D. degree in 2002,\nall from Harbin Engineering University, China. He\nthen worked in King\u2019s College London, Cardiff Uni-\nversity, Tao Group Ltd. (now Antix Labs Ltd.), and\nCreative Labs, before joining University of Surrey,\nUK, in May 2007, where he is currently a Professor\nin signal processing and machine learning, and a\nCo-Director of the Machine Audition Lab within the\nCentre for Vision Speech and Signal Processing. He\nhas been a Guest Professor at Qingdao University of Science and Technology,\nChina, since 2018. His current research interests include blind signal processing,\nsparse signal processing, audio-visual signal processing, machine learning and\nperception, machine audition (listening), and statistical anomaly detection. He\nhas (co)-authored over 200 publications in these areas. He served as an Associate\nEditor for IEEE T RANSACTIONS ON SIGNAL PROCESSING from 2014 to 2018.\nHe currently serves as Senior Area Editor for IEEE T RANSACTIONS ON SIGNAL\nPROCESSING and Associate Editor for IEEE/ACM Transactions on Audio Speech\nand Language Processing.\nMark D. Plumbley (Fellow, IEEE) received the\nB.A.(Hons.) degree in electrical sciences and the\nPh.D. degree in neural networks from the University\nof Cambridge, Cambridge, U.K., in 1984 and 1991,\nrespectively. Following his PhD, he became a Lec-\nturer at King\u2019s College London, before moving to\nQueen Mary University of London in 2002.", "mimetype": "text/plain", "start_char_idx": 2818, "end_char_idx": 4355, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7d918eb-0368-452e-9178-e09c2fc5ec05": {"__data__": {"id_": "d7d918eb-0368-452e-9178-e09c2fc5ec05", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "3a271db6b1883d1170d2d01fab25cc6312b99364136b5c90784b3c72d80d8e0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f68b309a-053d-486e-9355-47074dd4cb4e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}, "hash": "8e88599f89acd16116ca9e8f16476db67ca5986d19463c0d02b03e5d3759bf72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mark D. Plumbley (Fellow, IEEE) received the\nB.A.(Hons.) degree in electrical sciences and the\nPh.D. degree in neural networks from the University\nof Cambridge, Cambridge, U.K., in 1984 and 1991,\nrespectively. Following his PhD, he became a Lec-\nturer at King\u2019s College London, before moving to\nQueen Mary University of London in 2002. He subse-\nquently became Professor and Director of the Centre\nfor Digital Music, before joining the University of\nSurrey in 2015 as Professor of Signal Processing. He\nis known for his work on analysis and processing of\naudio and music, using a wide range of signal processing techniques, including\nmatrix factorization, sparse representations, and deep learning. He is a co-editor\nof the recent book on Computational Analysis of Sound Scenes and Events, and\nCo-Chair of the recent DCASE 2018 Workshop on Detection and Classi\ufb01cations\nof Acoustic Scenes and Events. He is a Member of the IEEE Signal Processing\nSociety Technical Committee on Signal Processing Theory and Methods, and a\nFellow of the IET and IEEE.\nAuthorized licensed use limited to: University of Maryland College Park. Downloaded on February 02,2025 at 02:01:59 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4020, "end_char_idx": 5226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9da70df-7ac2-4a2c-8b9a-75e3d229b35b": {"__data__": {"id_": "d9da70df-7ac2-4a2c-8b9a-75e3d229b35b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50ce1c55-a6be-43e5-b962-03da124cd818", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "8a72fc46916812762edfe1a25fabb4b291c963d7c04449ab855dfa3b49172180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "221aa72a-16a1-4638-9041-f14b562b942f", "node_type": "1", "metadata": {}, "hash": "bb5ceb339d57357d27c22ab9647ca068f4f498380270ae6cef69c056dd01e8ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Streaming keyword spotting on mobile devices\nOleg Rybakov1, Natasha Kononenko1, Niranjan Subrahmanya2, Mirk\u00b4o Visontai2, Stella Laurenzo1\n1Google Research 2Google Speech\n{rybakov, natashaknk, sniranjan, mirkov, laurenzo}@google.com\nAbstract\nIn this work we explore the latency and accuracy of keyword\nspotting (KWS) models in streaming and non-streaming modes\non mobile phones. NN model conversion from non-streaming\nmode (model receives the whole input sequence and then re-\nturns the classi\ufb01cation result) to streaming mode (model re-\nceives portion of the input sequence and classi\ufb01es it incremen-\ntally) may require manual model rewriting. We address this by\ndesigning a Tensor\ufb02ow/Keras based library which allows au-\ntomatic conversion of non-streaming models to streaming ones\nwith minimum effort. With this library we benchmark multi-\nple KWS models in both streaming and non-streaming modes\non mobile phones and demonstrate different tradeoffs between\nlatency and accuracy. We also explore novel KWS models with\nmulti-head attention which reduce the classi\ufb01cation error over\nthe state-of-art by 10% on Google speech commands data sets\nV2. The streaming library with all experiments is open-sourced.\n1\nIndex Terms: speech recognition, keyword spotting, on-device\ninference\n1. Introduction\nResearch and development of neural networks has many steps:\ndata collection, model design and training, model latency or\nmemory footprint optimization, model conversion to inference\nmode and execution of the model on different hardware, in-\ncluding mobile devices. In this work we are focused on the\nlast three steps: model optimization, model conversion to in-\nference mode and running it on mobile devices.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "221aa72a-16a1-4638-9041-f14b562b942f": {"__data__": {"id_": "221aa72a-16a1-4638-9041-f14b562b942f", "embedding": null, "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50ce1c55-a6be-43e5-b962-03da124cd818", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "8a72fc46916812762edfe1a25fabb4b291c963d7c04449ab855dfa3b49172180", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9da70df-7ac2-4a2c-8b9a-75e3d229b35b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7335f4ad782fa3124c867a479d8b492d09f36515a2fc182a26d3253c6cc4cbc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0", "node_type": "1", "metadata": {}, "hash": "98dba0f1a73de4a5d0a0289740ec03155e76d79ea518a92a9f90ec7a6474843c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction\nResearch and development of neural networks has many steps:\ndata collection, model design and training, model latency or\nmemory footprint optimization, model conversion to inference\nmode and execution of the model on different hardware, in-\ncluding mobile devices. In this work we are focused on the\nlast three steps: model optimization, model conversion to in-\nference mode and running it on mobile devices. A common\nmethod of model optimization is quantization [1, 2] allowing to\nreduce both model size and latency. It can be applied to many\nproblems, including computer vision [1] and speech recogni-\ntion [3]. Another optimization approach is the transformation\nof the NN model (which was used during training) into an-\nother NN streaming model which can be more ef\ufb01cient for in-\nference/prediction mode [4]. In several applications, such as\nimage classi\ufb01cation, model representation in the training and\ninference modes are the same, while in others, such as sequence\nclassi\ufb01cation problems (for example, KWS), it can be different.\n1.1. Model streaming example\nLet\u2019s consider an example of convolutional NN (shown on\nFig 1a) applied on KWS. A standard approach for model train-\ning is to use a non-streaming model representation, shown on\nFig 1a. It receives the whole input sequence and then returns\nthe classi\ufb01cation result (on Fig 1 the whole input sequence has\nlength 6 with single sample feature size 3).\nIn a KWS application we do not know when the keyword\nstarts or ends, so we need to process every audio packet and\n1Preprint. Submitted to INTERSPEECH.", "mimetype": "text/plain", "start_char_idx": 1283, "end_char_idx": 2860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0": {"__data__": {"id_": "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0", "embedding": null, "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50ce1c55-a6be-43e5-b962-03da124cd818", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "8a72fc46916812762edfe1a25fabb4b291c963d7c04449ab855dfa3b49172180", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "221aa72a-16a1-4638-9041-f14b562b942f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a56b44cfccd600e07127e6990ab831c430ef20730f2f6e40e53af87c1c4dfcda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "640f38a9-0f69-4c65-b760-e17c82ea3cb0", "node_type": "1", "metadata": {}, "hash": "29180b75b3d7642480b1e00eb0d883d9a179489b40c8473d815272e6741b95db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It receives the whole input sequence and then returns\nthe classi\ufb01cation result (on Fig 1 the whole input sequence has\nlength 6 with single sample feature size 3).\nIn a KWS application we do not know when the keyword\nstarts or ends, so we need to process every audio packet and\n1Preprint. Submitted to INTERSPEECH.\nFigure 1: Convolutional model: a) non-streaming b) streaming\nwith internal state c) streaming with external state\nreturn the classi\ufb01cation results in real time every 20ms (for ex-\nample) - it is called streaming inference. It will not be ef\ufb01-\ncient to use a non-streaming model representation (Fig 1a) in\nstreaming inference mode because the same convolution will\nbe recomputed on the input window multiple times. A stan-\ndard approach to optimize latency in this case is to transform\nthe non-streaming model (Fig 1 a) to a streaming one [4]. The\nresulting model will receive input samples with dimensions 3x1\nincrementally process it (keeping the previously computed con-\nvolutions in a buffer to avoid unnecessary computations) and\nreturn the classi\ufb01cation results in a streaming fashion, as shown\non Fig 1b, c.\nThere are two options of implementing streaming infer-\nence: with internal state (Fig 1b) and with external state\n(Fig 1c). A model with internal state receives a new input sam-\nple with dimensions 3x1 (grey box 3x1 on Fig 1b), appends it\nto the ring buffer State1i, and at the same time removes the old-\nest sample (marked by blue with dashed lines on State1i) from\nState1i. This way, State1i always has a shape of 3x3.", "mimetype": "text/plain", "start_char_idx": 2547, "end_char_idx": 4095, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "640f38a9-0f69-4c65-b760-e17c82ea3cb0": {"__data__": {"id_": "640f38a9-0f69-4c65-b760-e17c82ea3cb0", "embedding": null, "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50ce1c55-a6be-43e5-b962-03da124cd818", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "8a72fc46916812762edfe1a25fabb4b291c963d7c04449ab855dfa3b49172180", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b213d1105d4d280fbbb7e1c440fc94daf47b06cd23644ea3b647c4c27fb6dace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A model with internal state receives a new input sam-\nple with dimensions 3x1 (grey box 3x1 on Fig 1b), appends it\nto the ring buffer State1i, and at the same time removes the old-\nest sample (marked by blue with dashed lines on State1i) from\nState1i. This way, State1i always has a shape of 3x3. In the\nnext step State1i is used by convolution 3x3 (on Fig 1b). Then\nconv output (1x1 grey box) is concatenated with buffer State2i.\nAt the same time we remove the oldest sample (marked by blue\nwith dashed lines on State2i) from State2i. In the end, State2i\nis fed into a dense layer. As a result, the convolution will be\ncomputed on the new input data only and all previous compu-\ntations will be buffered, as shown on Fig 1 b. In this case states\nare internal variables of the model which have to be updated\nwith every prediction. The same approach is shown on Fig 1c,\nwith one difference: states State1e and State2e are inputs and\noutputs of the model. In this case model does not keep any in-\nternal states and developers will have to feed them into neural\nnetwork as additional inputs and then in addition to classi\ufb01ca-\ntion results receive the updated states and feed them back on the\nnext prediction cycle.\nIn the above example, model representation in training and\ninference models can be different, so developers have to reim-\nplement the NN model in streaming mode (for model latency\noptimization). In this work we would like to automate it, so that\narXiv:2005.06720v2  [eess.AS]  29 Jul 2020", "mimetype": "text/plain", "start_char_idx": 3799, "end_char_idx": 5299, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e3f9c73-9be0-4ccc-b52f-1346924d5d58": {"__data__": {"id_": "9e3f9c73-9be0-4ccc-b52f-1346924d5d58", "embedding": null, "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b857565a-9b31-49ad-8efc-750a044471e2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a2667bc14dc3aa99333aff1d423b19bcd3635a4810d60c329c2987f54e7d74aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc", "node_type": "1", "metadata": {}, "hash": "f5a66288133c0fbc6507d092746b9a6ff421d659a09a8a1ef230811b9a53aacd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "developers could write a NN model once, train it and then au-\ntomatically convert it to streaming mode. For this purpose we\ndesigned a Keras streaming wrapper layer, described in section\n2.\n1.2. About speech frontend and modeling pipeline\nKWS NN models use a speech feature extractor based on\nMFCC [6]. Our implementation of speech feature extractor sup-\nports both FFT and DFT. If DFT is selected, then it will increase\nmodel size, because of DFT weights. We implemented all the\ncomponents of the speech feature extractor in Keras layers, so\nthat they will be part of the Keras NN model and will be au-\ntomatically converted to TFLite [7]. Since the speech feature\nextractor is a part of the model it will be easier to deploy it on a\nmobile device. The overall modeling pipeline (with given data)\nwill have several steps:\n\u2022 Design a model using Keras layers and Stream wrapper.\n\u2022 Train the model.\n\u2022 Automatically convert the trained non-streaming Keras\nmodel to Keras streaming one.\n\u2022 Convert streaming Keras model to a TFLite module [7].\nQuantize it if needed to optimize model size and perfor-\nmance.\n\u2022 Benchmark TFLite module: measure accuracy and la-\ntency on CPU of mobile phone Pixel4 [8].\nThe main contributions of this paper are:\n\u2022 We implemented several popular KWS models in Keras\nand designed a streaming Keras layer wrapper for auto-\nmatic model conversion to streaming inference with in-\nternal/external states.\n\u2022 We improved the classi\ufb01cation error of the state of the\nart KWS model by 10% on datasets V2 [9].\n\u2022 We trained KWS NN models on Google speech com-\nmands dataset [10] and compared their accuracy with\nstreaming and non-streaming latency on a Pixel 4 phone.\n\u2022 The code, pretrained models and experimental results are\nopen-sourced and available at [11].\n2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc": {"__data__": {"id_": "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc", "embedding": null, "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b857565a-9b31-49ad-8efc-750a044471e2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a2667bc14dc3aa99333aff1d423b19bcd3635a4810d60c329c2987f54e7d74aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e3f9c73-9be0-4ccc-b52f-1346924d5d58", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "c623b07ee36dc908c17453597071beba569a6f635d482d384d96179e3fd5ba68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12032502-67b6-4840-af2b-8859c9b891b8", "node_type": "1", "metadata": {}, "hash": "18959bf9b9ecafe980b1c4ffa67e8d28454cc2d3cb6351c1539db3bc5680d10c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 We improved the classi\ufb01cation error of the state of the\nart KWS model by 10% on datasets V2 [9].\n\u2022 We trained KWS NN models on Google speech com-\nmands dataset [10] and compared their accuracy with\nstreaming and non-streaming latency on a Pixel 4 phone.\n\u2022 The code, pretrained models and experimental results are\nopen-sourced and available at [11].\n2. Model streaming\nWe designed a streaming Keras layer wrapper which allows\nautomatic conversion of Keras models to streaming inference\n(with internal or external states, Fig 1b, c). With this approach\ndeveloper does not need to manually rewrite the model for\nstreaming mode. This allows us to reduce the time to model\ndeployment. Below is an example of a functional Keras model:\noutput = tf.keras.layers.Conv2D(...)(input)\noutput = tf.keras.layers.Flatten(...)(output)\noutput = tf.keras.layers.Dense(...)(output)\nBy wrapping layers (which have to be streamed and require a\nbuffer as shown in Fig 1b, c) with the Stream wrapper we get\nthe model (in this example Dense layer does not keep any states\nin time, so it is streamable by default):\noutput = Stream(cell=tf.keras.layers.Conv2D(...))(input)\noutput = Stream(cell=tf.keras.layers.Flatten(...))(output)\noutput = tf.keras.layers.Dense(...)(output)\nNow we can train this model (with no impact on training\ntime) and convert it to streaming inference mode automatically.\nStream wrapper creates states and manages in inference mode.\nIn addition, the Stream wrapper needs to know the effective time\n\ufb01lter size which is different for different layers, that is why in-\nside of the Stream wrapper we have different logic for extracting\nthe effective time \ufb01lter size for a particular layer.\nWe designed the Stream wrapper with several requirements.", "mimetype": "text/plain", "start_char_idx": 1426, "end_char_idx": 3169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12032502-67b6-4840-af2b-8859c9b891b8": {"__data__": {"id_": "12032502-67b6-4840-af2b-8859c9b891b8", "embedding": null, "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b857565a-9b31-49ad-8efc-750a044471e2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a2667bc14dc3aa99333aff1d423b19bcd3635a4810d60c329c2987f54e7d74aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "82dcb86031d221e19d52ed1d626eb38e686a9cccd553b1862a9ae3c06e7586d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9961334-20f5-45a9-ad38-0dbef61f6bb9", "node_type": "1", "metadata": {}, "hash": "5eaf4fe1200f0787bb5abb4acaee00fc21191423fefc96ac18876f8e89995f5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stream wrapper creates states and manages in inference mode.\nIn addition, the Stream wrapper needs to know the effective time\n\ufb01lter size which is different for different layers, that is why in-\nside of the Stream wrapper we have different logic for extracting\nthe effective time \ufb01lter size for a particular layer.\nWe designed the Stream wrapper with several requirements.\nFirst, it shouldn\u2019t impact the original model training or default\nnon-streaming inference and will be used only for streaming\ninference. In training mode it uses the cell as it is, so the training\ntime will be the same as without the Stream wrapper. Next,\nwe would like to support cells with internal and external states\nas it is shown in Fig 1b (with internal state) and Fig 1c (with\nexternal state). So that we can use different inference engines\nand compare them with each other.\nRNN layers also require states during streaming inference,\nso we build streaming-aware RNN layers with streaming func-\ntion. It behaves as standard RNN during training, but after\nmodel conversion to streaming inference it will only call the\nRNN cell (with internal or external state de\ufb01ned by the user).\nAutomatic conversion to streaming inference mode has sev-\neral steps: 1) set input layer feature size equal one frame; 2)\ntraverse Keras NN representation and insert ring buffer for lay-\ners which have to be streamed or call streaming function in\nstreaming-aware layers such as RNN. This approach is not spe-\nci\ufb01c to KWS models and can be used in other applications.\nCurrent version of Stream wrapper does not support striding\nnor pooling more than 1 in the time dimension, but it can be\nimplemented in the future.\n3. Model architectures\nThe standard end to end model architecture applied for KWS\n[12, 14] consists of a speech feature extractor(optional) and a\nneural network based classi\ufb01er.", "mimetype": "text/plain", "start_char_idx": 2798, "end_char_idx": 4649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9961334-20f5-45a9-ad38-0dbef61f6bb9": {"__data__": {"id_": "f9961334-20f5-45a9-ad38-0dbef61f6bb9", "embedding": null, "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b857565a-9b31-49ad-8efc-750a044471e2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a2667bc14dc3aa99333aff1d423b19bcd3635a4810d60c329c2987f54e7d74aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12032502-67b6-4840-af2b-8859c9b891b8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "0fd7dd4610dec71a6a55471a3095ee6e58d10bb94f979b2e222ead3011da5ce2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c23e8af-3854-4dd6-96b7-7468324ce64f", "node_type": "1", "metadata": {}, "hash": "91b38ace33b33712678a09bb11b2a15e30567753484ad6895795832ab1b3e53f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This approach is not spe-\nci\ufb01c to KWS models and can be used in other applications.\nCurrent version of Stream wrapper does not support striding\nnor pooling more than 1 in the time dimension, but it can be\nimplemented in the future.\n3. Model architectures\nThe standard end to end model architecture applied for KWS\n[12, 14] consists of a speech feature extractor(optional) and a\nneural network based classi\ufb01er. An input audio signal of length\n1sec is framed into overlapping frames of length 40ms with an\noverlap of 20ms as in [5]. Each frame is fed into a speech\nfeature extractor which is based on MFCC [6]. The extracted\nfeatures are classi\ufb01ed by a neural network which produces the\nprobabilities of the output classes. We use cross entropy loss\nfunction with Adam optimizer for model training. In this sec-\ntion we overview the neural network architectures evaluated in\nthis work, We implemented popular models from [5], [27], [28],\nin our library for benchmarking streaming and non streaming\ninference on mobile phone.\n3.1. Deep Neural Network (DNN)\nThe DNN model applies fully-connected layers with recti\ufb01ed\nlinear unit (ReLU) activation function on every input speech\nfeature. Then the outputs are stacked over 49 frames and pro-\ncessed by a pooling layer followed by another sequence of fully-\nconnected layers with ReLU. We observed that this architecture\nwith a pooling layer gives higher accuracy than the standard\nDNN model published in [5], as shown in Table 1. We use a\nsimilar number of model parameters with DNN model in [5].\n3.2. Convolutional Neural Network (CNN)\nCNN [15] is a popular model which is used in many applica-\ntions including KWS [14,15,22].", "mimetype": "text/plain", "start_char_idx": 4240, "end_char_idx": 5911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c23e8af-3854-4dd6-96b7-7468324ce64f": {"__data__": {"id_": "6c23e8af-3854-4dd6-96b7-7468324ce64f", "embedding": null, "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b857565a-9b31-49ad-8efc-750a044471e2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a2667bc14dc3aa99333aff1d423b19bcd3635a4810d60c329c2987f54e7d74aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9961334-20f5-45a9-ad38-0dbef61f6bb9", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "da5339cf68b3d6e65c93c50d8b43b98f031c1db5f2b7be47da3d437731385dfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We observed that this architecture\nwith a pooling layer gives higher accuracy than the standard\nDNN model published in [5], as shown in Table 1. We use a\nsimilar number of model parameters with DNN model in [5].\n3.2. Convolutional Neural Network (CNN)\nCNN [15] is a popular model which is used in many applica-\ntions including KWS [14,15,22]. As in [5] it is composed of se-\nquence of 2D colvolutions with ReLU non linearities followed\nby fully connected layers in the end. Below we benchmarked\nseveral variations of CNN: one with striding equal 2 (CNN+strd\non Table 1 and Table 2) and another with no striding (CNN on\nTable 2, it can be automatically converted to streaming mode).", "mimetype": "text/plain", "start_char_idx": 5569, "end_char_idx": 6250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61d5de64-924b-48b9-8d09-de3c39535ea6": {"__data__": {"id_": "61d5de64-924b-48b9-8d09-de3c39535ea6", "embedding": null, "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7ecf0578215eb353e650e2995169b59b4dd7b2de4912f38ae0306ab92d4cceed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9", "node_type": "1", "metadata": {}, "hash": "8fc7bd9f39da1a2592ac9e1c6fe4cd3fcb1f7cd1d5d99e728a26e47a07617808", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Recurrent Neural Networks: LSTM, GRU\nRNNs [16] are successfully applied in KWS problems [17, 18].\nIt receives speech features as a sequence and processes it se-\nquentially with updating the internal states of the model. This\napproach is well suited for streaming inference mode. We\nexplore two versions of RNN: LSTM [19] (in Table 1 called\nLSTM) and a GRU-based model [20] and compare them with\nbaseline [5] on Table 1.\n3.4. Convolutional Recurrent Neural Network (CRNN)\nCRNN [17] combines properties of both CNN, which captures\nshort term dependencies, and RNN, which uses longer range\ncontext. It applies a set of 2D convolutions on speech features\nfollowed by a GRU layer with fully connected layers. We com-\npare CRNN model in our library with baseline [5] on Table 1.\n3.5. Depthwise Separable Convolutional Neural Network\n(DSCNN)\nDSCNN [21] models are well applied on KWS [5]. This model\nprocesses speech features by using a sequence of 2D convolu-\ntional and 2D depthwise layers followed by batch normalization\nwith average pooling (as in [5]) and \ufb01nished by applying fully\nconnected layers. Below we benchmarked several variations\nof DSCNN: one with striding equal 2 (it is similar to DSCNN\nin [5] and shown as DSCNN+strd on Table 1 and Table 2) and\nanother with no striding (DSCNN on Table 2, it can be automat-\nically converted to streaming mode).\n3.6. Multihead attention RNN (MHAtt-RNN)\nThe development of the Attention mechanism [24,25] improved\nthe accuracy on multiple tasks including KWS [27].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9": {"__data__": {"id_": "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9", "embedding": null, "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7ecf0578215eb353e650e2995169b59b4dd7b2de4912f38ae0306ab92d4cceed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61d5de64-924b-48b9-8d09-de3c39535ea6", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "4ba00da643e944c0e8358e5b171b1c98782f8ebf5ac92e23d434594716cb5758", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "882487fa-dd27-4ac4-abfb-9e131b99b8b3", "node_type": "1", "metadata": {}, "hash": "181b8c5b7f2615d475815a92ba992f796c72d29dc64e51ec003a6b5e8aa8540c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.6. Multihead attention RNN (MHAtt-RNN)\nThe development of the Attention mechanism [24,25] improved\nthe accuracy on multiple tasks including KWS [27]. In [27] the\nauthors build a model Att-RNN which takes a mel-scale spec-\ntrogram and convolves it with a set of 2D convolutions. Then\ntwo bidirectional LSTM [16] layers are used to capture two-\nway long term dependencies in the audio data. The feature in\nthe center of the bidirectional LSTM\u2019s output sequence is pro-\njected using a dense layer and is used as a query vector for the\nattention mechanism. Finally, the weighted (by attention score)\naverage of the bidirectional LSTM output is processed by a set\nof fully connected layers for classi\ufb01cation [27]. We extended\nthis approach with multi-head attention (4 heads) and replaced\nLSTM with GRU (and call this version MHAtt-RNN). It allows\nus to reduce classi\ufb01cation error by 10% in comparison to the\nstate of the art (shown in Table 1). Both Att-RNN and MHAtt-\nRNN models are using bidirectional RNN, so they cannot be\nconverted to streaming mode and have to receive the whole se-\nquence before producing classi\ufb01cation results.\n3.7. Singular value decomposition \ufb01lter (SVDF)\nWe implemented a simpli\ufb01ed version of [28], so that it does\nnot require aligned annotation of audio data for training. This\nmodel is composed of several SVDF and bottleneck layers with\none softmax layer in the end. The SVDF block is a sequence\nof one dimensional convolution and one dimensional depthwise\nconvolution layers [28].\n3.8. Temporal Convolution ResNet (TC-ResNet)\nThe TC-ResNet model [22] is composed of sequence of resid-\nual blocks which use one dimensional convolution.", "mimetype": "text/plain", "start_char_idx": 1362, "end_char_idx": 3026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "882487fa-dd27-4ac4-abfb-9e131b99b8b3": {"__data__": {"id_": "882487fa-dd27-4ac4-abfb-9e131b99b8b3", "embedding": null, "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7ecf0578215eb353e650e2995169b59b4dd7b2de4912f38ae0306ab92d4cceed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a4cf0389b94b4c8fe0b7857e6d3c16e8d4f8ce94ba8063397301e7fbb3b5f1bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "373d57d1-8de6-4d76-85df-0606c58f68b5", "node_type": "1", "metadata": {}, "hash": "cf09cd3c749681ddd019e5f1ae8b1e2a1dbadea1b7ee7c0b8dbf2dd6ddc79f09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This\nmodel is composed of several SVDF and bottleneck layers with\none softmax layer in the end. The SVDF block is a sequence\nof one dimensional convolution and one dimensional depthwise\nconvolution layers [28].\n3.8. Temporal Convolution ResNet (TC-ResNet)\nThe TC-ResNet model [22] is composed of sequence of resid-\nual blocks which use one dimensional convolution. In this paper\nwe use neural network topology called TC-ResNet14 [22]. To\nTable 1: Baseline models accuracy on data V1* and V2* with\npaper references and our models accuracy on data V1 and V2\nModels V1*[%] V1[%] V2*[%] V2[%]\nDNN 86.7 [5] 91.2 90.6\nCNN+strd 92.7 [5] 95.4 95.6\nSVDF 96.3 96.9\nDSCNN+strd 95.4 [5] 97.0 97.1\nGRU 94.7 [5] 96.6 97.2\nLSTM 94.8 [5] 96.9 97.5\nCRNN 95.0 [5] 97.0 97.5\nAtt-RNN 95.6 [27] 96.9 [27]\nTC-ResNet 96.6 [22] 97.1 97.4\nEmbed+head 97.7 [13]\nMHAtt-RNN 97.2 98.0\nimprove accuracy of this model we increase number of param-\neters from 305K to 365K and use SpecAugment [26]. Baseline\nTC-ResNet accuracy with its improvement on data sets V1 and\nV2 are shown on Table 1.\n4. Experimental results\n4.1.", "mimetype": "text/plain", "start_char_idx": 2662, "end_char_idx": 3749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "373d57d1-8de6-4d76-85df-0606c58f68b5": {"__data__": {"id_": "373d57d1-8de6-4d76-85df-0606c58f68b5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7ecf0578215eb353e650e2995169b59b4dd7b2de4912f38ae0306ab92d4cceed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "882487fa-dd27-4ac4-abfb-9e131b99b8b3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "a697dcc41d6592d6aa52e80791273523e44a59c9b4645dfdb8e9823007655a47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6b2c537-b1c4-4e47-9263-978274ca886e", "node_type": "1", "metadata": {}, "hash": "f2e74732bc41ff75e21fa2d94baa2f737c6ac089bafac138f62e1c94a2c5c427", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Baseline\nTC-ResNet accuracy with its improvement on data sets V1 and\nV2 are shown on Table 1.\n4. Experimental results\n4.1. Datasets and accuracy metrics\nOn Table 1 we compare the accuracy of published models with\nour implementation on a Google dataset V1 [29] and V2 [9].\nWe use the standard data set up from TensorFlow speech\ncommands example code, proposed at [10]. The NN is trained\non twelve labels: ten words \u201dyes\u201d, \u201dno\u201d, \u201dup\u201d, \u201ddown\u201d, \u201dleft\u201d,\n\u201dright\u201d, \u201don\u201d, \u201doff\u201d, \u201dstop\u201d, and \u201dgo\u201d with additional two\nwords: \u201csilence\u201d and \u201cunknown\u201d. The \u201dunknown\u201d category\ncontains remaining 20 keywords from the dataset. As in [5, 10]\nwe use an algorithm from [30] for splitting the data into train-\ning, validation and testing set with ratio 80:10:10. The length of\nthe one training speech sample is 1 sec, and the sampling rate is\n16kHz.\nAfter applying the standard data set up (described above)\non data sets V1 [29] we have 22246, 3093, 3081 samples for\ntraining validation and testing respectively. With data set V2 [9]\nwe have 36923, 4445, 4890 samples for training validation and\ntesting respectively.\nThe training data is augmented with:\n\u2022 time shift in range -100ms...100ms (as in [30], [5]);\n\u2022 signal resampling with resampling factor in range\n0.85...1.15;\n\u2022 background noise (as in [30], [5]);\n\u2022 frequency/time masking, based on SpecAugment [26]\n(except time warping).\nFor side-by-side comparison purposes we use classi\ufb01ca-\ntion accuracy metric as in [5].", "mimetype": "text/plain", "start_char_idx": 3627, "end_char_idx": 5083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6b2c537-b1c4-4e47-9263-978274ca886e": {"__data__": {"id_": "a6b2c537-b1c4-4e47-9263-978274ca886e", "embedding": null, "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7ecf0578215eb353e650e2995169b59b4dd7b2de4912f38ae0306ab92d4cceed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "373d57d1-8de6-4d76-85df-0606c58f68b5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "69bc8cd0975b26cb13417d9a4194cf009ef1921e7333c8a2976714ab3a7ac815", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The training data is augmented with:\n\u2022 time shift in range -100ms...100ms (as in [30], [5]);\n\u2022 signal resampling with resampling factor in range\n0.85...1.15;\n\u2022 background noise (as in [30], [5]);\n\u2022 frequency/time masking, based on SpecAugment [26]\n(except time warping).\nFor side-by-side comparison purposes we use classi\ufb01ca-\ntion accuracy metric as in [5]. It is calculated by running the\nmodel on the testing data, and comparing the classi\ufb01cation re-\nsult against the expected label.\n4.2. Comparison with baseline\nWe implemented popular KWS approaches DNN [5], CNN [5],\nLSTM [5], GRU [5], CRNN [5], DSCNN [5], TC-ResNet [22],\ndescribed above, using our library for benchmarking stream-\ning and non streaming models. We improved their accuracy on\ndatasets V1 and V2, as shown on Table 1, by applying SpecAug-\nment [26](except time warping) with hyper-parameters opti-\nmization of both neural net and speech feature extractor param-\neters. After model is trained on datasets V1/V2 we convert it", "mimetype": "text/plain", "start_char_idx": 4726, "end_char_idx": 5720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b149d58f-d752-422a-8418-8258206ff9cc": {"__data__": {"id_": "b149d58f-d752-422a-8418-8258206ff9cc", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf", "node_type": "1", "metadata": {}, "hash": "484047a6890fab9e94ac1afa893f207db1d4222f9fd68e7cd599499ba19ad56d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to TFLite format (to be able to run it on a mobile phone), then\nrun inference with TFLite and report its accuracy on Table 1\n(columns V1, V2). The baseline accuracy with reference to pa-\nper is shown on Table 1 (columns V1*, V2*).\nOne of the best KWS models is Embed+head [13]. It\nachieves the state of the art accuracy by using additional data\nsets from YouTube to train embedding layer. We introduced\nMHAtt-RNN model. It reduces classi\ufb01cation error on datasets\nV2 by 10% in comparison to Embed+head [13], as shown on\nTable 1. The cost of this improvement is MHAtt-RNN model\nhas two times more parameters than Embed+head [13]. An-\nother recently published promising approach is Matchbox [23],\nbut authors use different training testing data set up, so we could\nnot compare it side by side.\nIn addition we implemented and benchmarked SVDF\nmodel [28] on both data sets V1 and V2 shown at Table 1 and\ndemonstrated that it has good properties for streaming at Ta-\nble 2.\n4.3. Streaming and non-streaming latency with accuracy\nIn the real production environment we do not know neither the\nbeginning nor ending of the speech command produced by the\nuser. Also we need to provide real time responses for a good\nuser experience. As a result, the speech command detector is\nrunning in streaming mode by classifying every 20 milliseconds\n(for example) of the input audio stream.\nWe trained all the models on datasets V2 and converted\nDNN, CNN no stride, CRNN, DSCNN no stride, SVDF to a\nstreaming inference mode and benchmarked them on datasets\nV2. The models DSCNN with stride, CNN with stride and\nMHAtt-RNN are not streamable with our library.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf": {"__data__": {"id_": "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b149d58f-d752-422a-8418-8258206ff9cc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "66bac4dd57fdd91f093d5b0449bb73d1d950527397923120e303013c35f64da6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "943d269b-0be7-48ad-86a1-9ab8196db174", "node_type": "1", "metadata": {}, "hash": "010d0d7fa30f55f089c52cc17bb42e1e26254dc117240ee4c75f57a9f5fd1017", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We trained all the models on datasets V2 and converted\nDNN, CNN no stride, CRNN, DSCNN no stride, SVDF to a\nstreaming inference mode and benchmarked them on datasets\nV2. The models DSCNN with stride, CNN with stride and\nMHAtt-RNN are not streamable with our library. To emulate\nthe streaming environment during accuracy evaluation we did\nnot reset the RNN model states between testing sequences. We\nobserved up to 2x accuracy reduction on such models (also\nbaseline RNN models in Table 1 have the same issue). We ad-\ndressed it by re-training RNN models GRU, CRNN) with state-\nful argument=True [31]. The last state for each sample at index\ni in a batch will be used as initial state for the sample of index\ni in the following batch (the model will learn how to process\ncell states on its own). To distinguish such RNN models on Ta-\nble 2 we append (S). We observed accuracy reduction of state-\nfully trained models GRU (S) CRNN (S), shown on Table 2 in\ncomparison to their non statefully trained versions GRU, CRNN\nshown Table 1 (column V2).\nThe latency and accuracy of non-streaming and streaming\nmodels with the number of parameters are presented in Table 2.\nWe use a Pixel 4 mobile phone [8] and TFlite benchmarking\ntools [32] to measure the models latency. Non-streaming la-\ntency is the processing time of the whole 1 sec speech sequence\nby the non-streaming model representation. Streaming latency\nis the processing time of one audio frame by the streaming\nmodel (it receives 20ms of audio and returns classi\ufb01cation re-\nsult). Processing time includes both feature extraction and neu-\nral network classi\ufb01cation (end to end).", "mimetype": "text/plain", "start_char_idx": 1370, "end_char_idx": 3001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "943d269b-0be7-48ad-86a1-9ab8196db174": {"__data__": {"id_": "943d269b-0be7-48ad-86a1-9ab8196db174", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "c05b550061ca3d8f65a3161025aab999d5d9188275e47d940c5e83b91d0e9134", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f", "node_type": "1", "metadata": {}, "hash": "48ac4bc77ab5c93b5bdab69264a4e41fe561b55899873a89c8ab96117d4d719d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Non-streaming la-\ntency is the processing time of the whole 1 sec speech sequence\nby the non-streaming model representation. Streaming latency\nis the processing time of one audio frame by the streaming\nmodel (it receives 20ms of audio and returns classi\ufb01cation re-\nsult). Processing time includes both feature extraction and neu-\nral network classi\ufb01cation (end to end).\nIn Table 2 we observe that the most effective and accu-\nrate streaming models are SVDF, CRNN and GRU. Layers with\nstriding/pooling are not streamable in our library now, but it can\nbe implemented in the future. With support of striding/pooling\nin streaming mode, models such as TC-ResNet, Embed+head\nand Matchbox can be more preferable. The most accurate non-\nstreaming model is MHAtt-RNN. It is based on bidirectional\nLSTM, so non streamable by default.", "mimetype": "text/plain", "start_char_idx": 2632, "end_char_idx": 3456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f": {"__data__": {"id_": "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "943d269b-0be7-48ad-86a1-9ab8196db174", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "2cfd8b84e011c545ad7d6e6aed2d36da6d82a68ccbaa137d4c47270e8201ff2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "378b365c-395e-459a-961c-846bd5f2c727", "node_type": "1", "metadata": {}, "hash": "2e0e3aa15f33a6be1769dd8c914b60993f2848d4154fef43256ad79dcb9ff870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Layers with\nstriding/pooling are not streamable in our library now, but it can\nbe implemented in the future. With support of striding/pooling\nin streaming mode, models such as TC-ResNet, Embed+head\nand Matchbox can be more preferable. The most accurate non-\nstreaming model is MHAtt-RNN. It is based on bidirectional\nLSTM, so non streamable by default.\nTable 2 shows that the average latency ratio of non-\nTable 2: Accuracy on data V2 with latency and model size\nModels accuracy,\n[%]\nnon\nstream\nlatency,\n[ms]\nstream\nlatency,\n[ms]\nmodel\nsize,\n[K]\nDNN 90.6 4 0.6 447\nCNN+strd 95.6 6 N/I 529\nCNN 96.0 15 1.5 606\nGRU (S) 96.3 11 0.5 593\nCRNN (S) 96.5 9 0.5 467\nSVDF 96.9 5 0.6 354\nDSCNN 96.9 19 1.6 490\nDSCNN+strd 97.0 9 N/I 485\nTC-ResNet 97.4 5 N/I 365\nMHAtt-RNN 98.0 10 N/A 743\nstreaming to streaming convolutional models (CNN, SVDF,\nDSCNN) is around 10x. The same ratio for RNN models\n(GRU(S), CRNN(S)) is around 20x. We explain this difference\nby the fact that non-streaming RNN models still have to be exe-\ncuted sequentially frame by frame over all frames belonging to\n1 second of input audio, whereas non-streaming convolutions\ncan be computed over the whole sequence in one batch.", "mimetype": "text/plain", "start_char_idx": 3104, "end_char_idx": 4288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "378b365c-395e-459a-961c-846bd5f2c727": {"__data__": {"id_": "378b365c-395e-459a-961c-846bd5f2c727", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "bcafc4708e1453beaacd024bf602834d5ab9557d43199abfba193dabf6240aad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87ccae18-aff8-4c26-a145-0c3516a2818f", "node_type": "1", "metadata": {}, "hash": "c011e9bb74f30c59481261bf2cee6352e5ccbf34d3b3a074ac3dc10aea624c84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same ratio for RNN models\n(GRU(S), CRNN(S)) is around 20x. We explain this difference\nby the fact that non-streaming RNN models still have to be exe-\ncuted sequentially frame by frame over all frames belonging to\n1 second of input audio, whereas non-streaming convolutions\ncan be computed over the whole sequence in one batch. In an\nideal case this ratio has to be around 50x (there are 50 frames\nin one second). As a result, there are opportunities for latency\nspeed-up of streaming models: reduce memory allocations in\nthe ring buffers and enable support of internal state in the infer-\nence engine. At the same time the latency of non-streaming\nmodels also can be optimized further. Detailed pro\ufb01ling of\nnon-streaming models showed that the speech feature extrac-\ntion latency is around 3.7ms. It can be reduced by using FFT\nand model quantization (after enabling both we observe almost\n2x latency reduction). As expected non-streaming models with\nstriding are several times faster than the same model with no\nstriding: CNN and DSCNN with striding are two times faster\nthan the same models without striding.\n5. Conclusion\nWe built a library allowing end-to-end model conversion to\nstreaming inference on mobile phones using Keras and TFLite.\nThe converted model encapsulates speech feature extraction. It\nsimpli\ufb01es model deployment on mobile devices. We reduced\nclassi\ufb01cation error by 10% relative, in comparison to the state\nof the art models on datasets V2. It was achieved by extend-\ning the Att-RNN [27] model with multi-head attention and ap-\nplying SpecAugment [26].", "mimetype": "text/plain", "start_char_idx": 3958, "end_char_idx": 5537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87ccae18-aff8-4c26-a145-0c3516a2818f": {"__data__": {"id_": "87ccae18-aff8-4c26-a145-0c3516a2818f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67675d31-cf82-42b4-abbb-258c7dc5f285", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b561a3b3c1486759d40dfcddca1d2502f391c1ca0465365357a54e502188d37a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "378b365c-395e-459a-961c-846bd5f2c727", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "9ba31f727651d083a67132a9a504c4ec9409a9fa893aad9b889fd592faeb026c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The converted model encapsulates speech feature extraction. It\nsimpli\ufb01es model deployment on mobile devices. We reduced\nclassi\ufb01cation error by 10% relative, in comparison to the state\nof the art models on datasets V2. It was achieved by extend-\ning the Att-RNN [27] model with multi-head attention and ap-\nplying SpecAugment [26]. For benchmarking purpose we im-\nplemented several popular models using our streaming library\nand measured streaming and non streaming latency on a Pixel\n4 mobile phone and demonstrated different tradeoffs between\naccuracy and latency. All code with experimentation results are\nopen-sourced and available at [11].\n6. Acknowledgements\nThe authors would like to thank Pete Warden, Karolis Misiunas,\nYukun Zhu, Ben Vanik, Robert Suderman, Rohit Prabhavalkar,\nKevin Kilgour and BDI team for valuable discussions and sug-\ngestions.", "mimetype": "text/plain", "start_char_idx": 5207, "end_char_idx": 6063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68e7cb03-b125-45ff-96b4-3d7512506ad8": {"__data__": {"id_": "68e7cb03-b125-45ff-96b4-3d7512506ad8", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8237a993-2c64-4f41-972b-950fda4551d1", "node_type": "1", "metadata": {}, "hash": "0e4d78976ec888200149297760b4dfe00f497ddaeb468ce74025cd5e7233c13e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7. References\n[1] Y . Gong, L. Liu, M. Yang, and L. D. Bourdev, \u201cCompressing\ndeep convolutional networks using vector quantization,\u201d CoRR,\nvol. abs/1412.6115, 2014. [Online]. Available: http://arxiv.org/\nabs/1412.6115\n[2] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,\nH. Adam, and D. Kalenichenko, \u201cQuantization and training of\nneural networks for ef\ufb01cient integer-arithmetic-only inference,\u201d\n06 2018, pp. 2704\u20132713.\n[3] Y . He, T. Sainath et al, \u201cStreaming End-to-end Speech\nRecognition For Mobile Devices,\u201d CoRR, vol. abs/1811.06621,\n2018. [Online]. Available: https://arxiv.org/abs/1811.06621\n[4] A. Coucke, M. Chlieh, T. Gisselbrecht, D. Leroy, M. Poumeyrol,\nT. Lavril, \u201cEf\ufb01cient keyword spotting using dilated convolutions\nand gating,\u201d ICASSP 2019, [Online]. Available: https://arxiv.org/\nabs/1811.07684\n[5] Y . Zhang, N. Suda, L. Lai, and V . Chandra, \u201cHello edge: Keyword\nspotting on microcontrollers,\u201d CoRR, vol. abs/1711.07128, 2017.\n[Online].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8237a993-2c64-4f41-972b-950fda4551d1": {"__data__": {"id_": "8237a993-2c64-4f41-972b-950fda4551d1", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68e7cb03-b125-45ff-96b4-3d7512506ad8", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "f8ead11e06fe1a0c032796a6aec6591ad5f8151e675f5ba8e5dbb125ff600376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eabf95ad-ff91-48b8-b63a-52f4e460cf77", "node_type": "1", "metadata": {}, "hash": "7c4d5912df3b07f29779419ee51ee45c30a9977f1aa8b2441785fa2b29f8717b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Available: https://arxiv.org/\nabs/1811.07684\n[5] Y . Zhang, N. Suda, L. Lai, and V . Chandra, \u201cHello edge: Keyword\nspotting on microcontrollers,\u201d CoRR, vol. abs/1711.07128, 2017.\n[Online]. Available: http://arxiv.org/abs/1711.07128\n[6] S. B. Davis and P. Mermelstein, \u201cComparison of parametric repre-\nsentation for monosyllabic word recognition in continuously spo-\nken sentences,\u201dIEEE Transactions on Acoustics, Speech and Sig-\nnal Processing, vol. 28, no. 4, pp. 357\u2013366, 1980.\n[7] [Online]. Available: https://www.tensor\ufb02ow.org/lite\n[8] [Online]. Available: https://store.google.com/product/pixel 4\n[9] \u201cSpeech commands dataset v2.\u201d [Online]. Available: https:\n//storage.googleapis.com/download.tensor\ufb02ow.org/data/speech\ncommands v0.02.tar.gz\n[10] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary\nspeech recognition,\u201d CoRR, vol. abs/1804.03209, 2018. [Online].\nAvailable: http://arxiv.org/abs/1804.03209\n[11] [Online]. Available: https://github.com/google-research/google-\nresearch/tree/master/kws streaming\n[12] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword\nspotting using deep neural networks,\u201d in IEEE International\nConference on Acoustics, Speech and Signal Processing, ICASSP\n2014, Florence, Italy, May 4-9, 2014 . IEEE, 2014, pp.\n4087\u20134091. [Online].", "mimetype": "text/plain", "start_char_idx": 776, "end_char_idx": 2063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eabf95ad-ff91-48b8-b63a-52f4e460cf77": {"__data__": {"id_": "eabf95ad-ff91-48b8-b63a-52f4e460cf77", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8237a993-2c64-4f41-972b-950fda4551d1", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "fe6b2486fb5843965d0074edc7b07720197ef820da980c66cd4f4e1d6a3ad21f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92a799df-57fc-44a0-a904-36309541ed64", "node_type": "1", "metadata": {}, "hash": "386c1124f6375b2c0009a7db062ced113ab0b02684de9c1783973f791c394dd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE, 2014, pp.\n4087\u20134091. [Online]. Available: https://doi.org/10.1109/ICASSP.\n2014.6854370\n[13] J. Lin, K. Kilgour, D. Roblek, and M. Shari\ufb01, \u201cTraining\nKeyword Spotters with Limited and Synthesized Speech\nData,\u201d in IEEE International Conference on Acoustics, Speech\nand Signal Processing, ICASSP 2020, Barcelona, Spain .\nIEEE, 2020, pp. 7474\u20137478. [Online]. Available: https:\n//doi.org/10.1109/ICASSP40776.2020.9053193\n[14] T. N. Sainath and C. Parada, \u201cConvolutional neural networks for\nsmall-footprint keyword spotting,\u201d inINTERSPEECH 2015, 16th\nAnnual Conference of the International Speech Communication\nAssociation, Dresden, Germany, September 6-10, 2015. ISCA,\n2015, pp. 1478\u20131482. [Online]. Available: http://www.isca-\nspeech.org/archive/interspeech 2015/i15 1478.html\n[15] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, \u201cGradient-based\nlearning applied to document recognition,\u201d Proceedings of the\nIEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998.\n[16] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d\nNeural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[Online]. Available: https://doi.org/10.1162/neco.1997.9.8.1735\n[17] S.", "mimetype": "text/plain", "start_char_idx": 2027, "end_char_idx": 3178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92a799df-57fc-44a0-a904-36309541ed64": {"__data__": {"id_": "92a799df-57fc-44a0-a904-36309541ed64", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eabf95ad-ff91-48b8-b63a-52f4e460cf77", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "0b2d86d57c4bc619e5de3dcc6b6dec0f9c20956fbb379ab80c347d59847722a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cea040a4-31df-40ee-8f15-563dfc620e40", "node_type": "1", "metadata": {}, "hash": "5ccf4c8c53bc15e46a133d5df10e28f01a458cb392603cdbd435c396ef097220", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2278\u20132324, Nov 1998.\n[16] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d\nNeural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[Online]. Available: https://doi.org/10.1162/neco.1997.9.8.1735\n[17] S. \u00a8O. Arik, M. Kliegl, R. Child, J. Hestness, A. Gibiansky,\nC. Fougner, R. Prenger, and A. Coates, \u201cConvolutional recurrent\nneural networks for small-footprint keyword spotting,\u201d in\nInterspeech 2017, 18th Annual Conference of the International\nSpeech Communication Association, Stockholm, Sweden, August\n20-24, 2017 , [Online]. Available: https://arxiv.org/abs/1703.\n05390\n[18] M. Sun, A. Raju, G. Tucker, S. Panchapagesan, G. Fu,\nA. Mandal, S. Matsoukas, N. Strom, and S. Vitaladevuni,\n\u201cMax-pooling loss training of long short-term memory networks\nfor small-footprint keyword spotting,\u201d in 2016 IEEE Spoken\nLanguage Technology Workshop, SLT 2016, San Diego, CA, USA,\nDecember 13-16, 2016. IEEE, 2016, pp. 474\u2013480. [Online].\nAvailable: https://doi.org/10.1109/SLT.2016.7846306\n[19] F. A. Gers and J. Schmidhuber, \u201cLSTM recurrent networks\nlearn simple context-free and context-sensitive languages,\u201d IEEE\nTrans. Neural Networks, vol. 12, no. 6, pp. 1333\u20131340, 2001.", "mimetype": "text/plain", "start_char_idx": 2965, "end_char_idx": 4140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cea040a4-31df-40ee-8f15-563dfc620e40": {"__data__": {"id_": "cea040a4-31df-40ee-8f15-563dfc620e40", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92a799df-57fc-44a0-a904-36309541ed64", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "511a3100a906e53b8ca06c607b6f8ff82446c47d6ebd2e4ff03d563f9e58f8e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0de7b9d-4b9c-4207-b27a-df61da25339f", "node_type": "1", "metadata": {}, "hash": "5b54f8d99372de3a6bf735e94bf39cba0e5244606cd4dbb26523a839e243930d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "474\u2013480. [Online].\nAvailable: https://doi.org/10.1109/SLT.2016.7846306\n[19] F. A. Gers and J. Schmidhuber, \u201cLSTM recurrent networks\nlearn simple context-free and context-sensitive languages,\u201d IEEE\nTrans. Neural Networks, vol. 12, no. 6, pp. 1333\u20131340, 2001.\n[Online]. Available: https://doi.org/10.1109/72.963769\n[20] K. Cho, B. van Merrienboer, C \u00b8 . G \u00a8ulc \u00b8ehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y . Bengio, \u201cLearning phrase\nrepresentations using RNN encoder-decoder for statistical\nmachine translation,\u201d in Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP\n2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT,\na Special Interest Group of the ACL , [Online]. Available:\nhttps://arxiv.org/abs/1406.1078\n[21] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, \u201cMobilenets:\nEf\ufb01cient convolutional neural networks for mobile vision\napplications,\u201d CoRR, vol. abs/1704.04861, 2017. [Online].", "mimetype": "text/plain", "start_char_idx": 3883, "end_char_idx": 4876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0de7b9d-4b9c-4207-b27a-df61da25339f": {"__data__": {"id_": "f0de7b9d-4b9c-4207-b27a-df61da25339f", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cea040a4-31df-40ee-8f15-563dfc620e40", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "2d608d065d10a7622349a5ec967fb3cd5f71fdf1333dda24e21cb959c006f02d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c0426b4-04c1-48cc-bd80-135b942abe0c", "node_type": "1", "metadata": {}, "hash": "5b953f0c7642c3d8bafed40bc36ce2898de59b499134309cba6aab106d7e875d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "abs/1704.04861, 2017. [Online].\nAvailable: http://arxiv.org/abs/1704.04861\n[22] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim,\nS. Ha, \u201cTemporal Convolution for Real-time Keyword Spotting\non Mobile Devices,\u201d CoRR, vol. abs/1704.04861, 2019. [Online].\nAvailable: http://arxiv.org/abs/1904.03814\n[23] S. Majumdar, B. Ginsburg, \u201cMatchboxNet: 1D Time-Channel\nSeparable Convolutional Neural Network Architecture for Speech\nCommands Recognition,\u201dCoRR, vol. arXiv:2004.08531v2, 2020.\n[Online]. Available: https://arxiv.org/abs/2004.08531v2\n[24] D. Bahdanau, K. Cho, and Y . Bengio, \u201cNeural machine\ntranslation by jointly learning to align and translate,\u201d in 3rd\nInternational Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings. [Online]. Available: http://arxiv.org/abs/1409.0473\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you\nneed,\u201d inAdvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA, [Online].", "mimetype": "text/plain", "start_char_idx": 4845, "end_char_idx": 6003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c0426b4-04c1-48cc-bd80-135b942abe0c": {"__data__": {"id_": "4c0426b4-04c1-48cc-bd80-135b942abe0c", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0de7b9d-4b9c-4207-b27a-df61da25339f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "d4126f3e5063d82423ed92e0b6123e1d0c5eb7d7f93dc7d76191a823ba290df7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7347f1c-a447-47d5-9a63-fcbc8108c8fc", "node_type": "1", "metadata": {}, "hash": "76da8118479069ba34f9ff262ec589be978f77336651c5798324a41dd256c287", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Available: http://papers.nips.cc/paper/7181-attention-is-all-you-\nneed\n[26] D. Park, W. Chan, Y . Zhang, C. Chiu, B. Zoph, E. Cubuk and\nQ. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Interspeech 2019, [Online].\nAvailable: https://arxiv.org/pdf/1904.08779\n[27] D. C. de Andrade, S. Leo, M. L. D. S. Viana, and\nC. Bernkopf, \u201cA neural attention model for speech command\nrecognition,\u201d CoRR, vol. abs/1808.08929, 2018. [Online].\nAvailable: http://arxiv.org/abs/1808.08929\n[28] R. Alvarez and H. Park, \u201cEnd-to-end streaming keyword\nspotting,\u201d inIEEE International Conference on Acoustics, Speech\nand Signal Processing, ICASSP 2019, Brighton, United Kingdom,\nMay 12-17, 2019 . IEEE, 2019, pp. 6336\u20136340. [Online].\nAvailable: https://doi.org/10.1109/ICASSP.2019.8683557\n[29] \u201cSpeech commands dataset v1.\u201d [Online]. Available: http:\n//download.tensor\ufb02ow.org/data/speech commands v0.01.tar.gz\n[30] [Online]. Available: https://github.com/tensor\ufb02ow/tensor\ufb02ow/\nblob/master/tensor\ufb02ow/examples/speech commands/input data.\npy#L61\n[31] [Online]. Available: https://www.tensor\ufb02ow.org/api docs/python/\ntf/keras/layers/LSTM\n[32] [Online].", "mimetype": "text/plain", "start_char_idx": 6004, "end_char_idx": 7169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7347f1c-a447-47d5-9a63-fcbc8108c8fc": {"__data__": {"id_": "c7347f1c-a447-47d5-9a63-fcbc8108c8fc", "embedding": null, "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "b8569ad5af1e9f39b93b288439bf7da26ced29c3dad02fccd67d54e46336f209", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c0426b4-04c1-48cc-bd80-135b942abe0c", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}, "hash": "7314f6b3cfbbf2b847258ac084a7cc082acbc162606bafb98bd06bf5b5b97294", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Available: https://github.com/tensor\ufb02ow/tensor\ufb02ow/\nblob/master/tensor\ufb02ow/examples/speech commands/input data.\npy#L61\n[31] [Online]. Available: https://www.tensor\ufb02ow.org/api docs/python/\ntf/keras/layers/LSTM\n[32] [Online]. Available: https://github.com/tensor\ufb02ow/tensor\ufb02ow/\ntree/master/tensor\ufb02ow/lite/tools/benchmark", "mimetype": "text/plain", "start_char_idx": 6948, "end_char_idx": 7263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc9edbf6-0e73-4e9c-9a3e-389317270d4b": {"__data__": {"id_": "bc9edbf6-0e73-4e9c-9a3e-389317270d4b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "836f0f92-11db-468c-b612-489056a194ca", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "100c2abf2e60a771d5b304e8bed34f28bbb538692c2d90bdfc7183dab86a00cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93582014-c191-41b6-bac3-ea65c8b89adb", "node_type": "1", "metadata": {}, "hash": "218d788f3651cca8dcf1b1f6e4e7d50d73563c363483438fe56a5c952c126cf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\nLi Yuan1*, Yunpeng Chen2, Tao Wang1,3\u2217, Weihao Yu1, Yujun Shi1,\nZihang Jiang1, Francis E.H. Tay1, Jiashi Feng1, Shuicheng Yan1\n1 National University of Singapore 2 YITU Technology 3 Institute of Data Science, National University of Singapore\nyuanli@u.nus.edu, yunpeng.chen@yitu-inc.com, shuicheng.yan@gmail.com\nAbstract\nTransformers, which are popular for language modeling,\nhave been explored for solving vision tasks recently, e.g.,\nthe Vision Transformer (ViT) for image classi\ufb01cation. The\nViT model splits each image into a sequence of tokens with\n\ufb01xed length and then applies multiple Transformer layers\nto model their global relation for classi\ufb01cation. However,\nViT achieves inferior performance to CNNs when trained\nfrom scratch on a midsize dataset like ImageNet. We \ufb01nd\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample ef\ufb01ciency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for \ufb01xed com-\nputation budgets and limited training samples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93582014-c191-41b6-bac3-ea65c8b89adb": {"__data__": {"id_": "93582014-c191-41b6-bac3-ea65c8b89adb", "embedding": null, "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "836f0f92-11db-468c-b612-489056a194ca", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "100c2abf2e60a771d5b304e8bed34f28bbb538692c2d90bdfc7183dab86a00cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc9edbf6-0e73-4e9c-9a3e-389317270d4b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "cd18f10ce23950df0015e8c317f3f73b6fda95db0e67c34d3bf19510b8859c83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8057992a-fd5b-4ed1-9135-289212cf8a68", "node_type": "1", "metadata": {}, "hash": "7c0b594b5eab9732d17082dc16590d05125b803f6977a535ffdaff0e10204ed9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \ufb01nd\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample ef\ufb01ciency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for \ufb01xed com-\nputation budgets and limited training samples. To overcome\nsuch limitations, we propose a new Tokens-To-Token Vi-\nsion Transformer (T2T-ViT), which incorporates 1) a layer-\nwise Tokens-to-Token (T2T) transformation to progressively\nstructurize the image to tokens by recursively aggregating\nneighboring Tokens into one Token (Tokens-to-Token), such\nthat local structure represented by surrounding tokens can\nbe modeled and tokens length can be reduced; 2) an ef-\n\ufb01cient backbone with a deep-narrow structure for vision\ntransformer motivated by CNN architecture design after\nempirical study. Notably, T2T-ViT reduces the parameter\ncount and MACs of vanilla ViT by half, while achieving\nmore than 3.0% improvement when trained from scratch on\nImageNet. It also outperforms ResNets and achieves com-\nparable performance with MobileNets by directly training\non ImageNet. For example, T2T-ViT with comparable size\nto ResNet50 (21.5M parameters) can achieve 83.3% top1\naccuracy in image resolution 384\u00d7384 on ImageNet. 1\n1. Introduction\nSelf-attention models for language modeling like Trans-\nformers [37] have been recently applied to vision tasks,\nincluding image classi\ufb01cation [5, 12, 43], object detec-\n*Work done during an internship at Yitu Tech.", "mimetype": "text/plain", "start_char_idx": 847, "end_char_idx": 2395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8057992a-fd5b-4ed1-9135-289212cf8a68": {"__data__": {"id_": "8057992a-fd5b-4ed1-9135-289212cf8a68", "embedding": null, "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "836f0f92-11db-468c-b612-489056a194ca", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "100c2abf2e60a771d5b304e8bed34f28bbb538692c2d90bdfc7183dab86a00cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93582014-c191-41b6-bac3-ea65c8b89adb", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "16d0d5b9c4b7efd9d2f9feae2a02d141c0b3445c727c3bfffe4ee8a309b02be9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56aea8ef-7a53-4807-addc-21f3f1539210", "node_type": "1", "metadata": {}, "hash": "146141fc55231e95e694cfda1344dd9c4dcccbfe2b80e596e9d4f24e7a256400", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\n1. Introduction\nSelf-attention models for language modeling like Trans-\nformers [37] have been recently applied to vision tasks,\nincluding image classi\ufb01cation [5, 12, 43], object detec-\n*Work done during an internship at Yitu Tech.\n1Code: https://github.com/yitu-opensource/T2T-ViT\nFigure 1. Comparison between T2T-ViT with ViT, ResNets and\nMobileNets when trained from scratch on ImageNet. Left: per-\nformance curve of MACs vs. top-1 accuracy. Right: performance\ncurve of model size vs. top-1 accuracy.\ntion [3, 61] and image processing like denoising, super-\nresolution and deraining [4]. Among them, the Vision\nTransformer (ViT) [12] is the \ufb01rst full-transformer model\nthat can be directly applied for image classi\ufb01cation. In par-\nticular, ViT splits each image into14\u00d714 or 16\u00d716 patches\n(a.k.a., tokens) with \ufb01xed length; then following practice of\nthe transformer for language modeling, ViT applies trans-\nformer layers to model the global relation among these to-\nkens for classi\ufb01cation.\nThough ViT proves the full-transformer architecture is\npromising for vision tasks, its performance is still inferior\nto that of similar-sized CNN counterparts ( e.g. ResNets)\nwhen trained from scratch on a midsize dataset ( e.g., Im-\nageNet).", "mimetype": "text/plain", "start_char_idx": 2162, "end_char_idx": 3401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56aea8ef-7a53-4807-addc-21f3f1539210": {"__data__": {"id_": "56aea8ef-7a53-4807-addc-21f3f1539210", "embedding": null, "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "836f0f92-11db-468c-b612-489056a194ca", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "100c2abf2e60a771d5b304e8bed34f28bbb538692c2d90bdfc7183dab86a00cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8057992a-fd5b-4ed1-9135-289212cf8a68", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "61cd96155d39e771c33a88a67a8796490ef2de965fa354fe7892e8e7b77c42c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Though ViT proves the full-transformer architecture is\npromising for vision tasks, its performance is still inferior\nto that of similar-sized CNN counterparts ( e.g. ResNets)\nwhen trained from scratch on a midsize dataset ( e.g., Im-\nageNet). We hypothesize that such performance gap roots\nin two main limitations of ViT: 1) the straightforward tok-\nenization of input images by hard split makes ViT unable\nto model the image local structure like edges and lines,\nand thus it requires signi\ufb01cantly more training samples (like\nJFT-300M for pretraining) than CNNs for achieving similar\nperformance; 2) the attention backbone of ViT is not well-\ndesigned as CNNs for vision tasks, which contains redun-\ndancy and leads to limited feature richness and dif\ufb01culties\nin model training.\nTo verify our hypotheses, we conduct a pilot study to\ninvestigate the difference in the learned features of ViT-\nL/16 [12] and ResNet50 [15] through visualization in Fig. 2.\nWe observe the features of ResNet capture the desired local\narXiv:2101.11986v3  [cs.CV]  30 Nov 2021", "mimetype": "text/plain", "start_char_idx": 3159, "end_char_idx": 4212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e6c5ea1-c5d3-4c93-9488-e350660d73f4": {"__data__": {"id_": "0e6c5ea1-c5d3-4c93-9488-e350660d73f4", "embedding": null, "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "766b1c7a9fe54de6eeae47df010489b19ddd120c39b5fc51ec48010b2839464c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95", "node_type": "1", "metadata": {}, "hash": "8c0f7c454bfd8fe6708885595418628cb119d54ea3bdf9f11f198a178584c860", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ResNet50: conv1\nViT-L/16: block24\nResNet50: conv25\nViT-L/16: block12\nT2T-ViT-24: block12\nT2T-ViT-24: block24\nResNet50: conv49\nViT-L/16: block1\nT2T-ViT-24: T2T block1\nFigure 2. Feature visualization of ResNet50, ViT-L/16 [12] and our proposed T2T-ViT-24 trained on ImageNet. Green boxes highlight\nlearned low-level structure features such as edges and lines; red boxes highlight invalid feature maps with zero or too large values. Note the\nfeature maps visualized here for ViT and T2T-ViT are not attention maps, but image features reshaped from tokens. For better visualization,\nwe scale the input image to size 1024 \u00d7 1024 or 2048 \u00d7 2048.\nstructure (edges, lines, textures, etc.) progressively from the\nbottom layer (conv1) to the middle layer (conv25). How-\never, the features of ViT are quite different: the structure\ninformation is poorly modeled while the global relations\n(e.g., the whole dog) are captured by all the attention blocks.\nThese observations indicate that the vanilla ViT ignores the\nlocal structure when directly splitting images to tokens with\n\ufb01xed length. Besides, we \ufb01nd many channels in ViT have\nzero value (highlighted in red in Fig. 2), implying the back-\nbone of ViT is not ef\ufb01cient as ResNets and offers limited\nfeature richness when training samples are not enough.\nWe are then motivated to design a new full-transformer\nvision model to overcome above limitations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95": {"__data__": {"id_": "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95", "embedding": null, "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "766b1c7a9fe54de6eeae47df010489b19ddd120c39b5fc51ec48010b2839464c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e6c5ea1-c5d3-4c93-9488-e350660d73f4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "800d4707198f99249f1205ca13d6c47f91c646b917306e01bdd3a520a9d7ca67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cdd4e00-4c8f-411b-b39b-63f87193e09c", "node_type": "1", "metadata": {}, "hash": "217ffa09418707f65892597318b95610c58b3404abd8a59e19302d6ada55b64a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Besides, we \ufb01nd many channels in ViT have\nzero value (highlighted in red in Fig. 2), implying the back-\nbone of ViT is not ef\ufb01cient as ResNets and offers limited\nfeature richness when training samples are not enough.\nWe are then motivated to design a new full-transformer\nvision model to overcome above limitations. 1) Instead of\nthe naive tokenization used in ViT [12], we propose a pro-\ngressive tokenization module to aggregate neighboring To-\nkens to one Token (named Tokens-to-Token module), which\ncan model the local structure information of surrounding\ntokens and reduce the length of tokens iteratively. Speci\ufb01-\ncally, in each Token-to-Token (T2T) step, the tokens output\nby a transformer layer are reconstructed as an image ( re-\nstructurization) which is then split into tokens with over-\nlapping (soft split ) and \ufb01nally the surrounding tokens are\naggregated together by \ufb02attening the split patches. Thus\nthe local structure from surrounding patches is embedded\ninto the tokens to be input into the next transformer layer.\nBy conducting T2T iteratively, the local structure is aggre-\ngated into tokens and the length of tokens can be reduced\nby the aggregation process. 2) To \ufb01nd an ef\ufb01cient back-\nbone for vision transformers, we explore borrowing some\narchitecture designs from CNNs to build transformer lay-\ners for improving the feature richness, and we \ufb01nd \u201cdeep-\nnarrow\u201d architecture design with fewer channels but more\nlayers in ViT brings much better performance at compara-\nble model size and MACs (Multi-Adds).", "mimetype": "text/plain", "start_char_idx": 1078, "end_char_idx": 2609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cdd4e00-4c8f-411b-b39b-63f87193e09c": {"__data__": {"id_": "1cdd4e00-4c8f-411b-b39b-63f87193e09c", "embedding": null, "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "766b1c7a9fe54de6eeae47df010489b19ddd120c39b5fc51ec48010b2839464c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "b63bbec1b12dd844622eb2a49bda9f478cbb2d23930dab7db0e70717d2ca43cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86993457-55fe-40fc-a436-a84463a1ab36", "node_type": "1", "metadata": {}, "hash": "76d403858a2d6f203e85490eacab547dd72cb3ae096cb6bf2c7030c51101c624", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) To \ufb01nd an ef\ufb01cient back-\nbone for vision transformers, we explore borrowing some\narchitecture designs from CNNs to build transformer lay-\ners for improving the feature richness, and we \ufb01nd \u201cdeep-\nnarrow\u201d architecture design with fewer channels but more\nlayers in ViT brings much better performance at compara-\nble model size and MACs (Multi-Adds). Speci\ufb01cally, we\ninvestigate Wide-ResNets (shallow-wide vs deep-narrow\nstructure) [52], DenseNet (dense connection) [21], ResneXt\nstructure [44], Ghost operation [14, 59] and channel atten-\ntion [20]. We \ufb01nd among them, deep-narrow structure [52]\nis the most ef\ufb01cient and effective for ViT, reducing the pa-\nrameter count and MACs signi\ufb01cantly with nearly no degra-\ndation in performance. This also indicates the architecture\nengineering of CNNs can bene\ufb01t the backbone design of\nvision transformers.\nBased on the T2T module and deep-narrow backbone ar-\nchitecture, we develop the Tokens-to-Token Vision Trans-\nformer (T2T-ViT), which signi\ufb01cantly boosts the perfor-\nmance when trained from scratch on ImageNet (Fig. 1), and\nis more lightweight than the vanilla ViT. As shown in Fig. 1,\nour T2T-ViT with 21.5M parameters and 4.8G MACs can\nachieve 81.5% top-1 accuracy on ImageNet, much higher\nthan that of ViT [12] with 48.6M parameters and 10.1G\nMACs (78.1%).", "mimetype": "text/plain", "start_char_idx": 2259, "end_char_idx": 3569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86993457-55fe-40fc-a436-a84463a1ab36": {"__data__": {"id_": "86993457-55fe-40fc-a436-a84463a1ab36", "embedding": null, "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "766b1c7a9fe54de6eeae47df010489b19ddd120c39b5fc51ec48010b2839464c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cdd4e00-4c8f-411b-b39b-63f87193e09c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "b0aeaf1d9b3449ec3648797e19e81a0e4ebc9bf0e13f550a4f2e19caf8ea758e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1), and\nis more lightweight than the vanilla ViT. As shown in Fig. 1,\nour T2T-ViT with 21.5M parameters and 4.8G MACs can\nachieve 81.5% top-1 accuracy on ImageNet, much higher\nthan that of ViT [12] with 48.6M parameters and 10.1G\nMACs (78.1%). This result is also higher than the popu-\nlar CNNs of similar size, like ResNet50 with 25.5M param-\neters (76%-79%). Besides, we also design lite variants of\nT2T-ViT by simply adopting fewer layers, which achieve\ncomparable results with MobileNets [17, 32] (Fig. 1).\nTo sum up, our contributions are three-fold:\n\u2022 For the \ufb01rst time, we show by carefully designing\ntransformers architecture (T2T module and ef\ufb01cient\nbackbone), visual transformers can outperform CNNs\nat different complexities on ImageNet without pre-\ntraining on JFT-300M.\n\u2022 We develop a novel progressive tokenization for ViT\nand demonstrate its advantage over the simple tok-\nenization approach by ViT, and we propose a T2T\nmodule that can encode the important local structure\nfor each token.", "mimetype": "text/plain", "start_char_idx": 3326, "end_char_idx": 4330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "423ea5e4-3d44-49d8-8a8d-f035b1888e7e": {"__data__": {"id_": "423ea5e4-3d44-49d8-8a8d-f035b1888e7e", "embedding": null, "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6792247428e4025739a16583ffce28fcc6fcaa6cdf80fa7ed15931db32aab1db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc941632-bfc3-4337-aeaf-ed6fab3b518c", "node_type": "1", "metadata": {}, "hash": "dcd1dd32c0e41a603db181c25dc4a72ed660bd833c3f11ff3544b23a6747fa78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 We show the architecture engineering of CNNs can\nbene\ufb01t the backbone design of ViT to improve the fea-\nture richness and reduce redundancy. Through exten-\nsive experiments, we \ufb01nd deep-narrow architecture de-\nsign works best for ViT.\n2. Related Work\nTransformers in Vision Transformers [37] are the mod-\nels that entirely rely on the self-attention mechanism\nto draw global dependencies between input and output,\nand currently they have dominated natural language mod-\nelling [10, 30, 2, 46, 29, 23]. A transformer layer usu-\nally consists of a multi-head self-attention layer (MSA) and\nan MLP block. Layernorm (LN) is applied before each\nlayer and residual connections in both the self-attention\nlayer and MLP block. Recent works have explored apply-\ning transformers to various vision tasks: image classi\ufb01ca-\ntion [5, 12], object detection [3, 61, 58, 8, 34], segmen-\ntation [4, 40], image enhancement [4, 45], image genera-\ntion [27], video processing [60, 53], and 3D point cloud\nprocessing [56]. Among them, the Vision Transformer\n(ViT) proves that a pure Transformer architecture can also\nattain state-of-the-art performance on image classi\ufb01cation.\nHowever, ViT heavily relies on large-scale datasets such as\nImageNet-21k and JFT-300M (which is not publically avail-\nable) for model pretraining, requiring huge computation re-\nsources. In contrast, our proposed T2T-ViT is more ef\ufb01cient\nand can be trained on ImageNet without using those large-\nscale datasets.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc941632-bfc3-4337-aeaf-ed6fab3b518c": {"__data__": {"id_": "dc941632-bfc3-4337-aeaf-ed6fab3b518c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6792247428e4025739a16583ffce28fcc6fcaa6cdf80fa7ed15931db32aab1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423ea5e4-3d44-49d8-8a8d-f035b1888e7e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "60d09505db734f9f90cb91bb55a5313e1321de1bf659ff0d186addab72e04423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8667874a-42fb-49d0-86f7-492365e5f8ea", "node_type": "1", "metadata": {}, "hash": "aafdeee023b2bded55c1f6740148a3de55c36add550af9d3de1a5f4ec09c6038", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, ViT heavily relies on large-scale datasets such as\nImageNet-21k and JFT-300M (which is not publically avail-\nable) for model pretraining, requiring huge computation re-\nsources. In contrast, our proposed T2T-ViT is more ef\ufb01cient\nand can be trained on ImageNet without using those large-\nscale datasets. A recent concurrent work DeiT [36] applies\nKnowledge Distillation [16, 49] to improve the original ViT\nby adding a KD token along with the class token, which is\northogonal to our work, as our T2T-ViT focuses on the ar-\nchitecture design, and our T2T-ViT can achieve higher per-\nformance than DeiT without CNN as teacher model.\nSelf-attention in CNNs Self-attention mechanism has\nbeen widely applied to CNNs in vision task [38, 57, 19,\n47, 20, 39, 1, 6, 18, 31, 42, 13, 50, 48]. Among these\nworks, the SE block [20] applies attention to channel di-\nmensions and non-local networks [39] are designed for cap-\nturing long-range dependencies via global attention. Com-\npared with most of the works exploring global attention on\nimages [1, 42, 13, 39], some works [18, 31] also explore\nself-attention in a local patch to reduce the memory and\ncomputation cost. More recently, SAN [55] investigates\nboth pairwise and patchwise self-attention for image recog-\nnition, where the patchwise self-attention is a generalization\nof convolution.", "mimetype": "text/plain", "start_char_idx": 1157, "end_char_idx": 2500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8667874a-42fb-49d0-86f7-492365e5f8ea": {"__data__": {"id_": "8667874a-42fb-49d0-86f7-492365e5f8ea", "embedding": null, "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6792247428e4025739a16583ffce28fcc6fcaa6cdf80fa7ed15931db32aab1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc941632-bfc3-4337-aeaf-ed6fab3b518c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "e68aef5d1f0bd34bf934b9de22489f660c658d36ec7ecb5cdf7953ccc309785f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e50a6da-6de7-4dde-a84c-e539cbe7c797", "node_type": "1", "metadata": {}, "hash": "ac1cc8a966bc40726296f37611afdf8977a3295b75503bcc51f7435a824960c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Com-\npared with most of the works exploring global attention on\nimages [1, 42, 13, 39], some works [18, 31] also explore\nself-attention in a local patch to reduce the memory and\ncomputation cost. More recently, SAN [55] investigates\nboth pairwise and patchwise self-attention for image recog-\nnition, where the patchwise self-attention is a generalization\nof convolution. In this work, we also replace the T2T mod-\nule with multiple convolution layers in experiments and \ufb01nd\nthe convolution layers do not perform better than our de-\nsigned T2T module.\nReshape789456123 Unfold\n9\n3\n78\n12\n56\n4\nTokens to Token\nT2T  Transformer\nT2T  Transformer\nT2Tprocess\nTi\nTi\u2019\nIi Ti+1\nstep2: soft split1step1: re-structurizationnextT2T\n1245\n5689\nFigure 3. Illustration of T2T process. The tokens Ti are re-\nstructurized as an image Ii after transformation and reshaping;\nthen Ii is split with overlapping to tokensTi+1 again. Speci\ufb01cally,\nas shown in the pink panel, the four tokens (1,2,4,5) of the input\nIi are concatenated to form one token in Ti+1. The T2T trans-\nformer can be a normal Transformer layer [37] or other ef\ufb01cient\ntransformers like Performer layer [34] at limited GPU memory.\n3.", "mimetype": "text/plain", "start_char_idx": 2129, "end_char_idx": 3307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e50a6da-6de7-4dde-a84c-e539cbe7c797": {"__data__": {"id_": "6e50a6da-6de7-4dde-a84c-e539cbe7c797", "embedding": null, "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6792247428e4025739a16583ffce28fcc6fcaa6cdf80fa7ed15931db32aab1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8667874a-42fb-49d0-86f7-492365e5f8ea", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "c22125e20770f02794132978152e712f12467d84b59d0ff86193f4c1c22138ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c270f5c-fe1f-45fc-8755-3201adfa935b", "node_type": "1", "metadata": {}, "hash": "8e8875aa3a8b09851f4892f7745bc2647a66e5ce97ccb9f049994ddc5fc0eb8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Speci\ufb01cally,\nas shown in the pink panel, the four tokens (1,2,4,5) of the input\nIi are concatenated to form one token in Ti+1. The T2T trans-\nformer can be a normal Transformer layer [37] or other ef\ufb01cient\ntransformers like Performer layer [34] at limited GPU memory.\n3. Tokens-to-Token ViT\nTo overcome the limitations of simple tokenization and\ninef\ufb01cient backbone of ViT, we propose Tokens-to-Token\nVision Transformer (T2T-ViT) which can progressively to-\nkenize the image to tokens and has an ef\ufb01cient backbone.\nHence, T2T-ViT consists of two main components (Fig. 4):\n1) a layer-wise \u201cTokens-to-Token module\u201d (T2T module)\nto model the local structure information of the image and\nreduce the length of tokens progressively; 2) an ef\ufb01cient\n\u201cT2T-ViT backbone\u201d to draw the global attention relation\non tokens from the T2T module. We adopt a deep-narrow\nstructure for the backbone to reduce redundancy and im-\nprove the feature richness after exploring several CNN-\nbased architecture designs. We now explain these compo-\nnents one by one.\n3.1. Tokens-to-Token: Progressive Tokenization\nThe Token-to-Token (T2T) module aims to overcome the\nlimitation of simple tokenization in ViT. It progressively\nstructurizes an image to tokens and models the local struc-\nture information, and in this way the length of tokens can\nbe reduced iteratively. Each T2T process has two steps:Re-\nstructurization and Soft Split (SS) (Fig. 3).\nRe-structurization As shown in Fig.", "mimetype": "text/plain", "start_char_idx": 3037, "end_char_idx": 4494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c270f5c-fe1f-45fc-8755-3201adfa935b": {"__data__": {"id_": "2c270f5c-fe1f-45fc-8755-3201adfa935b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6792247428e4025739a16583ffce28fcc6fcaa6cdf80fa7ed15931db32aab1db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e50a6da-6de7-4dde-a84c-e539cbe7c797", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "ae0805aa4ea9ff693db764ad1608902357d03ebc0a4d18fc5623ba0a81cc829a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It progressively\nstructurizes an image to tokens and models the local struc-\nture information, and in this way the length of tokens can\nbe reduced iteratively. Each T2T process has two steps:Re-\nstructurization and Soft Split (SS) (Fig. 3).\nRe-structurization As shown in Fig. 3, given a sequence\nof tokens T from the preceding transformer layer, it will\nbe transformed by the self-attention block (the T2T trans-\nformer in Fig. 3):\nT\u2032= MLP(MSA(T)), (1)\nwhere MSA denotes the multihead self-attention operation\nwith layer normalization and \u201cMLP\u201d is the multilayer per-", "mimetype": "text/plain", "start_char_idx": 4218, "end_char_idx": 4786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ff7253e-c5ff-4c06-affc-88d34d7cdb95": {"__data__": {"id_": "4ff7253e-c5ff-4c06-affc-88d34d7cdb95", "embedding": null, "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "12f16b05ce790b71f31141db61b6c419ad00f037bd08495a6ace52fb47838337", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afddfe59-4e82-4684-b6fd-8379c37e512e", "node_type": "1", "metadata": {}, "hash": "5de03414daea3995c2c70407164a58c92c597c97986029b89a8fc522e648535e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ceptron with layer normalization in the standard Trans-\nformer [12]. Then the tokens T\u2032 will be reshaped as an\nimage in the spatial dimension,\nI = Reshape(T\u2032). (2)\nHere \u201cReshape\u201d re-organizes tokens T\u2032 \u2208 Rl\u00d7c to I \u2208\nRh\u00d7w\u00d7c, where l is the length of T\u2032, h, w, c are height,\nwidth and channel respectively, and l = h \u00d7w.\nSoft Split As shown in Fig. 3, after obtaining the re-\nstructurized image I, we apply the soft split on it to model\nlocal structure information and reduce length of tokens.\nSpeci\ufb01cally, to avoid information loss in generating tokens\nfrom the re-structurizated image, we split it into patches\nwith overlapping. As such, each patch is correlated with\nsurrounding patches to establish a prior that there should\nbe stronger correlations between surrounding tokens. The\ntokens in each split patch are concatenated as one token\n(Tokens-to-Token, Fig. 3), and thus the local information\ncan be aggregated from surrounding pixels and patches.\nWhen conducting the soft split, the size of each patch is\nk\u00d7k with s overlapping and p padding on the image, where\nk\u2212s is similar to the stride in convolution operation. So for\nthe reconstructed image I \u2208Rh\u00d7w\u00d7c, the length of output\ntokens To after soft split is\nlo =\n\u230ah + 2p \u2212k\nk \u2212s + 1\n\u230b\n\u00d7\n\u230aw + 2p \u2212k\nk \u2212s + 1\n\u230b\n. (3)\nEach split patch has size k \u00d7k \u00d7c.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1308, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afddfe59-4e82-4684-b6fd-8379c37e512e": {"__data__": {"id_": "afddfe59-4e82-4684-b6fd-8379c37e512e", "embedding": null, "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "12f16b05ce790b71f31141db61b6c419ad00f037bd08495a6ace52fb47838337", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ff7253e-c5ff-4c06-affc-88d34d7cdb95", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "29d2f6496f2fd4cc0da8079757381b0ee90b7b0b2aa197925d9bb1c838b3336f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1fe1997-ab22-4675-831d-eb9d5d5f7082", "node_type": "1", "metadata": {}, "hash": "e10d8e5c4514cde3d27dd58035fd4006c335943b9f20a95218ec6343c7d82fac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So for\nthe reconstructed image I \u2208Rh\u00d7w\u00d7c, the length of output\ntokens To after soft split is\nlo =\n\u230ah + 2p \u2212k\nk \u2212s + 1\n\u230b\n\u00d7\n\u230aw + 2p \u2212k\nk \u2212s + 1\n\u230b\n. (3)\nEach split patch has size k \u00d7k \u00d7c. We \ufb02atten all patches in\nspatial dimensions to tokens To \u2208Rlo\u00d7ck2\n. After the soft\nsplit, the output tokens are fed for the next T2T process.\nT2T module By conducting the above Re-structurization\nand Soft Split iteratively, the T2T module can progressively\nreduce the length of tokens and transform the spatial struc-\nture of the image. The iterative process in T2T module can\nbe formulated as\nT\u2032\ni = MLP(MSA(Ti),\nIi = Reshape(T\u2032\ni ),\nTi+1 = SS(Ii), i = 1...(n \u22121).\n(4)\nFor the input image I0, we apply a soft split at \ufb01rst to split\nit to tokens: T1 = SS(I0). After the \ufb01nal iteration, the\noutput tokens Tf of the T2T module has \ufb01xed length, so the\nbackbone of T2T-ViT can model the global relation onTf .\nAdditionally, as the length of tokens in the T2T module\nis larger than the normal case ( 16 \u00d716) in ViT, the MACs\nand memory usage are huge.", "mimetype": "text/plain", "start_char_idx": 1124, "end_char_idx": 2155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1fe1997-ab22-4675-831d-eb9d5d5f7082": {"__data__": {"id_": "e1fe1997-ab22-4675-831d-eb9d5d5f7082", "embedding": null, "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "12f16b05ce790b71f31141db61b6c419ad00f037bd08495a6ace52fb47838337", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afddfe59-4e82-4684-b6fd-8379c37e512e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4ddd76e2ddb351dfee579f59afe218173d79b19580863f339b868752bc47651c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d25d7e4-8ab1-4d26-b50d-d27364223436", "node_type": "1", "metadata": {}, "hash": "325afccc39b347950851546ee724af0371ec2e4bbda1b220541003653c6782a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After the \ufb01nal iteration, the\noutput tokens Tf of the T2T module has \ufb01xed length, so the\nbackbone of T2T-ViT can model the global relation onTf .\nAdditionally, as the length of tokens in the T2T module\nis larger than the normal case ( 16 \u00d716) in ViT, the MACs\nand memory usage are huge. To address the limitations, in\nour T2T module, we set the channel dimension of the T2T\nlayer small (32 or 64) to reduce MACs, and optionally adopt\nan ef\ufb01cient Transformer such as Performer [7] layer to re-\nduce memory usage at limited GPU memory. We provide an\nablation study on the difference between adopting standard\nTransformer layer and Performer layer in our experiments.\n3.2. T2T-ViT Backbone\nAs many channels in the backbone of vanilla ViT are in-\nvalid (Fig. 2), we plan to \ufb01nd an ef\ufb01cient backbone for our\nT2T-ViT to reduce the redundancy and improve the feature\nrichness. Thus we explore different architecture designs for\nViT and borrow some designs from CNNs to improve the\nbackbone ef\ufb01ciency and enhance the richness of the learned\nfeatures. As each transformer layer has skip connection as\nResNets, a straightforward idea is to apply dense connec-\ntion as DenseNet [21] to increase the connectivity and fea-\nture richness, or apply Wide-ResNets or ResNeXt structure\nto change the channel dimension and head number in the\nbackbone of ViT. We explore \ufb01ve architecture designs from\nCNNs to ViT:\n1. Dense connection as DenseNet [21];\n2. Deep-narrow vs.", "mimetype": "text/plain", "start_char_idx": 1869, "end_char_idx": 3319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d25d7e4-8ab1-4d26-b50d-d27364223436": {"__data__": {"id_": "1d25d7e4-8ab1-4d26-b50d-d27364223436", "embedding": null, "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "12f16b05ce790b71f31141db61b6c419ad00f037bd08495a6ace52fb47838337", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1fe1997-ab22-4675-831d-eb9d5d5f7082", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "a061b2a0e924e5f8c1dffa06d55da481fc109461f34f8efc54dff20ebead3159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f21446d-d189-4465-acf9-77d0866403a9", "node_type": "1", "metadata": {}, "hash": "6829a707da5de9c6da7d5d87eb880aefbdab289d5bbf999b21493a89a915e611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We explore \ufb01ve architecture designs from\nCNNs to ViT:\n1. Dense connection as DenseNet [21];\n2. Deep-narrow vs. shallow-wide structure as in Wide-\nResNets [52];\n3. Channel attention as Squeeze-an-Excitation (SE) Net-\nworks [20];\n4. More split heads in multi-head attention layer as\nResNeXt [44];\n5. Ghost operations as GhostNet [14].\nThe details of these structure designs in ViT are given in the\nappendix. We conduct extensive experiments on the struc-\ntures transferring in Sec. 4.2. We empirically \ufb01nd that 1)\nby adopting a deep-narrow structure that simply decreases\nchannel dimensions to reduce the redundancy in channels\nand increase layer depth to improve feature richness in ViT,\nboth the model size and MACs are decreased but perfor-\nmance is improved; 2) the channel attention as SE block\nalso improves ViT but is less effective than using the deep-\nnarrow structure.\nBased on these \ufb01ndings, we design a deep-narrow ar-\nchitecture for our T2T-ViT backbone.", "mimetype": "text/plain", "start_char_idx": 3209, "end_char_idx": 4174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f21446d-d189-4465-acf9-77d0866403a9": {"__data__": {"id_": "5f21446d-d189-4465-acf9-77d0866403a9", "embedding": null, "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "12f16b05ce790b71f31141db61b6c419ad00f037bd08495a6ace52fb47838337", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d25d7e4-8ab1-4d26-b50d-d27364223436", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "b9b1fa52486ce273a1b7cec7c51421cd1fd1c6d4faafb286e574ea8a59ff9b8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Based on these \ufb01ndings, we design a deep-narrow ar-\nchitecture for our T2T-ViT backbone. Speci\ufb01cally, it has a\nsmall channel number and a hidden dimension d but more\nlayers b. For tokens with \ufb01xed length Tf from the last layer\nof T2T module, we concatenate a class token to it and then\nadd Sinusoidal Position Embedding (PE) to it, the same as\nViT to do classi\ufb01cation:\nTf0 = [tcls; Tf ] +E, E \u2208R(l+1)\u00d7d\nTfi = MLP(MSA(Tfi\u22121)), i = 1...b\ny = fc(LN(Tfb ))\n(5)\nwhere E is Sinusoidal Position Embedding, LN is layer nor-\nmalization, fc is one fully-connected layer for classi\ufb01cation\nand y is the output prediction.", "mimetype": "text/plain", "start_char_idx": 4086, "end_char_idx": 4695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32181b2e-94d5-47ba-8d58-1714e8612b8f": {"__data__": {"id_": "32181b2e-94d5-47ba-8d58-1714e8612b8f", "embedding": null, "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6f592da66c1c89e8b75f833730d3ed36eee02fc6566d0e9a57e621e61716efe4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70edd82f-8056-44d1-81fa-67db0367dbbf", "node_type": "1", "metadata": {}, "hash": "817a759d1aa64aacebf598ba8bc849a8ba85b665ab92a4a9e7884b78f5502c0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fixed Tokens\nImage224 x 22477\nTransformer  layer\n+ PEclstoken\nclassMLPHead\nTransformer  layer\nT2T-ViT Backbone\nT1\nT2I0\nUnfold\nTokens-to-Token module\nT2T  Transformer\nT2T T2TT2T  Transformer\nTf\nFigure 4. The overall network architecture of T2T-ViT. In the T2T module, the input image is \ufb01rst soft split as patches, and then unfolded\nas a sequence of tokens T0. The length of tokens is reduced progressively in the T2T module (we use two iterations here and output Tf ).\nThen the T2T-ViT backbone takes the \ufb01xed tokens as input and outputs the predictions. The two T2T blocks are the same as Fig. 3 and PE\nis Position Embedding.\nTable 1. Structure details of T2T-ViT. T2T-ViT-14/19/24 have comparable model size with ResNet50/101/152. T2T-ViT-7/12 have com-\nparable model size with MobileNetV1/V2. For T2T transformer layer, we adopt Transformer layer for T2T-ViT t-14 and Performer layer\nfor T2T-ViT-14 at limited GPU memory. For ViT, \u2018S\u2019 means Small, \u2018B\u2019 is Base and \u2018L\u2019 is Large. \u2018ViT-S/16\u2019 is a variant from original\nViT-B/16 [12] with smaller MLP size and layer depth.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70edd82f-8056-44d1-81fa-67db0367dbbf": {"__data__": {"id_": "70edd82f-8056-44d1-81fa-67db0367dbbf", "embedding": null, "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6f592da66c1c89e8b75f833730d3ed36eee02fc6566d0e9a57e621e61716efe4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32181b2e-94d5-47ba-8d58-1714e8612b8f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "9e24dec869f919028c2f9639991b2502846cc72bdd4d2c13f04fb5924fa9401b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8ae14b5-c3f8-4638-be51-9bbc31f7971a", "node_type": "1", "metadata": {}, "hash": "7475c2eb1457b2b5f77b8f47920972344a5dbcdb41a4bf9009d5ba889aa20925", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For T2T transformer layer, we adopt Transformer layer for T2T-ViT t-14 and Performer layer\nfor T2T-ViT-14 at limited GPU memory. For ViT, \u2018S\u2019 means Small, \u2018B\u2019 is Base and \u2018L\u2019 is Large. \u2018ViT-S/16\u2019 is a variant from original\nViT-B/16 [12] with smaller MLP size and layer depth.\nModels\nTokens-to-Token module T2T-ViT backbone Model size\nT2T\ntransformer\nDepth Hidden\ndim\nMLP\nsize\nDepth Hidden\ndim\nMLP\nsize\nParams\n(M)\nMACs\n(G)\nViT-S/16 [12] - - - - 8 786 2358 48.6 10.1\nViT-B/16 [12] - - - - 12 786 3072 86.8 17.6\nViT-L/16 [12] - - - - 24 1024 4096 304.3 63.6\nT2T-ViT-14 Performer 2 64 64 14 384 1152 21.5 4.8\nT2T-ViT-19 Performer 2 64 64 19 448 1344 39.2 8.5\nT2T-ViT-24 Performer 2 64 64 24 512 1536 64.1 13.8\nT2T-ViTt-14 Transformer 2 64 64 14 384 1152 21.5 6.1\nT2T-ViT-7 Performer 2 64 64 8 256 512 4.2 1.1\nT2T-ViT-12 Performer 2 64 64 12 256 512 6.8 1.8\n3.3.", "mimetype": "text/plain", "start_char_idx": 796, "end_char_idx": 1653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8ae14b5-c3f8-4638-be51-9bbc31f7971a": {"__data__": {"id_": "e8ae14b5-c3f8-4638-be51-9bbc31f7971a", "embedding": null, "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6f592da66c1c89e8b75f833730d3ed36eee02fc6566d0e9a57e621e61716efe4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70edd82f-8056-44d1-81fa-67db0367dbbf", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "1e8fd79def5752f6672fb3a43ba3465ee519b0662481e893bdb330634c68e685", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99a51fa7-311d-4292-9437-18b5a44d652d", "node_type": "1", "metadata": {}, "hash": "73e59f72c0af3dd73144159c2a613b422048c7559be8b6b06405fc13405751ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "T2T-ViT Architecture\nThe T2T-ViT has two parts: the Tokens-to-Token (T2T)\nmodule and the T2T-ViT backbone (Fig. 4). There are var-\nious possible design choices for the T2T module. Here, we\nset n = 2as shown in Fig. 4, which means there isn+1 = 3\nsoft split and n = 2re-structurization in T2T module. The\npatch size for the three soft splits is P = [7, 3, 3], and the\noverlapping is S = [3, 1, 1], which reduces size of the input\nimage from 224 \u00d7224 to 14 \u00d714 according to Eqn. (3).\nThe T2T-ViT backbone takes tokens with \ufb01xed length\nfrom the T2T module as input, the same as ViT; but has\na deep-narrow architecture design with smaller hidden di-\nmensions (256-512) and MLP size (512-1536) than ViT. For\nexample, T2T-ViT-14 has 14 transformer layers in T2T-ViT\nbackbone with 384 hidden dimensions, while ViT-B/16 has\n12 transformer layers and 768 hidden dimensions, which is\n3x larger than T2T-ViT-14 in parameters and MACs.\nTo fairly compare with common hand-designed CNNs,\nwe make T2T-ViT models have comparable size with\nResNets and MobileNets. Speci\ufb01cally, we design three\nmodels: T2T-ViT-14, T2T-ViT-19 and T2T-ViT-24 of\ncomparable parameters with ResNet50, ResNet101 and\nResNet152 respectively.", "mimetype": "text/plain", "start_char_idx": 1654, "end_char_idx": 2853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99a51fa7-311d-4292-9437-18b5a44d652d": {"__data__": {"id_": "99a51fa7-311d-4292-9437-18b5a44d652d", "embedding": null, "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6f592da66c1c89e8b75f833730d3ed36eee02fc6566d0e9a57e621e61716efe4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8ae14b5-c3f8-4638-be51-9bbc31f7971a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "1a6dd24efe98f4d811129e0f588bc54b496919b7b0fecde6f62d14557c79b9a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To fairly compare with common hand-designed CNNs,\nwe make T2T-ViT models have comparable size with\nResNets and MobileNets. Speci\ufb01cally, we design three\nmodels: T2T-ViT-14, T2T-ViT-19 and T2T-ViT-24 of\ncomparable parameters with ResNet50, ResNet101 and\nResNet152 respectively. To compare with small models like\nMobileNets, we design two lite models: T2T-ViT-7, T2T-\nViT-12 with comparable model size with MibileNetV1 and\nMibileNetV2. The two lite TiT-ViT have no special designs\nor tricks like ef\ufb01cient convolution [26] and simply reduce\nthe layer depth, hidden dimension, and MLP ratio. The net-\nwork details are summarized in Tab. 1.\n4. Experiments\nWe conduct the following experiments with T2T-ViT for\nimage classi\ufb01cation on ImageNet. a) We validate the T2T-\nViT by training from scratch on ImageNet and compare it\nwith some common convolutional neural networks such as\nResNets and MobileNets of comparable size; we also trans-\nfer the pretrained T2T-ViT to downstream datasets such\nas CIFAR10 and CIFAR100 (Sec. 4.1). (b) We compare", "mimetype": "text/plain", "start_char_idx": 2578, "end_char_idx": 3613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59e60626-4fd3-4835-b905-084a1344928f": {"__data__": {"id_": "59e60626-4fd3-4835-b905-084a1344928f", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06a7078d-dc86-4d21-9bd2-bb2445d89fd8", "node_type": "1", "metadata": {}, "hash": "ef094fb252333ada9d8cae10dffd377d6cd96eb444667e6eba78b73ee299dfd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\ufb01ve T2T-ViT backbone architecture designs inspired from\nCNNs (Sec. 4.2). (c) We conduct ablation study to demon-\nstrate effects of the T2T module and the deep-narrow archi-\ntecture design of T2T-ViT (Sec. 4.3).\n4.1. T2T-ViT on ImageNet\nAll experiments are conducted on ImageNet dataset [9],\nwith around 1.3 million images in training set and 50k im-\nages in validation set. We use batch size 512 or 1024 with 8\nNVIDIA GPUs for training. We adopt Pytorch [28] library\nand Pytorch image models library (timm) [41] to implement\nour models and conduct all experiments. For fair compar-\nisons, we implement the same training scheme for the CNN\nmodels, ViT, and our T2T-ViT. Throughout the experiments\non ImageNet, we set default image size as224\u00d7224 except\nfor some speci\ufb01c cases on 384 \u00d7384, and adopt some com-\nmon data augmentation methods such as mixup [54] and\ncutmix [11, 51] for both CNN and ViT&T2T-ViT model\ntraining, because ViT models need more training data to\nreach reasonable performance. We train these models for\n310 epochs, using AdamW [25] as the optimizer and co-\nsine learning rate decay [24]. The details of experiment set-\nting are given in appendix. We also use both Transformer\nlayer and Performer layer in T2T module for our models, re-\nsulting in T2T-ViTt-14/19/24 (Transformer) and T2T-ViT-\n14/19/24 (Performer).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06a7078d-dc86-4d21-9bd2-bb2445d89fd8": {"__data__": {"id_": "06a7078d-dc86-4d21-9bd2-bb2445d89fd8", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59e60626-4fd3-4835-b905-084a1344928f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6adc37773c609e57dff8f6d890773d4a46cb1ebf03a7e2eb33be841d73d8ebaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c65cb3e1-dfa4-4d14-9a84-3fba479cda33", "node_type": "1", "metadata": {}, "hash": "86876c91ce347be7a031b418b93de6b89afc92c482e8056ab680d8fdd8c7a8a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We train these models for\n310 epochs, using AdamW [25] as the optimizer and co-\nsine learning rate decay [24]. The details of experiment set-\nting are given in appendix. We also use both Transformer\nlayer and Performer layer in T2T module for our models, re-\nsulting in T2T-ViTt-14/19/24 (Transformer) and T2T-ViT-\n14/19/24 (Performer).\nT2T-ViT vs. ViT We \ufb01rst compare performance of T2T-\nViT and ViT on ImageNet. The results are given in Tab. 2.\nOur T2T-ViT is much smaller than ViT in number of pa-\nrameters and MACs, yet giving higher performance. For\nexample, the small ViT model ViT-S/16 with 48.6M and\n10.1G MACs has 78.1% top-1 accuracy when trained from\nscratch on ImageNet, while our T2T-ViT t-14 with only\n44.2% parameters and 51.5% MACs achieves more than\n3.0% improvement (81.5%). If we compare T2T-ViT t-24\nwith ViT-L/16, the former reduces parameters and MACs\naround 500% but achieves more than 1.0% improvement\non ImageNet. Comparing T2T-ViT-14 with DeiT-small and\nDeiT-small-Distilled, our T2T-ViT can achieve higher accu-\nracy without large CNN models as teacher to enhance ViT.\nWe also adopt higher image resolution as 384\u00d7384 and get\n83.3% accuracy by our T2T-ViT-14\u2191384.\nT2T-ViT vs.", "mimetype": "text/plain", "start_char_idx": 998, "end_char_idx": 2200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c65cb3e1-dfa4-4d14-9a84-3fba479cda33": {"__data__": {"id_": "c65cb3e1-dfa4-4d14-9a84-3fba479cda33", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06a7078d-dc86-4d21-9bd2-bb2445d89fd8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "f00b7e6e8adb12e4554b3539cf897df590344433783bd2835641fb59ff363fe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d50bbab5-6cf5-43dc-a386-c49daac9d6c2", "node_type": "1", "metadata": {}, "hash": "260976fc04ec2b73a380fd93de732b71036cf23ab55e42dd8719e66f008b7217", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparing T2T-ViT-14 with DeiT-small and\nDeiT-small-Distilled, our T2T-ViT can achieve higher accu-\nracy without large CNN models as teacher to enhance ViT.\nWe also adopt higher image resolution as 384\u00d7384 and get\n83.3% accuracy by our T2T-ViT-14\u2191384.\nT2T-ViT vs. ResNet For fair comparisons, we set up\nthree T2T-ViT models that have similar model size and\nMACs with ResNet50, ResNet101 and ResNet152. The ex-\nperimental results are given in Tab. 3. The proposed T2T-\nViT achieves 1.4%-2.7% performance gain over ResNets\nwith similar model size and MACs. For example, compared\nwith ResNet50 of 25.5M parameters and 4.3G MACs, our\nT2T-ViT-14 have 21.5M parameters and 4.8G MACs obtain\n81.5% accuracy on ImageNet.\nT2T-ViT vs. MobileNets The T2T-ViT-7 and T2T-ViT-\n12 have similar model size with MobileNetV1 [17] and Mo-\nTable 2. Comparison between T2T-ViT and ViT by training from\nscratch on ImageNet.", "mimetype": "text/plain", "start_char_idx": 1937, "end_char_idx": 2837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d50bbab5-6cf5-43dc-a386-c49daac9d6c2": {"__data__": {"id_": "d50bbab5-6cf5-43dc-a386-c49daac9d6c2", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c65cb3e1-dfa4-4d14-9a84-3fba479cda33", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "f5711284092e8a40558bd53cad253cdda1ab0bbe10551423ffba5186628aac55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a524821-8bb5-44af-9a3b-4846d0b0767d", "node_type": "1", "metadata": {}, "hash": "0bb0425d8225d02a6297964826ec3d680e8147302947f52171eb69b047feed31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "T2T-ViT vs. MobileNets The T2T-ViT-7 and T2T-ViT-\n12 have similar model size with MobileNetV1 [17] and Mo-\nTable 2. Comparison between T2T-ViT and ViT by training from\nscratch on ImageNet.\nModels Top1-Acc (%) Params\n(M)\nMACs\n(G)\nViT-S/16 [12] 78.1 48.6 10.1\nDeiT-small [36] 79.9 22.1 4.6\nDeiT-small-Distilled [36] 81.2 22.1 4.7\nT2T-ViT-14 81.5 21.5 4.8\nT2T-ViT-14\u2191384 83.3 21.5 17.1\nViT-B/16 [12] 79.8 86.4 17.6\nViT-L/16 [12] 81.1 304.3 63.6\nT2T-ViT-24 82.3 64.1 13.8\nTable 3. Comparison between our T2T-ViT with ResNets on Im-\nageNet. T2T-ViT t-14: using Transformer in T2T module. T2T-\nViT-14: using Performer in T2T module. * means we train the\nmodel with our training scheme for fair comparisons.", "mimetype": "text/plain", "start_char_idx": 2649, "end_char_idx": 3349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a524821-8bb5-44af-9a3b-4846d0b0767d": {"__data__": {"id_": "1a524821-8bb5-44af-9a3b-4846d0b0767d", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d50bbab5-6cf5-43dc-a386-c49daac9d6c2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "f17ea8eeec7d2bb3aac120f73cf9ef603f82c16496fc6044bc223737e9585027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d234997-a658-420a-a333-21fe3536aeb3", "node_type": "1", "metadata": {}, "hash": "4d04e566ed3d6b9f6c5a0ca56e185867dba6a3eac8d4cbfccaaac580b2224e1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison between our T2T-ViT with ResNets on Im-\nageNet. T2T-ViT t-14: using Transformer in T2T module. T2T-\nViT-14: using Performer in T2T module. * means we train the\nmodel with our training scheme for fair comparisons.\nModels Top1-Acc (%) Params\n(M)\nMACs\n(G)\nResNet50 [15] 76.2 25.5 4.3\nResNet50* 79.1 25.5 4.3\nT2T-ViT-14 81.5 21.5 4.8\nT2T-ViTt-14 81.7 21.5 6.1\nResNet101 [15] 77.4 44.6 7.9\nResNet101* 79.9 44.6 7.9\nT2T-ViT-19 81.9 39.2 8.5\nT2T-ViTt-19 82.2 39.2 9.8\nResNet152 [15] 78.3 60.2 11.6\nResNet152* 80.8 60.2 11.6\nT2T-ViT-24 82.3 64.1 13.8\nT2T-ViTt-24 82.6 64.1 15.0\nbileNetV2 [32], but achieve comparable or higher perfor-\nmance than MobileNets (Tab. 4).", "mimetype": "text/plain", "start_char_idx": 3126, "end_char_idx": 3795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d234997-a658-420a-a333-21fe3536aeb3": {"__data__": {"id_": "8d234997-a658-420a-a333-21fe3536aeb3", "embedding": null, "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "503b89ab9aefdcd84f087c7751a21632df8b1d068c5c158d320283efbf509f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a524821-8bb5-44af-9a3b-4846d0b0767d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "353e886b79875fdcd536c381235a82ff39c371da90ce887e6bca47a6fff17d76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4). For example, Our T2T-\nViT-12 with 6.9M parameters achieves 76.5% top1 accu-\nracy, which is higher than MobileNetsV21.4x by 0.9%. But\nwe also note the MACs of our T2T-ViT are still larger than\nMobileNets because of the dense operations in Transform-\ners. However, there are no special operations or tricks like\nef\ufb01cient convolution [26, 32] in current T2T-ViT-7 and T2T-\nViT-12, and we only reduce model size by reducing the hid-\nden dimension, MLP ratio and depth of layers, indicating\nT2T-ViT is also very promising as a lite model. We also ap-\nply knowledge distillation on our T2T-ViT as the concurrent\nwork DeiT [36] and \ufb01nd that our T2T-ViT-7 and T2T-ViT-\n12 can be further improved by distillation. Overall, the ex-\nperimental results show, our T2T-ViT can achieve superior\nperformance when it has mid-size as ResNets and reason-\nable results when it has a small model size as MobileNets.\nTransfer learning We transfer our pretrained T2T-ViT to\ndownstream datasets such as CIFAR10 and CIFAR100. We", "mimetype": "text/plain", "start_char_idx": 3792, "end_char_idx": 4799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bccf927f-2bd6-40b6-868c-0c4cc7193ad2": {"__data__": {"id_": "bccf927f-2bd6-40b6-868c-0c4cc7193ad2", "embedding": null, "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6465db16-388e-4280-9be3-b7bb772a2365", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "88d9f4c0b0fda43ddbc7990d55107b206786f96fb97075efca5c5eb2589fe433", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51d8b82b-51be-4501-8db0-f60bf3a9b389", "node_type": "1", "metadata": {}, "hash": "2b538c092c312adafdc382ec47e9a89bedd75a2ed6c1a9dc61e25ff135f03700", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4. Comparison between our lite T2T-ViT with MobileNets.\nModels with \u2019-Distilled\u2019 are taught by teacher model with the\nmethod as DeiT [36].\nModels Top1-Acc (%) Params\n(M)\nMACs\n(G)\nMobileNetV1 1.0x* 70.8 4.2 0.6\nT2T-ViT-7 71.7 4.3 1.1\nT2T-ViT-7-Distilled 73.1 4.3 1.1\nMobileNetV2 1.0x* 72.8 3.5 0.3\nMobileNetV2 1.4x* 75.6 6.9 0.6\nMobileNetV3 (Searched) 75.2 5.4 0.2\nT2T-ViT-12 76.5 6.9 1.8\nT2T-ViT-12-Distilled 77.4 6.9 1.9\nTable 5. The results of \ufb01ne-tuning the pretrained T2T-ViT to down-\nstream datasets: CIFAR10 and CIFAR100.\nModels Params (M) ImageNet CIFAR10 CIFAR100\nViT/S-16 48.6 78.1 97.1 87.1\nT2T-ViT-14 21.5 81.5 97.5 88.4\nT2T-ViT-19 39.1 81.9 98.3 89.0\n\ufb01netune the pretrained T2T-ViT-14/19 with 60 epochs by\nusing SGD optimizer and cosine learning rate decay.The\nresults are given in Tab.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51d8b82b-51be-4501-8db0-f60bf3a9b389": {"__data__": {"id_": "51d8b82b-51be-4501-8db0-f60bf3a9b389", "embedding": null, "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6465db16-388e-4280-9be3-b7bb772a2365", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "88d9f4c0b0fda43ddbc7990d55107b206786f96fb97075efca5c5eb2589fe433", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bccf927f-2bd6-40b6-868c-0c4cc7193ad2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0c7fc8676f7a2f13ad80318e42181c8ebbcb464ad9376ed733e39a2d20fd1104", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acfca739-f62a-447c-b84a-aac58ac59b1d", "node_type": "1", "metadata": {}, "hash": "5c06c19fe07be7750c11a19eb020e943d427a7939a6a77cdee8f222df50045f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. We \ufb01nd that our T2T-ViT\ncan achieve higher performance than the original ViT with\nsmaller model sizes on the downstream datasets.\n4.2. From CNN to ViT\nTo \ufb01nd an ef\ufb01cient backbone for vision transformers,\nwe experimentally apply DenseNet structure, Wide-ResNet\nstructure (wide or narrow channel dimensions), SE block\n(channel attention), ResNeXt structure (more heads in mul-\ntihead attention), and Ghost operation from CNN to ViT.\nThe details of these architecture designs are given in the\nappendix. From experimental results on \u201cCNN to ViT\u201d in\nTab. 6, we can \ufb01nd both SE (ViT-SE) and Deep-Narrow\nstructure (ViT-DN) bene\ufb01t the ViT but the most effective\nstructure is deep-narrow structure, which decreases model\nsize and MACs nearly 2x and brings 0.9% improvement on\nthe baseline model ViT-S/16.\nWe further apply these structures from CNN to our T2T-\nViT, and conduct experiments on ImageNet under the same\ntraining scheme. We take ResNet50 as the baseline for\nCNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2T-ViT. All\nexperimental results are given in Tab. 6, and those on CNN\nand ViT&T2T-ViT are marked with the same colors. We\nsummarize the effects of each CNN-based structure below.\nDeep-narrow structure bene\ufb01ts ViT: The models ViT-\nDN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab.", "mimetype": "text/plain", "start_char_idx": 805, "end_char_idx": 2093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acfca739-f62a-447c-b84a-aac58ac59b1d": {"__data__": {"id_": "acfca739-f62a-447c-b84a-aac58ac59b1d", "embedding": null, "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6465db16-388e-4280-9be3-b7bb772a2365", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "88d9f4c0b0fda43ddbc7990d55107b206786f96fb97075efca5c5eb2589fe433", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51d8b82b-51be-4501-8db0-f60bf3a9b389", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "daf139c260fb0df6a61d33b6d61707acf9d9acdd947922f6d3b360cb8992a86d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e53b97-815f-4e82-acfa-2842a1acd529", "node_type": "1", "metadata": {}, "hash": "b3a4c531e19f09114894aef07df81e10a4033cc4857f64627931af275de8d841", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All\nexperimental results are given in Tab. 6, and those on CNN\nand ViT&T2T-ViT are marked with the same colors. We\nsummarize the effects of each CNN-based structure below.\nDeep-narrow structure bene\ufb01ts ViT: The models ViT-\nDN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab. 6\nare two opposite designs in channel dimension and layer\ndepth, where ViT-DN has 384 hidden dimensions and 16\nlayers and ViT-SW has 1,024 hidden dimensions and 4 lay-\ners. Compared with the baseline model ViT-S/16 with 768\nhidden dimensions and 8 layers, shallow-wide model ViT-\nSW has 8.2% decrease in performance while ViT-DN with\nonly half of model size and MACs achieve 0.9% increase.\nThese results validate our hypothesis that vanilla ViT with\nshallow-wide structure is redundant in channel dimensions\nand limited feature richness with shallow layers.\nDense connection hurts performance of both ViT and\nT2T-ViT: Compared with the ResNet50, DenseNet201\nhas smaller parameters and comparable MACs, while it has\nhigher performance. However, the dense connection can\nhurt performance of ViT-Dense and T2T-ViT-Dense (dark\nblue rows in Tab. 6).\nSE block improves both ViT and T2T-ViT: From red\nrows in Tab. 6, we can \ufb01nd SENets, ViT-SE and T2T-ViT-\nSE are higher than the corresponding baseline. The SE\nmodule can improve performance on both CNN and ViT,\nwhich means applying attention to channels bene\ufb01ts both\nCNN and ViT models.", "mimetype": "text/plain", "start_char_idx": 1820, "end_char_idx": 3224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82e53b97-815f-4e82-acfa-2842a1acd529": {"__data__": {"id_": "82e53b97-815f-4e82-acfa-2842a1acd529", "embedding": null, "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6465db16-388e-4280-9be3-b7bb772a2365", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "88d9f4c0b0fda43ddbc7990d55107b206786f96fb97075efca5c5eb2589fe433", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acfca739-f62a-447c-b84a-aac58ac59b1d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "8c410bad07828da7162fcc9f6c9fd286abe2a66cd5dd6eafaf958e8f08909479", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18dca209-9d59-47fd-a55d-66fefd858a26", "node_type": "1", "metadata": {}, "hash": "8f2e9d8ffc795b9802f4a36cd457ab05c646dfbfd3ef9b897a8c7ea16e03d508", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6).\nSE block improves both ViT and T2T-ViT: From red\nrows in Tab. 6, we can \ufb01nd SENets, ViT-SE and T2T-ViT-\nSE are higher than the corresponding baseline. The SE\nmodule can improve performance on both CNN and ViT,\nwhich means applying attention to channels bene\ufb01ts both\nCNN and ViT models.\nResNeXt structure has few effects on ViT and T2T-ViT:\nResNeXts adopt multi-head on ResNets, while Transform-\ners are also multi-head attention structure. When we adopt\nmore heads like 32, we can \ufb01nd it has few effects on per-\nformance (red rows in Tab 6). However, adopting a large\nnumber of heads makes the GPU memory large, which is\nthus unnecessary in ViT and T2T-ViT.\nGhost can further compress model and reduce MACs of\nT2T-ViT: Comparing experimental results of Ghost op-\neration (magenta row in Tab. 6), the accuracy decreases\n2.9% on ResNet50, 2.0% on T2T-ViT, and 4.4% on ViT.\nSo the Ghost operation can further reduce the parameters\nand MACs of T2T-ViT with smaller performance degrada-\ntion than ResNet. But for the original ViT, it would cause\nmore decrease than ResNet.\nBesides, for all \ufb01ve structures, the T2T-ViT performs\nbetter than ViT, which further validates the superiority of\nour proposed T2T-ViT. And we also wish this study of trans-\nferring CNN structure to ViT can motivate the network de-\nsign of Transformers in vision tasks.\n4.3.", "mimetype": "text/plain", "start_char_idx": 2935, "end_char_idx": 4281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18dca209-9d59-47fd-a55d-66fefd858a26": {"__data__": {"id_": "18dca209-9d59-47fd-a55d-66fefd858a26", "embedding": null, "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6465db16-388e-4280-9be3-b7bb772a2365", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "88d9f4c0b0fda43ddbc7990d55107b206786f96fb97075efca5c5eb2589fe433", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e53b97-815f-4e82-acfa-2842a1acd529", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "83628fc9190cbfd570c05747b3ce773c3f89da88b49102ff8f17097fc80679d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But for the original ViT, it would cause\nmore decrease than ResNet.\nBesides, for all \ufb01ve structures, the T2T-ViT performs\nbetter than ViT, which further validates the superiority of\nour proposed T2T-ViT. And we also wish this study of trans-\nferring CNN structure to ViT can motivate the network de-\nsign of Transformers in vision tasks.\n4.3. Ablation study\nTo further identify effects of T2T module and deep-\nnarrow structure, we do ablation study on our T2T-ViT.\nT2T module To verify the effects of the proposed T2T\nmodule, we experimentally compare three different models:\nT2T-ViT-14, T2T-ViT-14wo T2T , and T2T-ViTt-14, where\nT2T-ViT-14wo T2T has the same T2T-ViT backbone but\nwithout T2T module. We can \ufb01nd with similar model size\nand MACs, the T2T module can improve model perfor-\nmance by 2.0%-2.2% on ImageNet.\nAs the soft split in T2T module is similar to convolu-\ntion operation without convolution \ufb01lters, we also replace\nthe T2T module by 3 convolution layers with kernel size\n(7,3,3), stride size (4,2,2) respectively. Such a model with", "mimetype": "text/plain", "start_char_idx": 3939, "end_char_idx": 4988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "922da7fb-431d-4cd4-8364-3e0406c28359": {"__data__": {"id_": "922da7fb-431d-4cd4-8364-3e0406c28359", "embedding": null, "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36529018-68c0-42bc-8369-da139934bcb5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4d41f5b86d6e395acd46df8d87e760809e61db323e70e2f954265007730ea816", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2896f8f-b999-4c25-8b17-2e32fffafaf2", "node_type": "1", "metadata": {}, "hash": "09dba644d523c1ca3d9729d8b534d8ef779613cce19a27f614fb13745537ffc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 6. Transfer of some common designs in CNN to ViT&T2T-ViT, including DenseNet, Wide-ResNet, SE module, ResNeXt, Ghost\noperation. The same color means the correspond transfer. All models are trained from scratch on ImageNet. * means we reproduce the\nmodel with our training scheme for fair comparisons.\nModel Type Models Top1-Acc (%) Params (M) MACs (G) Depth Hidden dim\nTraditional CNN\nAlexNet [22] 56.6 61.1 0.77 - -\nVGG11 [33] 69.1 132.8 7.7 11 -\nInception v3 [35] 77.4 27.2 5.7 - -\nSkip-connection CNN\nResNet50 [15] 76.2 25.6 4.3 50 -\nResNet50* (Baseline) 79.1 25.6 4.3 50 -\nWide-ResNet18x1.5* 78.0 (-1.1) 26.0 4.1 18 -\nDenseNet201* 77.5 (-1.6) 20.1 4.4 201 -\nSENet50* 80.3 (+1.2) 28.1 4.9 50 -\nResNeXt50* 79.9 (+0.8) 25.0 4.3 50 -\nResNet50-Ghost* 76.2 (-2.9) 19.9 3.2 50 -\nCNN to ViT\nViT-S/16 (Baseline) 78.1 48.6 10.1 8 768\nViT-DN 79.0 (+0.9) 24.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2896f8f-b999-4c25-8b17-2e32fffafaf2": {"__data__": {"id_": "a2896f8f-b999-4c25-8b17-2e32fffafaf2", "embedding": null, "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36529018-68c0-42bc-8369-da139934bcb5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4d41f5b86d6e395acd46df8d87e760809e61db323e70e2f954265007730ea816", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "922da7fb-431d-4cd4-8364-3e0406c28359", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "5e3f19c6e610513bc0049c0e7dcc5722dc6b873e9698d12a02a50b9fccb21282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da2cacc8-f7c3-473b-bde2-848003fcac35", "node_type": "1", "metadata": {}, "hash": "9d3243c4e71c06b2d94efc8c54f151cbdb808e25dda090251847e6be3779111d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 (+0.8) 25.0 4.3 50 -\nResNet50-Ghost* 76.2 (-2.9) 19.9 3.2 50 -\nCNN to ViT\nViT-S/16 (Baseline) 78.1 48.6 10.1 8 768\nViT-DN 79.0 (+0.9) 24.5 5.5 16 384\nViT-SW 69.9 (-8.2) 47.9 9.9 4 1024\nViT-Dense 76.8 (-1.3) 46.7 9.7 19 128-736\nViT-SE 78.4 (+0.3) 49.2 10.2 8 768\nViT-ResNeXt 78.0 (-0.1) 48.6 10.1 8 768\nViT-Ghost 73.7 (-4.4) 32.1 6.9 8 768\nCNN to T2T-ViT\nT2T-ViT-14 (Baseline) 81.5 21.5 4.8 14 384\nT2T-ViT-Wide 77.9 (-3.4) 25.1 5.0 14 768\nT2T-ViT-Dense 80.6 (-1.1) 23.7 5.5 19 128-584\nT2T-ViT-SE 81.6 (+0.1) 21.9 4.9 14 384\nT2T-ViT-ResNeXt 81.5 (+0.0) 21.5 4.", "mimetype": "text/plain", "start_char_idx": 717, "end_char_idx": 1277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da2cacc8-f7c3-473b-bde2-848003fcac35": {"__data__": {"id_": "da2cacc8-f7c3-473b-bde2-848003fcac35", "embedding": null, "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36529018-68c0-42bc-8369-da139934bcb5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4d41f5b86d6e395acd46df8d87e760809e61db323e70e2f954265007730ea816", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2896f8f-b999-4c25-8b17-2e32fffafaf2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "9643b03852ca7476574102e86bf510bb7f69a3e457b8cf914a381dc7fd54c5b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0", "node_type": "1", "metadata": {}, "hash": "0a42af4b424cc4c3d5c4068540791252209b8e0ab2cb95bea2cc49bf6c0e42f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 5.0 14 768\nT2T-ViT-Dense 80.6 (-1.1) 23.7 5.5 19 128-584\nT2T-ViT-SE 81.6 (+0.1) 21.9 4.9 14 384\nT2T-ViT-ResNeXt 81.5 (+0.0) 21.5 4.8 14 384\nT2T-ViT-Ghost 79.5 (-2.0) 16.3 3.7 14 384\nTable 7. Ablation study results on T2T module, Deep-Narrow(DN)\nstructure.\nAblation type Models Top1-Acc\n(%)\nParams\n(M)\nMACs\n(G)\nT2T module\nT2T-ViT-14wo T2T 79.5 21.1 4.2\nT2T-ViT-14 81.5 (+2.0) 21.5 4.8\nT2T-ViTt-14 81.7 (+2.2) 21.5 6.1\nT2T-ViTc-14 80.8 (+1.3) 21.3 4.6\nDN Structure\nT2T-ViT-14 81.5 21.5 4.8\nT2T-ViT-d768-4 78.8 (-2.7) 25.0 5.4\nconvolution layers to build T2T module is denoted as T2T-\nViTc-14. From Tab.", "mimetype": "text/plain", "start_char_idx": 1144, "end_char_idx": 1746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0": {"__data__": {"id_": "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0", "embedding": null, "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36529018-68c0-42bc-8369-da139934bcb5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4d41f5b86d6e395acd46df8d87e760809e61db323e70e2f954265007730ea816", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da2cacc8-f7c3-473b-bde2-848003fcac35", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "498ea7bc1ba72c1d5334eb09d12d5824d92b635b9476824ab9e93252aeadee52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03a7d3d5-135b-49d5-ba58-2b174f709eb4", "node_type": "1", "metadata": {}, "hash": "0a623a8e7382e91f85d71737800c9312d1fb977cd133fca00d425c3b947e97ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From Tab. 7, we can \ufb01nd the T2T-ViT c-14 is\nworse than T2T-ViT-14 and T2T-ViTt-14 by 0.5%-1.0% on\nImageNet. We also note that the T2T-ViTc-14 is still higher\nthan T2T-ViT-14wo T2T , as the convolution layers in the\nearly stage can also model the structure information. But\nour designed T2T module is better than the convolution lay-\ners as it can model both the global relation and the structure\ninformation of the images.\nDeep-narrow structure We use the deep-narrow struc-\nture with fewer hidden dimensions but more layers, rather\nthan the shallow-wide one in the original ViT. We com-\npare the T2T-ViT-14 and T2T-ViT-d768-4 to verify its ef-\nfects. T2T-ViT-d768-4 is a shallow-wide structure with hid-\nden dimension of 768 and 4 layers, with similar model size\nand MACs as T2T-ViT-14. From Tab. 7, we can \ufb01nd af-\nter changing our deep-narrow to shallow-wide structure, the\nT2T-ViT-d768-4 has 2.7% decrease in top-1 accuracy, vali-\ndating deep-narrow structure is crucial for T2T-ViT.\n5. Conclusion\nIn this work, we propose a new T2T-ViT model that can\nbe trained from scratch on ImageNet and achieve compara-\nble or even better performance than CNNs. T2T-ViT effec-\ntively models the structure information of images and en-\nhances feature richness, overcoming limitations of ViT.", "mimetype": "text/plain", "start_char_idx": 1737, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03a7d3d5-135b-49d5-ba58-2b174f709eb4": {"__data__": {"id_": "03a7d3d5-135b-49d5-ba58-2b174f709eb4", "embedding": null, "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36529018-68c0-42bc-8369-da139934bcb5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4d41f5b86d6e395acd46df8d87e760809e61db323e70e2f954265007730ea816", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "e443f23cddbca6648bb69b38773b66bc6d965b669a58f7d836f1bd660c93d2a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. Conclusion\nIn this work, we propose a new T2T-ViT model that can\nbe trained from scratch on ImageNet and achieve compara-\nble or even better performance than CNNs. T2T-ViT effec-\ntively models the structure information of images and en-\nhances feature richness, overcoming limitations of ViT. It\nintroduces the novel tokens-to-token (T2T) process to pro-\ngressively tokenize images to tokens and structurally ag-\ngregate tokens. We also explore various architecture de-\nsign choices from CNNs for improving T2T-ViT perfor-\nmance, and empirically \ufb01nd the deep-narrow architecture\nperforms better than the shallow-wide structure. Our T2T-\nViT achieves superior performance to ResNets and compa-\nrable performance to MobileNets with similar model size\nwhen trained from scratch on ImageNet. It paves the way\nfor further developing transformer-based models for vision\ntasks.", "mimetype": "text/plain", "start_char_idx": 2724, "end_char_idx": 3597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63ee965a-2361-4478-9ee7-bd3563674599": {"__data__": {"id_": "63ee965a-2361-4478-9ee7-bd3563674599", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efa04065-a027-4aa8-a220-eb40177d28c0", "node_type": "1", "metadata": {}, "hash": "f426175c6ddc34463d8f059e780e3c6b099a34217df7952845e09b5d512deadb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le. At-\ntention augmented convolutional networks. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 3286\u20133295, 2019.\n[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-end object detection with trans-\nformers. arXiv preprint arXiv:2005.12872, 2020.\n[4] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma,\nC. Xu, C. Xu, and W. Gao. Pre-trained image processing\ntransformer. arXiv preprint arXiv:2012.00364, 2020.\n[5] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever. Generative pretraining from pixels. In Interna-\ntional Conference on Machine Learning , pages 1691\u20131703.\nPMLR, 2020.\n[6] Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efa04065-a027-4aa8-a220-eb40177d28c0": {"__data__": {"id_": "efa04065-a027-4aa8-a220-eb40177d28c0", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63ee965a-2361-4478-9ee7-bd3563674599", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "61040f5214e56dcf7bfb1208391670d5261da78bcb1f028bdaeb4f85232b075f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4f342aa-04cc-445a-98fc-1e9069da2f43", "node_type": "1", "metadata": {}, "hash": "e8af7de9335970052238277871e14fce192206aca174bd0a428c3cdfb72ca360", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:2012.00364, 2020.\n[5] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever. Generative pretraining from pixels. In Interna-\ntional Conference on Machine Learning , pages 1691\u20131703.\nPMLR, 2020.\n[6] Y . Chen, Y . Kalantidis, J. Li, S. Yan, and J. Feng. A\u02c6 2-nets:\nDouble attention networks. In Advances in neural informa-\ntion processing systems, pages 352\u2013361, 2018.\n[7] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song,\nA. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin,\nL. Kaiser, et al. Rethinking attention with performers. arXiv\npreprint arXiv:2009.14794, 2020.\n[8] Z. Dai, B. Cai, Y . Lin, and J. Chen. Up-detr: Unsupervised\npre-training for object detection with transformers. arXiv\npreprint arXiv:2011.09094, 2020.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.", "mimetype": "text/plain", "start_char_idx": 719, "end_char_idx": 1704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4f342aa-04cc-445a-98fc-1e9069da2f43": {"__data__": {"id_": "d4f342aa-04cc-445a-98fc-1e9069da2f43", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efa04065-a027-4aa8-a220-eb40177d28c0", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "2f4893967062dd7f9bcd7401cc1f804181ead227d6c3b4dda0b9d1c71801e3eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6984cba-241b-4a14-adda-b02d918724f8", "node_type": "1", "metadata": {}, "hash": "03e79517df2bf57f6d7b8b46752306f218667e06d65e7954e479ff2930382bb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv\npreprint arXiv:2011.09094, 2020.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805, 2018.\n[11] T. DeVries and G. W. Taylor. Improved regularization of\nconvolutional neural networks with cutout. arXiv preprint\narXiv:1708.04552, 2017.\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[13] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu.\nDual attention network for scene segmentation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3146\u20133154, 2019.", "mimetype": "text/plain", "start_char_idx": 1451, "end_char_idx": 2500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6984cba-241b-4a14-adda-b02d918724f8": {"__data__": {"id_": "e6984cba-241b-4a14-adda-b02d918724f8", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4f342aa-04cc-445a-98fc-1e9069da2f43", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "5781c1894f2ee30d7303b44532211db302fb1bcc2bb29c7614545d8b953d2e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f76247-981f-48ea-807c-15a942ef9446", "node_type": "1", "metadata": {}, "hash": "ee7b9f8fb37c09b3941cac06544b8c85fd3c56c037fc931d75c68beb483d4f23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint\narXiv:2010.11929, 2020.\n[13] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu.\nDual attention network for scene segmentation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3146\u20133154, 2019.\n[14] K. Han, Y . Wang, Q. Tian, J. Guo, C. Xu, and C. Xu. Ghost-\nnet: More features from cheap operations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1580\u20131589, 2020.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n770\u2013778, 2016.\n[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[17] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Ef\ufb01-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017.\n[18] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition.", "mimetype": "text/plain", "start_char_idx": 2242, "end_char_idx": 3353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46f76247-981f-48ea-807c-15a942ef9446": {"__data__": {"id_": "46f76247-981f-48ea-807c-15a942ef9446", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6984cba-241b-4a14-adda-b02d918724f8", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "703dd4934eceb9cd99ddc6581b8a5d14c18ebcaddbedcbf65579733e443cecfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2", "node_type": "1", "metadata": {}, "hash": "1e7cc2a2f28d9b73693994240367d5bf491d299f7e1883b52bd4feb6c5d0968a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mobilenets: Ef\ufb01-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017.\n[18] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 3464\u20133473,\n2019.\n[19] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi. Gather-\nexcite: Exploiting feature context in convolutional neural\nnetworks. Advances in neural information processing sys-\ntems, 31:9401\u20139411, 2018.\n[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132\u20137141, 2018.\n[21] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 4700\u20134708, 2017.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsi\ufb01cation with deep convolutional neural networks. Commu-\nnications of the ACM, 60(6):84\u201390, 2017.\n[23] Y .", "mimetype": "text/plain", "start_char_idx": 3139, "end_char_idx": 4222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2": {"__data__": {"id_": "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46f76247-981f-48ea-807c-15a942ef9446", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "8304e6f8e1e9fe50f9fcd1b87c5618d599d5977a558bee8ab9a8b5b87e3e4c30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aecdf73-4f6f-44ba-96c5-9358e73957e1", "node_type": "1", "metadata": {}, "hash": "b884394cafe624d28acb83bc83fa8e6c149a2e4b58ef1608e653f355ae1f1172", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 4700\u20134708, 2017.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsi\ufb01cation with deep convolutional neural networks. Commu-\nnications of the ACM, 60(6):84\u201390, 2017.\n[23] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[24] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient de-\nscent with warm restarts. arXiv preprint arXiv:1608.03983,\n2016.\n[25] I. Loshchilov and F. Hutter. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017.\n[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Ef\ufb01cient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013.\n[27] N. Parmar, A. Vaswani, J. Uszkoreit, \u0141. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer.", "mimetype": "text/plain", "start_char_idx": 3939, "end_char_idx": 4919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3aecdf73-4f6f-44ba-96c5-9358e73957e1": {"__data__": {"id_": "3aecdf73-4f6f-44ba-96c5-9358e73957e1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "45b438d823d1056e58b86b032f9507a7e0be64bab849ab8d8fd12bb93f55093b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25ca17ed-8f48-4e41-84a7-8c2521a46492", "node_type": "1", "metadata": {}, "hash": "7b2e92fb7425b5e8bdf86a26d2a493b9484ad99752354945332bd3f137b3ea3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ef\ufb01cient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013.\n[27] N. Parmar, A. Vaswani, J. Uszkoreit, \u0141. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer. arXiv preprint\narXiv:1802.05751, 2018.\n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\net al. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in neural information process-\ning systems, pages 8026\u20138037, 2019.\n[29] M. E. Peters, M. Neumann, R. L. Logan IV , R. Schwartz,\nV . Joshi, S. Singh, and N. A. Smith. Knowledge en-\nhanced contextual word representations. arXiv preprint\narXiv:1909.04164, 2019.\n[30] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language understanding by generative pre-\ntraining, 2018.\n[31] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-\nskaya, and J. Shlens. Stand-alone self-attention in vision\nmodels. arXiv preprint arXiv:1906.05909, 2019.", "mimetype": "text/plain", "start_char_idx": 4717, "end_char_idx": 5722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25ca17ed-8f48-4e41-84a7-8c2521a46492": {"__data__": {"id_": "25ca17ed-8f48-4e41-84a7-8c2521a46492", "embedding": null, "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5662bdb9-10da-4a26-b339-469fd9bad27f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "0169828b190547e0057ec145e410e932eb595c619f8fdd850fd3ed72f9b3bdb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aecdf73-4f6f-44ba-96c5-9358e73957e1", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "2b8b78566b88d21b1396f68bec5ff526c0f35ece49899afb8fca786f4d5c624d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Improving language understanding by generative pre-\ntraining, 2018.\n[31] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-\nskaya, and J. Shlens. Stand-alone self-attention in vision\nmodels. arXiv preprint arXiv:1906.05909, 2019.\n[32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.\nChen. Mobilenetv2: Inverted residuals and linear bottle-\nnecks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4510\u20134520, 2018.", "mimetype": "text/plain", "start_char_idx": 5486, "end_char_idx": 5951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48ba4ea2-cf51-4e69-89c3-4085304a6536": {"__data__": {"id_": "48ba4ea2-cf51-4e69-89c3-4085304a6536", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb", "node_type": "1", "metadata": {}, "hash": "276baa972ce979dd69cd3f00dec8f154031cef71c581d8e3e7d525140c96176b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[34] Z. Sun, S. Cao, Y . Yang, and K. Kitani. Rethinking\ntransformer-based set prediction for object detection. arXiv\npreprint arXiv:2011.10881, 2020.\n[35] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818\u20132826, 2016.\n[36] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. J \u00b4egou. Training data-ef\ufb01cient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all\nyou need. Advances in neural information processing sys-\ntems, 30:5998\u20136008, 2017.\n[38] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,\nX. Wang, and X. Tang.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb": {"__data__": {"id_": "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48ba4ea2-cf51-4e69-89c3-4085304a6536", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "421b33a32c751b4b2d049fa6913fb822994aeb42768e8386ac5eb30f952219c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93dcf9c9-87a6-411b-ad4f-004002f99851", "node_type": "1", "metadata": {}, "hash": "9229edde371b7a62dc423981a1d015efea7a038dbb1513ce15bcaf1cd85b92e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Attention is all\nyou need. Advances in neural information processing sys-\ntems, 30:5998\u20136008, 2017.\n[38] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,\nX. Wang, and X. Tang. Residual attention network for im-\nage classi\ufb01cation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3156\u20133164,\n2017.\n[39] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neu-\nral networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7794\u20137803,\n2018.\n[40] Y . Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and\nH. Xia. End-to-end video instance segmentation with trans-\nformers. arXiv preprint arXiv:2011.14503, 2020.\n[41] R. Wightman. Pytorch image models. https://github.\ncom/rwightman/pytorch-image-models, 2019.\n[42] S. Woo, J. Park, J.-Y . Lee, and I. So Kweon. Cbam: Convo-\nlutional block attention module. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 3\u201319,\n2018.\n[43] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka,\nK. Keutzer, and P. Vajda.", "mimetype": "text/plain", "start_char_idx": 828, "end_char_idx": 1887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93dcf9c9-87a6-411b-ad4f-004002f99851": {"__data__": {"id_": "93dcf9c9-87a6-411b-ad4f-004002f99851", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "189de80c02027be80a06d63b6f6ffa2a599d0be46f6fa6081a0c805c80a99bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc3a40ae-0e93-413c-a38d-83c572ae60c0", "node_type": "1", "metadata": {}, "hash": "fc13672c69b962cdc6c9278cceb0877f000f1aa96a25ae74efc8d5b060464fda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lee, and I. So Kweon. Cbam: Convo-\nlutional block attention module. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 3\u201319,\n2018.\n[43] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka,\nK. Keutzer, and P. Vajda. Visual transformers: Token-based\nimage representation and processing for computer vision.\narXiv preprint arXiv:2006.03677, 2020.\n[44] S. Xie, R. Girshick, P. Doll \u00b4ar, Z. Tu, and K. He. Aggre-\ngated residual transformations for deep neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1492\u20131500, 2017.\n[45] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture\ntransformer network for image super-resolution. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5791\u20135800, 2020.\n[46] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V . Le. Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural informa-\ntion processing systems, pages 5753\u20135763, 2019.\n[47] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked at-\ntention networks for image question answering.", "mimetype": "text/plain", "start_char_idx": 1647, "end_char_idx": 2812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc3a40ae-0e93-413c-a38d-83c572ae60c0": {"__data__": {"id_": "bc3a40ae-0e93-413c-a38d-83c572ae60c0", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93dcf9c9-87a6-411b-ad4f-004002f99851", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "52087d62e24842a15ffe36c5c39712cae048bc16b3d64f0fb9499b2c9d1ac466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "feda05b9-2396-4b75-baa6-1dde06773c4f", "node_type": "1", "metadata": {}, "hash": "01514e78fe478ab305167b8e0b1a2f29458b6f51da38ecbe1cd506cf3de4aa78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Le. Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural informa-\ntion processing systems, pages 5753\u20135763, 2019.\n[47] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked at-\ntention networks for image question answering. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 21\u201329, 2016.\n[48] L. Yuan, S. Chang, Z. Huang, Y . Zhou, Y . Chen, X. Nie, F. E.\nTay, J. Feng, and S. Yan. A simple baseline for pose tracking\nin videos of crowed scenes. In Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 4684\u20134688,\n2020.\n[49] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. Revisiting\nknowledge distillation via label smoothing regularization. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3903\u20133911, 2020.\n[50] L. Yuan, Y . Zhou, S. Chang, Z. Huang, Y . Chen, X. Nie,\nT. Wang, J. Feng, and S. Yan. Toward accurate person-level\naction recognition in videos of crowed scenes. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 4694\u20134698, 2020.\n[51] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y .", "mimetype": "text/plain", "start_char_idx": 2545, "end_char_idx": 3725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "feda05b9-2396-4b75-baa6-1dde06773c4f": {"__data__": {"id_": "feda05b9-2396-4b75-baa6-1dde06773c4f", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc3a40ae-0e93-413c-a38d-83c572ae60c0", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "ec8b94762f1b944aa22f2fcb4b4f93c79382e41e7ed1cf00ceda098d5305fcf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3fb2b73-bcb1-424d-b286-492354199179", "node_type": "1", "metadata": {}, "hash": "e0d44a679729b0ae09b3c4031b82cef0584ed78c9e95f0540bdc3a7ade43a5f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chen, X. Nie,\nT. Wang, J. Feng, and S. Yan. Toward accurate person-level\naction recognition in videos of crowed scenes. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 4694\u20134698, 2020.\n[51] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y . Yoo. Cut-\nmix: Regularization strategy to train strong classi\ufb01ers with\nlocalizable features. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 6023\u20136032,\n2019.\n[52] S. Zagoruyko and N. Komodakis. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.\n[53] Y . Zeng, J. Fu, and H. Chao. Learning joint spatial-temporal\ntransformations for video inpainting. In European Confer-\nence on Computer Vision, pages 528\u2013543. Springer, 2020.\n[54] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz.\nmixup: Beyond empirical risk minimization. arXiv preprint\narXiv:1710.09412, 2017.\n[55] H. Zhao, J. Jia, and V . Koltun. Exploring self-attention for\nimage recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10076\u201310085, 2020.\n[56] H. Zhao, L. Jiang, J. Jia, P. Torr, and V .", "mimetype": "text/plain", "start_char_idx": 3451, "end_char_idx": 4588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3fb2b73-bcb1-424d-b286-492354199179": {"__data__": {"id_": "e3fb2b73-bcb1-424d-b286-492354199179", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "feda05b9-2396-4b75-baa6-1dde06773c4f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "6901f8a9de30c82b288f308fdb76560ba9d5c88724f9cb3233f7bac98e99971c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82dd1d13-31ec-4178-bdb8-90f7af0e5e02", "node_type": "1", "metadata": {}, "hash": "365099bbe6dcb9fa97019f9114c82a54bee00c8f58fb8b86088a2111f6b64b56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[55] H. Zhao, J. Jia, and V . Koltun. Exploring self-attention for\nimage recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10076\u201310085, 2020.\n[56] H. Zhao, L. Jiang, J. Jia, P. Torr, and V . Koltun. Point trans-\nformer. arXiv preprint arXiv:2012.09164, 2020.\n[57] H. Zhao, Y . Zhang, S. Liu, J. Shi, C. Change Loy, D. Lin,\nand J. Jia. Psanet: Point-wise spatial attention network for\nscene parsing. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 267\u2013283, 2018.\n[58] M. Zheng, P. Gao, X. Wang, H. Li, and H. Dong. End-to-end\nobject detection with adaptive clustering transformer. arXiv\npreprint arXiv:2011.09315, 2020.\n[59] D. Zhou, X. Jin, Q. Hou, K. Wang, J. Yang, and J. Feng. Neu-\nral epitome search for architecture-agnostic network com-\npression. In International Conference on Learning Repre-\nsentations, 2019.\n[60] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong.\nEnd-to-end dense video captioning with masked transformer.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8739\u20138748, 2018.", "mimetype": "text/plain", "start_char_idx": 4341, "end_char_idx": 5468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82dd1d13-31ec-4178-bdb8-90f7af0e5e02": {"__data__": {"id_": "82dd1d13-31ec-4178-bdb8-90f7af0e5e02", "embedding": null, "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "44bd36aa31355997f7076469fdcb4c3a8b7c709a64368640a86fc0ed0e0cee1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3fb2b73-bcb1-424d-b286-492354199179", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}, "hash": "4ddcb493f5fa012f222c09c46f49e416ac935834e25ee0dd175dcc3c8c2e0479", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In International Conference on Learning Repre-\nsentations, 2019.\n[60] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong.\nEnd-to-end dense video captioning with masked transformer.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8739\u20138748, 2018.\n[61] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. De-\nformable detr: Deformable transformers for end-to-end ob-\nject detection. arXiv preprint arXiv:2010.04159, 2020.", "mimetype": "text/plain", "start_char_idx": 5178, "end_char_idx": 5640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87b3dae9-4044-40b8-8e51-f874216dedff": {"__data__": {"id_": "87b3dae9-4044-40b8-8e51-f874216dedff", "embedding": null, "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2598b908-e27c-40e4-bff7-c4323558bdc3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ebc315d3e9d34c578f94fa4f61636dfab770fabb7b66cca7799102dc155559d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28cf8e56-f160-4ae1-a9bc-4d30601f8a03", "node_type": "1", "metadata": {}, "hash": "ec89d11e9f90369629bc575e8dd384c5c5d0c9b68a48e89faebaec56438112bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training data-ef\ufb01cient image transformers\n& distillation through attention\nHugo Touvron\u22c6,\u2020 Matthieu Cord\u2020 Matthijs Douze\u22c6\nFrancisco Massa\u22c6 Alexandre Sablayrolles\u22c6 Herv\u00b4e J\u00b4egou\u22c6\n\u22c6Facebook AI \u2020Sorbonne University\nAbstract\nRecently, neural networks purely based on attention were shown to ad-\ndress image understanding tasks such as image classi\ufb01cation. These high-\nperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption.\nIn this work, we produce competitive convolution-free transformers by\ntraining on Imagenet only. We train them on a single computer in less than\n3 days. Our reference vision transformer (86M parameters) achieves top-1\naccuracy of 83.1% (single-crop) on ImageNet with no external data.\nMore importantly, we introduce a teacher-student strategy speci\ufb01c to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet\n(where we obtain up to 85.2% accuracy) and when transferring to other\ntasks. We share our code and models.\n1 Introduction\nConvolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classi\ufb01cation tasks.\nOne of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28cf8e56-f160-4ae1-a9bc-4d30601f8a03": {"__data__": {"id_": "28cf8e56-f160-4ae1-a9bc-4d30601f8a03", "embedding": null, "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2598b908-e27c-40e4-bff7-c4323558bdc3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ebc315d3e9d34c578f94fa4f61636dfab770fabb7b66cca7799102dc155559d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87b3dae9-4044-40b8-8e51-f874216dedff", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "5ec769ac46a4eeaab16a85f16fd2a027fa4ad9f2e8669bc3aaaad8139eca5747", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We share our code and models.\n1 Introduction\nConvolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classi\ufb01cation tasks.\nOne of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based mod-\nels in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61].\nMore recently several researchers have proposed hybrid architecture trans-\nplanting transformer ingredients to convnets to solve vision tasks [6, 43].\nThe vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an ar-\nchitecture directly inherited from Natural Language Processing [52], but ap-\n1\narXiv:2012.12877v2  [cs.CV]  15 Jan 2021", "mimetype": "text/plain", "start_char_idx": 1235, "end_char_idx": 2097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45ad2f8a-1035-409d-ab1f-7aedf845e2f8": {"__data__": {"id_": "45ad2f8a-1035-409d-ab1f-7aedf845e2f8", "embedding": null, "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79db41c1-c88e-42fe-bc5c-fd97f83e7883", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "8c32f9ab7b4fc4c5d7dbc76a54e9aa2a9f04a93228070ace9394ea7992de8ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd978134-eb9c-4158-8124-35979d4142dc", "node_type": "1", "metadata": {}, "hash": "784ce5c1aa062b8cc3b5077296e8634dbf5c569c0be9b54331c3ac0165bc457e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2697\u2191\n\u2697\n\u2697\n\u2697\nFigure 1: Throughput and accuracy on Imagenet of our methods compared to\nEf\ufb01cientNets, trained on Imagenet1k only. The throughput is measured as the\nnumber of images processed per second on a V100 GPU. DeiT-B is identical to\nVIT-B, but the training is more adapted to a data-starving regime. It is learned\nin a few days on one machine. The symbol\n\u2697 refers to models trained with our\ntransformer-speci\ufb01c distillation. See Table 5 for details and more models.\nplied to image classi\ufb01cation with raw image patches as input. Their paper pre-\nsented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers \u201cdo not generalize well when trained on insuf\ufb01cient amounts of data\u201d ,\nand the training of these models involved extensive computing resources.\nIn this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of \ufb01ne-tuning)\nthat is competitive with convnets having a similar number of parameters and\nef\ufb01ciency. It uses Imagenet as the sole training set. We build upon the vi-\nsual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-ef\ufb01cient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a success-\nful training, such as repeated augmentation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd978134-eb9c-4158-8124-35979d4142dc": {"__data__": {"id_": "dd978134-eb9c-4158-8124-35979d4142dc", "embedding": null, "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79db41c1-c88e-42fe-bc5c-fd97f83e7883", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "8c32f9ab7b4fc4c5d7dbc76a54e9aa2a9f04a93228070ace9394ea7992de8ec7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45ad2f8a-1035-409d-ab1f-7aedf845e2f8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "0f97d9417c5c0d718aa3e54004a2fd507bf8a042ae395da509b6ca7b7a525b79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We build upon the vi-\nsual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-ef\ufb01cient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a success-\nful training, such as repeated augmentation.\nWe address another question: how to distill these models? We introduce\na token-based strategy, speci\ufb01c to transformers and denoted by DeiT\n\u2697 , and\nshow that it advantageously replaces the usual distillation.\n2", "mimetype": "text/plain", "start_char_idx": 1146, "end_char_idx": 1732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "172c70e2-921b-464d-ab19-b0bc20b6d52a": {"__data__": {"id_": "172c70e2-921b-464d-ab19-b0bc20b6d52a", "embedding": null, "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6470f693-69bb-401e-bcc7-f2affcf98b65", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "298dbb9c3fa1ba17f3c2fba3afea3770309cd87b91b4220c93c9678bc68dee9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "401b98da-ca32-403a-8b62-45e0ca3ad2c2", "node_type": "1", "metadata": {}, "hash": "f5c0e1570a8fef41e4f304b492ff316615ee5d4c6915985a3177868f2f64e622", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In summary, our work makes the following contributions:\n\u2022 We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1. Our two new models DeiT-S and DeiT-Ti have fewer param-\neters and can be seen as the counterpart of ResNet-50 and ResNet-18.\n\u2022 We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at re-\nproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-speci\ufb01c strategy outper-\nforms vanilla distillation by a signi\ufb01cant margin.\n\u2022 Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance.\n\u2022 Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as \ufb01ne-grained classi\ufb01cation, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 \ufb02owers,\nStanford Cars and iNaturalist-18/19.\nThis paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classi\ufb01cation in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental sec-\ntion 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-speci\ufb01c\ndistillation. Section 6 details our training scheme. It includes an extensive ab-\nlation of our data-ef\ufb01cient training choices, which gives some insight on the\nkey ingredients involved in DeiT.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "401b98da-ca32-403a-8b62-45e0ca3ad2c2": {"__data__": {"id_": "401b98da-ca32-403a-8b62-45e0ca3ad2c2", "embedding": null, "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6470f693-69bb-401e-bcc7-f2affcf98b65", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "298dbb9c3fa1ba17f3c2fba3afea3770309cd87b91b4220c93c9678bc68dee9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "172c70e2-921b-464d-ab19-b0bc20b6d52a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "69aa8926bba6d5b7fabb12ac054ce4de82756f07d9477ae04fc0e540d7cdc13e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We introduce\nour distillation strategy for transformers in Section 4. The experimental sec-\ntion 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-speci\ufb01c\ndistillation. Section 6 details our training scheme. It includes an extensive ab-\nlation of our data-ef\ufb01cient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.\n2 Related work\nImage Classi\ufb01cation is so core to computer vision that it is often used as a\nbenchmark to measure progress in image understanding. Any progress usu-\nally translates to improvement in other related tasks such as detection or seg-\nmentation. Since 2012\u2019s AlexNet [32], convnets have dominated this bench-\nmark and have become the de facto standard. The evolution of the state of the\nart on the ImageNet dataset [42] re\ufb02ects the progress with convolutional neural\nnetwork architectures and learning [32, 44, 48, 50, 51, 57].\nDespite several attempts to use transformers for image classi\ufb01cation [7], un-\ntil now their performance has been inferior to that of convnets. Nevertheless\nhybrid architectures that combine convnets and transformers, including the\nself-attention mechanism, have recently exhibited competitive results in image\nclassi\ufb01cation [56], detection [6, 28], video processing [45, 53], unsupervised ob-\nject discovery [35], and uni\ufb01ed text-vision tasks [8, 33, 37].\n1We can accelerate the learning of the larger model DeiT-B by training it on 8 GPUs in two days.\n3", "mimetype": "text/plain", "start_char_idx": 1289, "end_char_idx": 2843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae717b80-b3b7-4386-ba6b-1aca1cb0940d": {"__data__": {"id_": "ae717b80-b3b7-4386-ba6b-1aca1cb0940d", "embedding": null, "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4f5c87-39e0-44cb-9751-46269af21cca", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "acae2e2255966637ecd7e406f02047e03d1521ebb09b72e9ee23570368702850", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d772f735-cae7-4a46-bba6-9fd36dbf21db", "node_type": "1", "metadata": {}, "hash": "14150b6fb580cbe470fcb0ba76ced862f70922caaaae3f36844e071a5a8d59eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recently Vision transformers (ViT) [15] closed the gap with the state of the\nart on ImageNet, without using any convolution. This performance is remark-\nable since convnet methods for image classi\ufb01cation have bene\ufb01ted from years\nof tuning and optimization [22, 55]. Nevertheless, according to this study [15],\na pre-training phase on a large volume of curated data is required for the\nlearned transformer to be effective. In our paper we achieve a strong perfor-\nmance without requiring a large training dataset, i.e., with Imagenet1k only.\nThe Transformer architecture, introduced by Vaswani et al. [52] for machine\ntranslation are currently the reference model for all natural language process-\ning (NLP) tasks. Many improvements of convnets for image classi\ufb01cation are\ninspired by transformers. For example, Squeeze and Excitation [2], Selective\nKernel [34] and Split-Attention Networks [61] exploit mechanism akin to trans-\nformers self-attention (SA) mechanism.\nKnowledge Distillation (KD), introduced by Hinton et al. [24], refers to the\ntraining paradigm in which a student model leverages \u201csoft\u201d labels coming\nfrom a strongteacher network. This is the output vector of the teacher\u2019s softmax\nfunction rather than just the maximum of scores, wich gives a \u201chard\u201d label.\nSuch a training improves the performance of the student model (alternatively,\nit can be regarded as a form of compression of the teacher model into a smaller\none \u2013 the student). On the one hand the teacher\u2019s soft labels will have a similar\neffect to labels smoothing [58]. On the other hand as shown by Wei et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d772f735-cae7-4a46-bba6-9fd36dbf21db": {"__data__": {"id_": "d772f735-cae7-4a46-bba6-9fd36dbf21db", "embedding": null, "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4f5c87-39e0-44cb-9751-46269af21cca", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "acae2e2255966637ecd7e406f02047e03d1521ebb09b72e9ee23570368702850", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae717b80-b3b7-4386-ba6b-1aca1cb0940d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "426d627aa28ffa46c8aa1d0bbcc8a317c992e0a6798315ceb511b0329d6ff41e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8911a468-1ca2-4e2f-a83b-e31a3a85feac", "node_type": "1", "metadata": {}, "hash": "cc2faa07ec575ed2386a2ef2b223341d00eef566ca9b3427051e782d4da33d8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is the output vector of the teacher\u2019s softmax\nfunction rather than just the maximum of scores, wich gives a \u201chard\u201d label.\nSuch a training improves the performance of the student model (alternatively,\nit can be regarded as a form of compression of the teacher model into a smaller\none \u2013 the student). On the one hand the teacher\u2019s soft labels will have a similar\neffect to labels smoothing [58]. On the other hand as shown by Wei et al. [54]\nthe teacher\u2019s supervision takes into account the effects of the data augmenta-\ntion, which sometimes causes a misalignment between the real label and the\nimage. For example, let us consider image with a \u201ccat\u201d label that represents a\nlarge landscape and a small cat in a corner. If the cat is no longer on the crop\nof the data augmentation it implicitly changes the label of the image. KD can\ntransfer inductive biases [1] in a soft way in a student model using a teacher\nmodel where they would be incorporated in a hard way. For example, it may\nbe useful to induce biases due to convolutions in a transformer model by using\na convolutional model as teacher. In our paper we study the distillation of a\ntransformer student by either a convnet or a transformer teacher. We introduce\na new distillation procedure speci\ufb01c to transformers and show its superiority.\n3 Vision transformer: overview\nIn this section, we brie\ufb02y recall preliminaries associated with the vision trans-\nformer [15, 52], and further discuss positional encoding and resolution.\nMulti-head Self Attention layers (MSA). The attention mechanism is based\non a trainable associative memory with (key, value) vector pairs. Aquery vector\nq \u2208Rd is matched against a set of kkey vectors (packed together into a matrix\nK \u2208Rk\u00d7d) using inner products.", "mimetype": "text/plain", "start_char_idx": 1148, "end_char_idx": 2901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8911a468-1ca2-4e2f-a83b-e31a3a85feac": {"__data__": {"id_": "8911a468-1ca2-4e2f-a83b-e31a3a85feac", "embedding": null, "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4f5c87-39e0-44cb-9751-46269af21cca", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "acae2e2255966637ecd7e406f02047e03d1521ebb09b72e9ee23570368702850", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d772f735-cae7-4a46-bba6-9fd36dbf21db", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3dab9ba9caad4ca9f151f5671886cf04faca8b053bb44611a39e4e49b83e8dd8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multi-head Self Attention layers (MSA). The attention mechanism is based\non a trainable associative memory with (key, value) vector pairs. Aquery vector\nq \u2208Rd is matched against a set of kkey vectors (packed together into a matrix\nK \u2208Rk\u00d7d) using inner products. These inner products are then scaled and\n4", "mimetype": "text/plain", "start_char_idx": 2640, "end_char_idx": 2944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fce9724-49f1-4f3e-93c9-264df1f916c1": {"__data__": {"id_": "7fce9724-49f1-4f3e-93c9-264df1f916c1", "embedding": null, "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58be4cfa-322c-4520-ae71-69e0820fa62d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fabbaace5395cd5c1eecf91fa28c5a74e870f49a360e25d45b85a82b550754ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091", "node_type": "1", "metadata": {}, "hash": "daebe99a54cf45548e9963fa0cc013db254b7272bf24c2409999f4d8ff3ee36f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "normalized with a softmax function to obtain k weights. The output of the\nattention is the weighted sum of a set ofkvalue vectors (packed intoV \u2208Rk\u00d7d).\nFor a sequence of N query vectors (packed into Q \u2208RN\u00d7d), it produces an\noutput matrix (of size N \u00d7d):\nAttention(Q,K,V ) = Softmax(QK\u22a4/\n\u221a\nd)V, (1)\nwhere the Softmax function is applied over each row of the input matrix and\nthe\n\u221a\ndterm provides appropriate normalization.\nIn [52], a Self-attention layer is proposed. Query, key and values matrices\nare themselves computed from a sequence of N input vectors (packed into\nX \u2208RN\u00d7D): Q = XWQ, K = XWK, V = XWV, using linear transformations\nWQ,WK,WV with the constraint k = N, meaning that the attention is in be-\ntween all the input vectors.\nFinally, Multi-head self-attention layer (MSA) is de\ufb01ned by considering hat-\ntention \u201cheads\u201d, ie hself-attention functions applied to the input. Each head\nprovides a sequence of size N \u00d7d. These h sequences are rearranged into a\nN \u00d7dhsequence that is reprojected by a linear layer into N \u00d7D.\nTransformer block for images. To get a full transformer block as in [52], we\nadd a Feed-Forward Network (FFN) on top of the MSA layer. This FFN is\ncomposed of two linear layers separated by a GeLu activation [23]. The \ufb01rst\nlinear layer expands the dimension fromDto 4D, and the second layer reduces\nthe dimension from4Dback to D. Both MSA and FFN are operating as residual\noperators thank to skip-connections, and with a layer normalization [3].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091": {"__data__": {"id_": "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091", "embedding": null, "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58be4cfa-322c-4520-ae71-69e0820fa62d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fabbaace5395cd5c1eecf91fa28c5a74e870f49a360e25d45b85a82b550754ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fce9724-49f1-4f3e-93c9-264df1f916c1", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "a402dc7d7ece028369d33bd291d133176bf2b702289400f1fcda3e25b83ab716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "710b4bbe-d8cd-4096-8833-c9d060166697", "node_type": "1", "metadata": {}, "hash": "c5acd461c41bc4b17634eb4a283c5238db9830dc21e3fcfafe627ebca0e0fc23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This FFN is\ncomposed of two linear layers separated by a GeLu activation [23]. The \ufb01rst\nlinear layer expands the dimension fromDto 4D, and the second layer reduces\nthe dimension from4Dback to D. Both MSA and FFN are operating as residual\noperators thank to skip-connections, and with a layer normalization [3].\nIn order to get a transformer to process images, our work builds upon the\nViT model [15]. It is a simple and elegant architecture that processes input\nimages as if they were a sequence of input tokens. The \ufb01xed-size input RGB\nimage is decomposed into a batch of N patches of a \ufb01xed size of 16 \u00d716 pixels\n(N = 14 \u00d714). Each patch is projected with a linear layer that conserves its\noverall dimension 3 \u00d716 \u00d716 = 768.\nThe transformer block described above is invariant to the order of the patch\nembeddings, and thus does not consider their relative position. The positional\ninformation is incorporated as \ufb01xed [52] or trainable [18] positional embed-\ndings. They are added before the \ufb01rst transformer block to the patch tokens,\nwhich are then fed to the stack of transformer blocks.\nThe class token is a trainable vector, appended to the patch tokens before the\n\ufb01rst layer, that goes through the transformer layers, and is then projected with\na linear layer to predict the class. This class token is inherited from NLP [14],\nand departs from the typical pooling layers used in computer vision to predict\nthe class. The transformer thus process batches of (N+ 1)tokens of dimension\nD, of which only the class vector is used to predict the output.", "mimetype": "text/plain", "start_char_idx": 1165, "end_char_idx": 2719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "710b4bbe-d8cd-4096-8833-c9d060166697": {"__data__": {"id_": "710b4bbe-d8cd-4096-8833-c9d060166697", "embedding": null, "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58be4cfa-322c-4520-ae71-69e0820fa62d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fabbaace5395cd5c1eecf91fa28c5a74e870f49a360e25d45b85a82b550754ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d8bd78cf8ebcb373b6bacfd169f5d4ff9d7c2eda9c9227578c61ecb4d3033b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This class token is inherited from NLP [14],\nand departs from the typical pooling layers used in computer vision to predict\nthe class. The transformer thus process batches of (N+ 1)tokens of dimension\nD, of which only the class vector is used to predict the output. This architecture\nforces the self-attention to spread information between the patch tokens and\nthe class token: at training time the supervision signal comes only from the\nclass embedding, while the patch tokens are the model\u2019s only variable input.\n5", "mimetype": "text/plain", "start_char_idx": 2454, "end_char_idx": 2970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d7f4586-2f4b-4e73-81d4-fe051ff2f232": {"__data__": {"id_": "1d7f4586-2f4b-4e73-81d4-fe051ff2f232", "embedding": null, "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57fc2f8a-3851-4edc-af16-e8795cccb141", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "01580b253b07aaa87a1c58738ae74f1c28b7266058049c965cbd0f7a5fc4bc55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f69760d-630f-4f13-a890-2a74867b3693", "node_type": "1", "metadata": {}, "hash": "0886525db68649779d1a408d7ced41e44297c321c95742798f7819562db401be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fixing the positional encoding across resolutions. Touvron et al. [50] show\nthat it is desirable to use a lower training resolution and \ufb01ne-tune the network\nat the larger resolution. This speeds up the full training and improves the accu-\nracy under prevailing data augmentation schemes. When increasing the reso-\nlution of an input image, we keep the patch size the same, therefore the number\nN of input patches does change. Due to the architecture of transformer blocks\nand the class token, the model and classi\ufb01er do not need to be modi\ufb01ed to pro-\ncess more tokens. In contrast, one needs to adapt the positional embeddings,\nbecause there are N of them, one for each patch. Dosovitskiy et al. [15] inter-\npolate the positional encoding when changing the resolution and demonstrate\nthat this method works with the subsequent \ufb01ne-tuning stage.\n4 Distillation through attention\nIn this section we assume we have access to a strong image classi\ufb01er as a\nteacher model. It could be a convnet, or a mixture of classi\ufb01ers. We address\nthe question of how to learn a transformer by exploiting this teacher. As we\nwill see in Section 5 by comparing the trade-off between accuracy and image\nthroughput, it can be bene\ufb01cial to replace a convolutional neural network by\na transformer. This section covers two axes of distillation: hard distillation\nversus soft distillation, and classical distillation versus the distillation token.\nSoft distillation [24, 54] minimizes the Kullback-Leibler divergence between\nthe softmax of the teacher and the softmax of the student model.\nLet Zt be the logits of the teacher model, Zs the logits of the student model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f69760d-630f-4f13-a890-2a74867b3693": {"__data__": {"id_": "5f69760d-630f-4f13-a890-2a74867b3693", "embedding": null, "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57fc2f8a-3851-4edc-af16-e8795cccb141", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "01580b253b07aaa87a1c58738ae74f1c28b7266058049c965cbd0f7a5fc4bc55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d7f4586-2f4b-4e73-81d4-fe051ff2f232", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "452eccb0ab22e16a3bdc9c8056cb50b81c50f863906768d88fc71df3dbbf96e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a68087a-5362-42d5-aa61-5c7a7ef32c63", "node_type": "1", "metadata": {}, "hash": "4102aecee498f2d552ac5b208b8f8a647bd2e853b198b4a71702fa79980a396d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This section covers two axes of distillation: hard distillation\nversus soft distillation, and classical distillation versus the distillation token.\nSoft distillation [24, 54] minimizes the Kullback-Leibler divergence between\nthe softmax of the teacher and the softmax of the student model.\nLet Zt be the logits of the teacher model, Zs the logits of the student model.\nWe denote by \u03c4 the temperature for the distillation, \u03bb the coef\ufb01cient balanc-\ning the Kullback\u2013Leibler divergence loss (KL) and the cross-entropy (LCE) on\nground truth labels y, and \u03c8the softmax function. The distillation objective is\nLglobal = (1\u2212\u03bb)LCE(\u03c8(Zs),y) +\u03bb\u03c42KL(\u03c8(Zs/\u03c4),\u03c8(Zt/\u03c4)). (2)\nHard-label distillation. We introduce a variant of distillation where we take\nthe hard decision of the teacher as a true label. Let yt = argmaxcZt(c) be the\nhard decision of the teacher, the objective associated with this hard-label distil-\nlation is:\nLhardDistill\nglobal = 1\n2LCE(\u03c8(Zs),y) +1\n2LCE(\u03c8(Zs),yt). (3)\nFor a given image, the hard label associated with the teacher may change\ndepending on the speci\ufb01c data augmentation. We will see that this choice is\nbetter than the traditional one, while being parameter-free and conceptually\nsimpler: The teacher prediction yt plays the same role as the true label y.\nNote also that the hard labels can also be converted into soft labels with\nlabel smoothing [47], where the true label is considered to have a probability\nof 1 \u2212\u03b5, and the remaining \u03b5is shared across the remaining classes.", "mimetype": "text/plain", "start_char_idx": 1274, "end_char_idx": 2771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a68087a-5362-42d5-aa61-5c7a7ef32c63": {"__data__": {"id_": "9a68087a-5362-42d5-aa61-5c7a7ef32c63", "embedding": null, "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57fc2f8a-3851-4edc-af16-e8795cccb141", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "01580b253b07aaa87a1c58738ae74f1c28b7266058049c965cbd0f7a5fc4bc55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f69760d-630f-4f13-a890-2a74867b3693", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "f4b1d8cc302e27325f45ec91f33aad5b93c548e88a41b7f2a475c703698c5193", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will see that this choice is\nbetter than the traditional one, while being parameter-free and conceptually\nsimpler: The teacher prediction yt plays the same role as the true label y.\nNote also that the hard labels can also be converted into soft labels with\nlabel smoothing [47], where the true label is considered to have a probability\nof 1 \u2212\u03b5, and the remaining \u03b5is shared across the remaining classes. We \ufb01x this\nparameter to \u03b5= 0.1 in our all experiments that use true labels.\n6", "mimetype": "text/plain", "start_char_idx": 2365, "end_char_idx": 2849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb0ba9ce-e452-4b71-815f-a1c70b89f786": {"__data__": {"id_": "eb0ba9ce-e452-4b71-815f-a1c70b89f786", "embedding": null, "metadata": {"page_label": "7", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e9f733e-b72d-49cb-8540-9f38f0d8df40", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "5a3b2a8125162c5ed78f60f9ce570c20449f8673dea21e63735918aead3bcf88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "self-attentionFFN\nclass \ntoken \ndistillation \ntoken \npatch \ntokens \nFigure 2: Our distillation procedure: we simply include a new distillation token.\nIt interacts with the class and patch tokens through the self-attention layers.\nThis distillation token is employed in a similar fashion as the class token, ex-\ncept that on output of the network its objective is to reproduce the (hard) label\npredicted by the teacher, instead of true label. Both the class and distillation\ntokens input to the transformers are learned by back-propagation.\nDistillation token. We now focus on our proposal, which is illustrated in\nFigure 2. We add a new token, the distillation token, to the initial embeddings\n(patches and class token). Our distillation token is used similarly as the class\ntoken: it interacts with other embeddings through self-attention, and is output\nby the network after the last layer. Its target objective is given by the distillation\ncomponent of the loss. The distillation embedding allows our model to learn\nfrom the output of the teacher, as in a regular distillation, while remaining\ncomplementary to the class embedding.\nInterestingly, we observe that the learned class and distillation tokens con-\nverge towards different vectors: the average cosine similarity between these\ntokens equal to 0.06. As the class and distillation embeddings are computed\nat each layer, they gradually become more similar through the network, all the\nway through the last layer at which their similarity is high (cos=0.93), but still\nlower than 1. This is expected since as they aim at producing targets that are\nsimilar but not identical.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ae1c1ec-95e2-4a1e-8c2e-66be0db3dce3": {"__data__": {"id_": "9ae1c1ec-95e2-4a1e-8c2e-66be0db3dce3", "embedding": null, "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f7c4250-e675-4c6d-9aa8-f29582555fcb", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4f7e875d1777c142b490f2caf26e0937a7f2f86fd9cc0118ef5122a7231b662b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc09ef96-c917-44f2-8c53-8082890d63ee", "node_type": "1", "metadata": {}, "hash": "4f8c13a0a86c2a31530f711d32c52733c777ce9a2f7eff2bc9a7f4855e7fc7c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We veri\ufb01ed that our distillation token adds something to the model, com-\npared to simply adding an additional class token associated with the same tar-\nget label: instead of a teacher pseudo-label, we experimented with a trans-\nformer with two class tokens. Even if we initialize them randomly and inde-\npendently, during training they converge towards the same vector (cos=0.999),\nand the output embedding are also quasi-identical. This additional class token\ndoes not bring anything to the classi\ufb01cation performance. In contrast, our dis-\ntillation strategy provides a signi\ufb01cant improvement over a vanilla distillation\nbaseline, as validated by our experiments in Section 5.2.\nFine-tuning with distillation. We use both the true label and teacher predic-\ntion during the \ufb01ne-tuning stage at higher resolution. We use a teacher with the\nsame target resolution, typically obtained from the lower-resolution teacher by\nthe method of Touvron et al [50]. We have also tested with true labels only but\nthis reduces the bene\ufb01t of the teacher and leads to a lower performance.\nClassi\ufb01cation with our approach: joint classi\ufb01ers. At test time, both the\nclass or the distillation embeddings produced by the transformer are associ-\nated with linear classi\ufb01ers and able to infer the image label. Yet our referent\nmethod is the late fusion of these two separate heads, for which we add the\nsoftmax output by the two classi\ufb01ers to make the prediction. We evaluate these\nthree options in Section 5.\n5 Experiments\nThis section presents a few analytical experiments and results. We \ufb01rst discuss\nour distillation strategy. Then we comparatively analyze the ef\ufb01ciency and\naccuracy of convnets and vision transformers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc09ef96-c917-44f2-8c53-8082890d63ee": {"__data__": {"id_": "bc09ef96-c917-44f2-8c53-8082890d63ee", "embedding": null, "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f7c4250-e675-4c6d-9aa8-f29582555fcb", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4f7e875d1777c142b490f2caf26e0937a7f2f86fd9cc0118ef5122a7231b662b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ae1c1ec-95e2-4a1e-8c2e-66be0db3dce3", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "c4ec5ee5a84c55c33f78e6868abb2642a2644770f09cc498d7d5d0d2b51c8cec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yet our referent\nmethod is the late fusion of these two separate heads, for which we add the\nsoftmax output by the two classi\ufb01ers to make the prediction. We evaluate these\nthree options in Section 5.\n5 Experiments\nThis section presents a few analytical experiments and results. We \ufb01rst discuss\nour distillation strategy. Then we comparatively analyze the ef\ufb01ciency and\naccuracy of convnets and vision transformers.\n5.1 Transformer models\nAs mentioned earlier, our architecture design is identical to the one proposed\nby Dosovitskiy et al. [15] with no convolutions. Our only differences are the\ntraining strategies, and the distillation token. Also we do not use a MLP head\nfor the pre-training but only a linear classi\ufb01er. To avoid any confusion, we refer\nto the results obtained in the prior work by ViT, and pre\ufb01x ours by DeiT. If not\nspeci\ufb01ed, DeiT refers to our referent model DeiT-B, which has the same archi-\ntecture as ViT-B. When we \ufb01ne-tune DeiT at a larger resolution, we append the\nresulting operating resolution at the end, e.g, DeiT-B \u2191384. Last, when using\nour distillation procedure, we identify it with an alembic sign as DeiT\n\u2697 .\nThe parameters of ViT-B (and therefore of DeiT-B) are \ufb01xed as D = 768,\nh = 12and d = D/h = 64. We introduce two smaller models, namely DeiT-S\nand DeiT-Ti, for which we change the number of heads, keepingd\ufb01xed. Table 1\nsummarizes the models that we consider in our paper.\n8", "mimetype": "text/plain", "start_char_idx": 1286, "end_char_idx": 2706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c845544-8fec-48f6-ac89-bfe8c66193a4": {"__data__": {"id_": "3c845544-8fec-48f6-ac89-bfe8c66193a4", "embedding": null, "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "294cde77-4760-4611-8af9-5c63d4a9bcb2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4721104c2e576b26b77dcdfbe616ec28b78bf37191b66035fdcf3c31eda6285c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcd3662c-b435-4de0-81ba-9c74ff3c544c", "node_type": "1", "metadata": {}, "hash": "a67bb7c0c99c6ceceb43243c2fc2c8e6a561d9279619e781adc4093735f8ae69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Variants of our DeiT architecture. The larger model, DeiT-B, has the\nsame architecture as the ViT-B [15]. The only parameters that vary across mod-\nels are the embedding dimension and the number of heads, and we keep the\ndimension per head constant (equal to 64). Smaller models have a lower pa-\nrameter count, and a faster throughput. The throughput is measured for im-\nages at resolution 224\u00d7224.\nModel ViT model embedding #heads #layers #params training throughput\ndimension resolution (im/sec)\nDeiT-Ti N/A 192 3 12 5M 224 2536\nDeiT-S N/A 384 6 12 22M 224 940\nDeiT-B ViT-B 768 12 12 86M 224 292\nTable 2: We compare on ImageNet [42] the performance (top-1 acc., %) of the\nstudent as a function of the teacher model used for distillation.\nTeacher Student: DeiT-B\n\u2697\nModels acc. pretrain \u2191384\nDeiT-B 81.8 81.9 83.1\nRegNetY-4GF 80.0 82.7 83.6\nRegNetY-8GF 81.7 82.7 83.8\nRegNetY-12GF 82.4 83.1 84.1\nRegNetY-16GF 82.9 83.1 84.2\n5.2 Distillation\nOur distillation method produces a vision transformer that becomes on par\nwith the best convnets in terms of the trade-off between accuracy and through-\nput, see Table 5. Interestingly, the distilled model outperforms its teacher in\nterms of the trade-off between accuracy and throughput.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcd3662c-b435-4de0-81ba-9c74ff3c544c": {"__data__": {"id_": "dcd3662c-b435-4de0-81ba-9c74ff3c544c", "embedding": null, "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "294cde77-4760-4611-8af9-5c63d4a9bcb2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4721104c2e576b26b77dcdfbe616ec28b78bf37191b66035fdcf3c31eda6285c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c845544-8fec-48f6-ac89-bfe8c66193a4", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "e430c0605474edb764e160db9d85975df208c3a45ea62f4764c20aed6c994dea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interestingly, the distilled model outperforms its teacher in\nterms of the trade-off between accuracy and throughput. Our best model on\nImageNet-1k is 85.2% top-1 accuracy outperforms the best Vit-B model pre-\ntrained on JFT-300M at resolution 384 (84.15%). For reference, the current state\nof the art of 88.55% achieved with extra training data was obtained by the ViT-\nH model (600M parameters) trained on JFT-300M at resolution 512. Hereafter\nwe provide several analysis and observations.\nConvnets teachers. We have observed that using a convnet teacher gives bet-\nter performance than using a transformer. Table 2 compares distillation results\nwith different teacher architectures. The fact that the convnet is a better teacher\nis probably due to the inductive bias inherited by the transformers through\ndistillation, as explained in Abnar et al. [1]. In all of our subsequent distilla-\ntion experiments the default teacher is a RegNetY-16GF [40] (84M parameters)\nthat we trained with the same data and same data-augmentation as DeiT. This\nteacher reaches 82.9% top-1 accuracy on ImageNet.\n9", "mimetype": "text/plain", "start_char_idx": 1121, "end_char_idx": 2216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79de1b2d-6fe9-4faf-95e7-048bcd9c48ff": {"__data__": {"id_": "79de1b2d-6fe9-4faf-95e7-048bcd9c48ff", "embedding": null, "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "1f91f042e3ce282b6e93659d848806d7fba138cb9963d486f6aa26ae3efe5b26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "626ce9ad-357f-4db3-81f9-ef1b581fb00f", "node_type": "1", "metadata": {}, "hash": "c261ca35bbb0cab39e51836a80df5acc5e7eabe1a0f6a3eeb60c37296537224e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 3: Distillation experiments on Imagenet with DeiT, 300 epochs of pre-\ntraining. We report the results for our new distillation method in the last three\nrows. We separately report the performance when classifying with only one of\nthe class or distillation embeddings, and then with a classi\ufb01er taking both of\nthem as input. In the last row (class+distillation), the result correspond to the\nlate fusion of the class and distillation classi\ufb01ers.\nSupervision ImageNet top-1 (%)\nmethod\u2193 label teacher Ti 224 S 224 B 224 B \u2191384\nDeiT\u2013 no distillation \u0013 \u0017 72.2 79.8 81.8 83.1\nDeiT\u2013 usual distillation \u0017 soft 72.2 79.8 81.8 83.2\nDeiT\u2013 hard distillation \u0017 hard 74.3 80.9 83.0 84.0\nDeiT\n\u2697 : class embedding \u0013 hard 73.9 80.9 83.0 84.2\nDeiT\n\u2697 : distil. embedding \u0013 hard 74.6 81.1 83.1 84.4\nDeiT\n\u2697 : class+distillation \u0013 hard 74.5 81.2 83.4 84.5\nComparison of distillation methods. We compare the performance of differ-\nent distillation strategies in Table 3. Hard distillation signi\ufb01cantly outperforms\nsoft distillation for transformers, even when using only a class token: hard dis-\ntillation reaches 83.0% at resolution 224\u00d7224, compared to the soft distillation\naccuracy of 81.8%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "626ce9ad-357f-4db3-81f9-ef1b581fb00f": {"__data__": {"id_": "626ce9ad-357f-4db3-81f9-ef1b581fb00f", "embedding": null, "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "1f91f042e3ce282b6e93659d848806d7fba138cb9963d486f6aa26ae3efe5b26", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79de1b2d-6fe9-4faf-95e7-048bcd9c48ff", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d1ba61d8dd2216bb016f312c9e5d563eb0750e02e80654e26013a7e63d6c2ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff9cca8e-9e69-477f-b397-bfe8ae4b07ad", "node_type": "1", "metadata": {}, "hash": "e45da51ea43eb3c51b8111549dd69589c49cc5f3b03afab091aea3c91f5227fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We compare the performance of differ-\nent distillation strategies in Table 3. Hard distillation signi\ufb01cantly outperforms\nsoft distillation for transformers, even when using only a class token: hard dis-\ntillation reaches 83.0% at resolution 224\u00d7224, compared to the soft distillation\naccuracy of 81.8%. Our distillation strategy from Section 4 further improves\nthe performance, showing that the two tokens provide complementary infor-\nmation useful for classi\ufb01cation: the classi\ufb01er on the two tokens is signi\ufb01cantly\nbetter than the independent class and distillation classi\ufb01ers, which by them-\nselves already outperform the distillation baseline.\nThe distillation token gives slightly better results than the class token. It\nis also more correlated to the convnets prediction. This difference in perfor-\nmance is probably due to the fact that it bene\ufb01ts more from the inductive bias\nof convnets. We give more details and an analysis in the next paragraph. The\ndistillation token has an undeniable advantage for the initial training.\nAgreement with the teacher & inductive bias? As discussed above, the ar-\nchitecture of the teacher has an important impact. Does it inherit existing in-\nductive bias that would facilitate the training? While we believe it dif\ufb01cult to\nformally answer this question, we analyze in Table 4 the decision agreement\nbetween the convnet teacher, our image transformer DeiT learned from labels\nonly, and our transformer DeiT\n\u2697 .\nOur distilled model is more correlated to the convnet than with a trans-\nformer learned from scratch.", "mimetype": "text/plain", "start_char_idx": 875, "end_char_idx": 2430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff9cca8e-9e69-477f-b397-bfe8ae4b07ad": {"__data__": {"id_": "ff9cca8e-9e69-477f-b397-bfe8ae4b07ad", "embedding": null, "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "1f91f042e3ce282b6e93659d848806d7fba138cb9963d486f6aa26ae3efe5b26", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "626ce9ad-357f-4db3-81f9-ef1b581fb00f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fa5f8953e1905768ba442eb539906042e8f982129c44280e0eee3f0a3b183b03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Does it inherit existing in-\nductive bias that would facilitate the training? While we believe it dif\ufb01cult to\nformally answer this question, we analyze in Table 4 the decision agreement\nbetween the convnet teacher, our image transformer DeiT learned from labels\nonly, and our transformer DeiT\n\u2697 .\nOur distilled model is more correlated to the convnet than with a trans-\nformer learned from scratch. As to be expected, the classi\ufb01er associated with\nthe distillation embedding is closer to the convnet that the one associated with\nthe class embedding, and conversely the one associated with the class embed-\nding is more similar to DeiT learned without distillation. Unsurprisingly, the\njoint class+distil classi\ufb01er offers a middle ground.\n10", "mimetype": "text/plain", "start_char_idx": 2032, "end_char_idx": 2772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "574cf052-3ffe-42f0-a273-abadf79a11d6": {"__data__": {"id_": "574cf052-3ffe-42f0-a273-abadf79a11d6", "embedding": null, "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "417f751d247994520a3bff6712dd9fa6efad5fdd0b30c3b59d5509be77c4c23f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "674e7130-eadf-433e-a5ab-23d28a07124a", "node_type": "1", "metadata": {}, "hash": "a20000b54b9506d5e1590647aa9f440e899e0c880ec98bc2df9acf600fa4d442", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4: Disagreement analysis between convnet, image transformers and dis-\ntillated transformers: We report the fraction of sample classi\ufb01ed differently for\nall classi\ufb01er pairs, i.e., the rate of different decisions. We include two models\nwithout distillation (a RegNetY and DeiT-B), so that we can compare how our\ndistilled models and classi\ufb01cation heads are correlated to these teachers.\ngroundtruth no distillation DeiT\n\u2697 student (of the convnet)\nconvnet DeiT class distillation DeiT\n\u2697\ngroundtruth 0.000 0.171 0.182 0.170 0.169 0.166\nconvnet (RegNetY) 0.171 0.000 0.133 0.112 0.100 0.102\nDeiT 0.182 0.133 0.000 0.109 0.110 0.107\nDeiT\n\u2697 \u2013 class only 0.170 0.112 0.109 0.000 0.050 0.033\nDeiT\n\u2697 \u2013 distil. only 0.169 0.100 0.110 0.050 0.000 0.019\nDeiT\n\u2697 \u2013 class+distil. 0.166 0.102 0.107 0.033 0.019 0.000\nNumber of epochs. Increasing the number of epochs signi\ufb01cantly improves\nthe performance of training with distillation, see Figure 3. With 300 epochs,\nour distilled network DeiT-B\n\u2697 is already better than DeiT-B.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "674e7130-eadf-433e-a5ab-23d28a07124a": {"__data__": {"id_": "674e7130-eadf-433e-a5ab-23d28a07124a", "embedding": null, "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "417f751d247994520a3bff6712dd9fa6efad5fdd0b30c3b59d5509be77c4c23f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "574cf052-3ffe-42f0-a273-abadf79a11d6", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "8ed5ebc90a5daef1c1331b627b5abd0f89ebfd370ed13b3a82e6ebb94c7286d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330dd30f-e41c-4975-9b41-37f4f6d08565", "node_type": "1", "metadata": {}, "hash": "a8700137ae0f0a5f8f8abf3c7ce96c8a3a6c9e9393b70ede999e716035d6f1d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0.166 0.102 0.107 0.033 0.019 0.000\nNumber of epochs. Increasing the number of epochs signi\ufb01cantly improves\nthe performance of training with distillation, see Figure 3. With 300 epochs,\nour distilled network DeiT-B\n\u2697 is already better than DeiT-B. But while for the\nlatter the performance saturates with longer schedules, our distilled network\nclearly bene\ufb01ts from a longer training time.\n5.3 Ef\ufb01ciency vs accuracy: a comparative study with convnets\nIn the literature, the image classi\ufb01caton methods are often compared as a com-\npromise between accuracy and another criterion, such as FLOPs, number of\nparameters, size of the network, etc.\nWe focus in Figure 1 on the tradeoff between the throughput (images pro-\ncessed per second) and the top-1 classi\ufb01cation accuracy on ImageNet. We focus\non the popular state-of-the-art Ef\ufb01cientNet convnet, which has bene\ufb01ted from\nyears of research on convnets and was optimized by architecture search on the\nImageNet validation set.\nOur method DeiT is slightly below Ef\ufb01cientNet, which shows that we have\nalmost closed the gap between vision transformers and convnets when training\nwith Imagenet only. These results are a major improvement ( +6.3% top-1 in a\ncomparable setting) over previous ViT models trained on Imagenet1k only [15].\nFurthermore, when DeiT bene\ufb01ts from the distillation from a relatively weaker\nRegNetY to produce DeiT\n\u2697 , it outperforms Ef\ufb01cientNet. It also outperforms\nby 1% (top-1 acc.)", "mimetype": "text/plain", "start_char_idx": 770, "end_char_idx": 2217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "330dd30f-e41c-4975-9b41-37f4f6d08565": {"__data__": {"id_": "330dd30f-e41c-4975-9b41-37f4f6d08565", "embedding": null, "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "417f751d247994520a3bff6712dd9fa6efad5fdd0b30c3b59d5509be77c4c23f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "674e7130-eadf-433e-a5ab-23d28a07124a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "37c9cbe9d0de3a3b1324b4f6534ab248461252fa682ca984fa5337992d0ad10a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These results are a major improvement ( +6.3% top-1 in a\ncomparable setting) over previous ViT models trained on Imagenet1k only [15].\nFurthermore, when DeiT bene\ufb01ts from the distillation from a relatively weaker\nRegNetY to produce DeiT\n\u2697 , it outperforms Ef\ufb01cientNet. It also outperforms\nby 1% (top-1 acc.) the Vit-B model pre-trained on JFT300M at resolution 384\n(85.2% vs 84.15%), while being signi\ufb01cantly faster to train.\nTable 5 reports the numerical results in more details and additional evalu-\nations on ImageNet V2 and ImageNet Real, that have a test set distinct from\nthe ImageNet validation, which reduces over\ufb01tting on the validation set. Our\nresults show that DeiT-B\n\u2697 and DeiT-B\n\u2697 \u2191384 outperform, by some margin, the\nstate of the art on the trade-off between accuracy and inference time on GPU.\n11", "mimetype": "text/plain", "start_char_idx": 1910, "end_char_idx": 2722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76309f74-3eac-424d-a104-5596fc3a323c": {"__data__": {"id_": "76309f74-3eac-424d-a104-5596fc3a323c", "embedding": null, "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "172568fe9bcacdaa408e12f88450eb814733f553eeb47ab6d115cc7007e33ae3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdcf656f-a237-495e-92ee-d54f4a6a8542", "node_type": "1", "metadata": {}, "hash": "8196a3282400fd882d05ad4999d54a274da259d2bc0ce6829e20324a78a3b5a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "image throughputImNet Real V2\nNetwork #param. size (image/s) top-1 top-1 top-1\nConvnets\nResNet-18 [21] 12M 2242 4458.4 69.8 77.3 57.1\nResNet-50 [21] 25M 2242 1226.1 76.2 82.5 63.3\nResNet-101 [21] 45M 2242 753.6 77.4 83.7 65.7\nResNet-152 [21] 60M 2242 526.4 78.3 84.1 67.0\nRegNetY-4GF [40]\u22c6 21M 2242 1156.7 80.0 86.4 69.4\nRegNetY-8GF [40]\u22c6 39M 2242 591.6 81.7 87.4 70.8\nRegNetY-16GF [40]\u22c6 84M 2242 334.7 82.9 88.1 72.4\nEf\ufb01cientNet-B0 [48] 5M 2242 2694.3 77.1 83.5 64.3\nEf\ufb01cientNet-B1 [48] 8M 2402 1662.5 79.1 84.9 66.9\nEf\ufb01cientNet-B2 [48] 9M 2602 1255.7 80.1 85.9 68.8\nEf\ufb01cientNet-B3 [48] 12M 3002 732.1 81.6 86.8 70.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdcf656f-a237-495e-92ee-d54f4a6a8542": {"__data__": {"id_": "bdcf656f-a237-495e-92ee-d54f4a6a8542", "embedding": null, "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "172568fe9bcacdaa408e12f88450eb814733f553eeb47ab6d115cc7007e33ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76309f74-3eac-424d-a104-5596fc3a323c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "8c0dc5cf44692db0153d19da75dd0b328a6e8cba0f18d922fc439cae99a2111e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c366b789-6c1d-4064-8b47-f2eba02196fb", "node_type": "1", "metadata": {}, "hash": "1bbf9c916de4b20cec06afa5bed4dcde81a504990caf5d484b366a9d2803c2cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 79.1 84.9 66.9\nEf\ufb01cientNet-B2 [48] 9M 2602 1255.7 80.1 85.9 68.8\nEf\ufb01cientNet-B3 [48] 12M 3002 732.1 81.6 86.8 70.6\nEf\ufb01cientNet-B4 [48] 19M 3802 349.4 82.9 88.0 72.3\nEf\ufb01cientNet-B5 [48] 30M 4562 169.1 83.6 88.3 73.6\nEf\ufb01cientNet-B6 [48] 43M 5282 96.9 84.0 88.8 73.9\nEf\ufb01cientNet-B7 [48] 66M 6002 55.1 84.3\nEf\ufb01cientNet-B5 RA [12] 30M 4562 96.9 83.7\nEf\ufb01cientNet-B7 RA [12] 66M 6002 55.1 84.7\nKDforAA-B8 87M 8002 25.2 85.8\nTransformers\nViT-B/16 [15] 86M 3842 85.9 77.9 83.6\nViT-L/16 [15] 307M 3842 27.3 76.5 82.2\nDeiT-Ti 5M 2242 2536.5 72.2 80.1 60.4\nDeiT-S 22M 2242 940.4 79.8 85.7 68.5\nDeiT-B 86M 2242 292.3 81.", "mimetype": "text/plain", "start_char_idx": 501, "end_char_idx": 1110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c366b789-6c1d-4064-8b47-f2eba02196fb": {"__data__": {"id_": "c366b789-6c1d-4064-8b47-f2eba02196fb", "embedding": null, "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "172568fe9bcacdaa408e12f88450eb814733f553eeb47ab6d115cc7007e33ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdcf656f-a237-495e-92ee-d54f4a6a8542", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "db97aaa36c056c3c6c9289f196237f097bc58d5f3d445adeebe24f29f673b576", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28447695-b388-4789-a2fc-37104a68e88e", "node_type": "1", "metadata": {}, "hash": "eb5ca7f375c5da8383bd3cf68c5679abe89ecf2da9f3ff77d25ca57e757c45b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\nViT-L/16 [15] 307M 3842 27.3 76.5 82.2\nDeiT-Ti 5M 2242 2536.5 72.2 80.1 60.4\nDeiT-S 22M 2242 940.4 79.8 85.7 68.5\nDeiT-B 86M 2242 292.3 81.8 86.7 71.5\nDeiT-B\u2191384 86M 3842 85.9 83.1 87.7 72.4\nDeiT-Ti\n\u2697 6M 2242 2529.5 74.5 82.1 62.9\nDeiT-S\n\u2697 22M 2242 936.2 81.2 86.8 70.0\nDeiT-B\n\u2697 87M 2242 290.9 83.4 88.3 73.2\nDeiT-Ti\n\u2697 / 1000 epochs 6M 2242 2529.5 76.6 83.9 65.4\nDeiT-S\n\u2697 / 1000 epochs 22M 2242 936.2 82.6 87.8 71.7\nDeiT-B\n\u2697 / 1000 epochs 87M 2242 290.9 84.2 88.7 73.9\nDeiT-B\n\u2697 \u2191384 87M 3842 85.8 84.5 89.0 74.8\nDeiT-B\n\u2697 \u2191384 / 1000 epochs 87M 3842 85.8 85.2 89.3 75.", "mimetype": "text/plain", "start_char_idx": 969, "end_char_idx": 1538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28447695-b388-4789-a2fc-37104a68e88e": {"__data__": {"id_": "28447695-b388-4789-a2fc-37104a68e88e", "embedding": null, "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "172568fe9bcacdaa408e12f88450eb814733f553eeb47ab6d115cc7007e33ae3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c366b789-6c1d-4064-8b47-f2eba02196fb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "e797681d938b9c477cddaf796fd1c53db4bcb6fff62e1ec912413e45cb149ede", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 84.2 88.7 73.9\nDeiT-B\n\u2697 \u2191384 87M 3842 85.8 84.5 89.0 74.8\nDeiT-B\n\u2697 \u2191384 / 1000 epochs 87M 3842 85.8 85.2 89.3 75.2\nTable 5: Throughput on and accuracy on Imagenet [42], Imagenet Real [5] and\nImagenet V2 matched frequency [41] of DeiT and of several state-of-the-art\nconvnets, for models trained with no external data. The throughput is mea-\nsured as the number of images that we can process per second on one 16GB\nV100 GPU. For each model we take the largest possible batch size for the usual\nresolution of the model and calculate the average time over 30 runs to process\nthat batch. With that we calculate the number of images processed per second.\nThroughput can vary according to the implementation: for a direct comparison\nand in order to be as fair as possible, we use for each model the de\ufb01nition in\nthe same GitHub [55] repository.\n\u22c6 : Regnet optimized with a similar optimization procedure as ours, which boosts the\nresults. These networks serve as teachers when we use our distillation strategy.\n12", "mimetype": "text/plain", "start_char_idx": 1423, "end_char_idx": 2432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1007bb23-7ced-4885-8f14-fcbc7981bee2": {"__data__": {"id_": "1007bb23-7ced-4885-8f14-fcbc7981bee2", "embedding": null, "metadata": {"page_label": "13", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2185a971-8eb2-49e8-8439-c47cba87c9c6", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "2968fbf665a6b66809bf1b119c499dd5c0d282b35c6c3d85445db76ff4b6ef54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2697\u2191\u2697\nFigure 3: Distillation on ImageNet [42] with DeiT-B: performance as a func-\ntion of the number of training epochs. We provide the performance without\ndistillation (horizontal dotted line) as it saturates after 400 epochs.\n5.4 Transfer learning: Performance on downstream tasks\nAlthough DeiT perform very well on ImageNet it is important to evaluate them\non other datasets with transfer learning in order to measure the power of gen-\neralization of DeiT. We evaluated this on transfer learning tasks by \ufb01ne-tuning\non the datasets in Table 6. Table 7 compares DeiT transfer learning results to\nthose of ViT [15] and state of the art convolutional architectures [48]. DeiT is\non par with competitive convnet models, which is in line with our previous\nconclusion on ImageNet.\nComparison vs training from scratch. We investigate the performance when\ntraining from scratch on a small dataset, without Imagenet pre-training. We\nget the following results on the small CIFAR-10, which is small both w.r.t. the\nnumber of images and labels:\nMethod RegNetY-16GF DeiT-B DeiT-B\n\u2697\nTop-1 98.0 97.5 98.5\nFor this experiment, we tried we get as close as possible to the Imagenet\npre-training counterpart, meaning that (1) we consider longer training sched-\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80e9038c-ef38-4952-9a2d-3adffc7475cb": {"__data__": {"id_": "80e9038c-ef38-4952-9a2d-3adffc7475cb", "embedding": null, "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "9b94257b7a671b97149acc69b34e41574182c85588dbff2c453ef579a535b83c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7", "node_type": "1", "metadata": {}, "hash": "6f1c0a4acaa3b54c47dd45949372a33582354434b023a507a2c66420e44c5bc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 6: Datasets used for our different tasks.\nDataset Train size Test size #classes\nImageNet [42] 1,281,167 50,000 1000\niNaturalist 2018 [26] 437,513 24,426 8,142\niNaturalist 2019 [27] 265,240 3,003 1,010\nFlowers-102 [38] 2,040 6,149 102\nStanford Cars [30] 8,144 8,041 196\nCIFAR-100 [31] 50,000 10,000 100\nCIFAR-10 [31] 50,000 10,000 10\nTable 7: We compare Transformers based models on different transfer learning\ntask with ImageNet pre-training. We also report results with convolutional\narchitectures for reference.\nModel ImageNetCIFAR-10 CIFAR-100 Flowers Cars iNat-18 iNat-19im/sec\nGra\ufb01t ResNet-50 [49]79.6 98.2 92.5 69.8 75.9 1226.1\nGra\ufb01t RegNetY-8GF [49] 99.0 94.0 76.8 80.0 591.6\nResNet-152 [10] 69.1 526.3\nEf\ufb01cientNet-B7 [48] 84.3 98.9 91.7 98.8 94.7 55.1\nViT-B/32 [15] 73.4 97.8 86.3 85.4 394.5\nViT-B/16 [15] 77.9 98.1 87.1 89.5 85.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7": {"__data__": {"id_": "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7", "embedding": null, "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "9b94257b7a671b97149acc69b34e41574182c85588dbff2c453ef579a535b83c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80e9038c-ef38-4952-9a2d-3adffc7475cb", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "bd3f09fef9bff97f22eff20821d87efca546fb4b6b4e4509490ad209c2fc2f05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d2b765-aec9-45aa-a1db-4504f761fb26", "node_type": "1", "metadata": {}, "hash": "2bf81e5c9ff9312bcb9f64660f3df24634b905d097732c72ccb1e0dbd4cf9064", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 526.3\nEf\ufb01cientNet-B7 [48] 84.3 98.9 91.7 98.8 94.7 55.1\nViT-B/32 [15] 73.4 97.8 86.3 85.4 394.5\nViT-B/16 [15] 77.9 98.1 87.1 89.5 85.9\nViT-L/32 [15] 71.2 97.9 87.1 86.4 124.1\nViT-L/16 [15] 76.5 97.9 86.4 89.7 27.3\nDeiT-B 81.8 99.1 90.8 98.4 92.1 73.2 77.7 292.3\nDeiT-B\u2191384 83.1 99.1 90.8 98.5 93.3 79.5 81.4 85.9\nDeiT-B\n\u2697 83.4 99.1 91.3 98.8 92.9 73.7 78.4 290.9\nDeiT-B\n\u2697\u2191384 84.4 99.2 91.4 98.9 93.9 80.1 83.0 85.9\nules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that\nthe network has been fed a comparable number of images in total; (2) we re-\nscale images to 224 \u00d7224 to ensure that we have the same augmentation.", "mimetype": "text/plain", "start_char_idx": 708, "end_char_idx": 1347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47d2b765-aec9-45aa-a1db-4504f761fb26": {"__data__": {"id_": "47d2b765-aec9-45aa-a1db-4504f761fb26", "embedding": null, "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "9b94257b7a671b97149acc69b34e41574182c85588dbff2c453ef579a535b83c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "f6daac8a49e4b4c07ef7ae0bd84d66471ef0489f553a9c316b4aa3eeb2b57d9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 99.2 91.4 98.9 93.9 80.1 83.0 85.9\nules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that\nthe network has been fed a comparable number of images in total; (2) we re-\nscale images to 224 \u00d7224 to ensure that we have the same augmentation. The\nresults are not as good as with Imagenet pre-training (98.5% vs 99.1%), which\nis expected since the network has seen a much lower diversity. However they\nshow that it is possible to learn a reasonable transformer on CIFAR-10 only.\n6 Training details & ablation\nIn this section we discuss the DeiT training strategy to learn vision transform-\ners in a data-ef\ufb01cient manner. We build upon PyTorch [39] and the timm li-\nbrary [55]2. We provide hyper-parameters as well as an ablation study in which\nwe analyze the impact of each choice.\nInitialization and hyper-parameters. Transformers are relatively sensitive to\ninitialization. After testing several options in preliminary experiments, some\n2The timm implementation already included a training procedure that improved the accuracy\nof ViT-B from 77.91% to 79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine.\n14", "mimetype": "text/plain", "start_char_idx": 1089, "end_char_idx": 2227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7451320-0b38-465e-8641-45e2fc37823c": {"__data__": {"id_": "f7451320-0b38-465e-8641-45e2fc37823c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0020a1fd-69de-4308-9b30-cbec66818d18", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "dc4bc3faf7c40aa3039cf6a24a1b74889c3f798d20b60bfc00a1f4af04302267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c1cd556-0520-46ca-a237-933434d54356", "node_type": "1", "metadata": {}, "hash": "ed55973619da7d998aead4b582a32748f3c692b2cbdcbfd6b9b1c8d0a9cef8e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "top-1 accuracy\nAblation on\u2193\nPre-training\nFine-tuning\nRand-Augment\nAutoAug\nMixup\nCutMix\nErasing\nStoch. Depth\nRepeated Aug.\nDropout\nExp. Moving Avg.\npre-trained2242\n\ufb01ne-tuned3842\nnone: DeiT-B adamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 81.8\u00b10.2 83.1\u00b10.1\noptimizer SGD adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 74.5 77.3\nadamw SGD \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 81.8 83.1\ndata\naugmentation\nadamw adamw \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 79.6 80.4\nadamw adamw \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 81.2 81.9\nadamw adamw \u0013 \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 78.7 79.8\nadamw adamw \u0013 \u0017 \u0013 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 80.0 80.6\nadamw adamw \u0013 \u0017 \u0017 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 75.8 76.7\nregularization\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 \u0017 \u0017 4.3* 0.1\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0017 \u0013 \u0017 \u0017 3.4* 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c1cd556-0520-46ca-a237-933434d54356": {"__data__": {"id_": "9c1cd556-0520-46ca-a237-933434d54356", "embedding": null, "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0020a1fd-69de-4308-9b30-cbec66818d18", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "dc4bc3faf7c40aa3039cf6a24a1b74889c3f798d20b60bfc00a1f4af04302267", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7451320-0b38-465e-8641-45e2fc37823c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "8be12092bea83574ae978daa262c76f7b7c6f43802e8de9f0f153005ec9d0a66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd625431-0963-404c-a8d4-83bf8ccf4644", "node_type": "1", "metadata": {}, "hash": "d3b213ff0b4ebc2cf689824444c7bb45080f8bbb31862b8c0fc9404b9b1fb2ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\nadamw adamw \u0013 \u0017 \u0017 \u0017 \u0013 \u0013 \u0013 \u0017 \u0017 75.8 76.7\nregularization\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 \u0017 \u0017 4.3* 0.1\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0017 \u0013 \u0017 \u0017 3.4* 0.1\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0017 \u0017 \u0017 76.5 77.4\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 81.3 83.1\nadamw adamw \u0013 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0017 \u0013 81.9 83.1\nTable 8: Ablation study on training methods on ImageNet [42]. The top row\n(\u201dnone\u201d) corresponds to our default con\ufb01guration employed for DeiT. The\nsymbols \u0013 and \u0017 indicates that we use and do not use the corresponding\nmethod, respectively. We report the accuracy scores (%) after the initial train-\ning at resolution 224 \u00d7224, and after \ufb01ne-tuning at resolution 384 \u00d7384. The\nhyper-parameters are \ufb01xed according to Table 9, and may be suboptimal.\n* indicates that the model did not train well, possibly because hyper-parameters are not adapted.\nof them not converging, we follow the recommendation of Hanin and Rol-\nnick [20] to initialize the weights with a truncated normal distribution.\nTable 9 indicates the hyper-parameters that we use by default at training\ntime for all our experiments, unless stated otherwise. For distillation we follow\nthe recommendations from Cho et al.", "mimetype": "text/plain", "start_char_idx": 500, "end_char_idx": 1635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd625431-0963-404c-a8d4-83bf8ccf4644": {"__data__": {"id_": "dd625431-0963-404c-a8d4-83bf8ccf4644", "embedding": null, "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0020a1fd-69de-4308-9b30-cbec66818d18", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "dc4bc3faf7c40aa3039cf6a24a1b74889c3f798d20b60bfc00a1f4af04302267", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c1cd556-0520-46ca-a237-933434d54356", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "7b72b34ecdf71f7ae8acf870481783839e55c887f6bde706135e7390e97fe7c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "* indicates that the model did not train well, possibly because hyper-parameters are not adapted.\nof them not converging, we follow the recommendation of Hanin and Rol-\nnick [20] to initialize the weights with a truncated normal distribution.\nTable 9 indicates the hyper-parameters that we use by default at training\ntime for all our experiments, unless stated otherwise. For distillation we follow\nthe recommendations from Cho et al. [9] to select the parameters \u03c4 and \u03bb. We\ntake the typical values \u03c4 = 3.0 and \u03bb= 0.1 for the usual (soft) distillation.\nData-Augmentation. Compared to models that integrate more priors (such\nas convolutions), transformers require a larger amount of data. Thus, in order\nto train with datasets of the same size, we rely on extensive data augmentation.\nWe evaluate different types of strong data augmentation, with the objective to\nreach a data-ef\ufb01cient training regime.\nAuto-Augment [11], Rand-Augment [12], and random erasing [62] im-\nprove the results. For the two latter we use the timm [55] customizations, and\nafter ablation we choose Rand-Augment instead of AutoAugment. Overall our\nexperiments con\ufb01rm that transformers require a strong data augmentation: al-\nmost all the data-augmentation methods that we evaluate prove to be useful.\nOne exception is dropout, which we exclude from our training procedure.\n15", "mimetype": "text/plain", "start_char_idx": 1201, "end_char_idx": 2550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48bb1b04-df50-4bc9-b489-44a689eb3257": {"__data__": {"id_": "48bb1b04-df50-4bc9-b489-44a689eb3257", "embedding": null, "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4eda8d9-bbc2-4c72-87e3-4c28064eb001", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "ae586ea113c5b17223a674271add723e72963c9e7cbe89ed841539789621c9d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed7f4367-0b6f-4717-9ece-6a572acb0f0d", "node_type": "1", "metadata": {}, "hash": "51fe38c046b7a546269fef6e12fc8408d5acbaf847ab19b208957527fac2b3f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Methods ViT-B [15] DeiT-B\nEpochs 300 300\nBatch size 4096 1024\nOptimizer AdamW AdamW\nlearning rate 0.003 0.0005\u00d7batchsize\n512\nLearning rate decay cosine cosine\nWeight decay 0.3 0.05\nWarmup epochs 3.4 5\nLabel smoothing\u03b5 \u0017 0.1\nDropout 0.1 \u0017\nStoch. Depth \u0017 0.1\nRepeated Aug \u0017 \u0013\nGradient Clip. \u0013 \u0017\nRand Augment \u0017 9/0.5\nMixup prob. \u0017 0.8\nCutmix prob. \u0017 1.0\nErasing prob. \u0017 0.25\nTable 9: Ingredients and hyper-parameters for our method and Vit-B.\nRegularization & Optimizers. We have considered different optimizers and\ncross-validated different learning rates and weight decays. Transformers are\nsensitive to the setting of optimization hyper-parameters. Therefore, during\ncross-validation, we tried 3 different learning rates (5.10\u22124,3.10\u22124,5.10\u22125) and\n3 weight decay ( 0.03, 0.04, 0.05). We scale the learning rate according to the\nbatch size with the formula: lrscaled = lr\n512 \u00d7batchsize, similarly to Goyal et\nal. [19] except that we use 512 instead of 256 as the base value.\nThe best results use the AdamW optimizer with the same learning rates as\nViT [15] but with a much smaller weight decay, as the weight decay reported\nin the paper hurts the convergence in our setting.\nWe have employed stochastic depth [29], which facilitates the convergence\nof transformers, especially deep ones [16, 17].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed7f4367-0b6f-4717-9ece-6a572acb0f0d": {"__data__": {"id_": "ed7f4367-0b6f-4717-9ece-6a572acb0f0d", "embedding": null, "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4eda8d9-bbc2-4c72-87e3-4c28064eb001", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "ae586ea113c5b17223a674271add723e72963c9e7cbe89ed841539789621c9d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48bb1b04-df50-4bc9-b489-44a689eb3257", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "676639e775d039ecb0576610d08da01f1c2069f5f454a5f215f2d376e3d147c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] except that we use 512 instead of 256 as the base value.\nThe best results use the AdamW optimizer with the same learning rates as\nViT [15] but with a much smaller weight decay, as the weight decay reported\nin the paper hurts the convergence in our setting.\nWe have employed stochastic depth [29], which facilitates the convergence\nof transformers, especially deep ones [16, 17]. For vision transformers, they\nwere \ufb01rst adopted in the training procedure by Wightman [55]. Regularization\nlike Mixup [60] and Cutmix [59] improve performance. We also use repeated\naugmentation [4, 25], which provides a signi\ufb01cant boost in performance and is\none of the key ingredients of our proposed training procedure.\nExponential Moving Average (EMA). We evaluate the EMA of our network\nobtained after training. There are small gains, which vanish after \ufb01ne-tuning:\nthe EMA model has an edge of is 0.1 accuracy points, but when \ufb01ne-tuned the\ntwo models reach the same (improved) performance.\nFine-tuning at different resolution. We adopt the \ufb01ne-tuning procedure from\nTouvron et al. [51]: our schedule, regularization and optimization procedure\nare identical to that of FixEf\ufb01cientNet but we keep the training-time data aug-\n16", "mimetype": "text/plain", "start_char_idx": 913, "end_char_idx": 2128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "087c61fa-a077-4685-abb8-113ddffd73e8": {"__data__": {"id_": "087c61fa-a077-4685-abb8-113ddffd73e8", "embedding": null, "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "7ff723bcf6acd69821a2e24313cab548083886057a5a526070e3e0aff3c41d06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1c33251-f787-41f9-9c02-75127bc54f68", "node_type": "1", "metadata": {}, "hash": "8ce740a7ec7f9e9aedc9d42c9bcfadda5555ebe2054945af583e97ffc08b1070", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "image throughput Imagenet [42] Real [5] V2 [41]\nsize (image/s) acc. top-1 acc. top-1 acc. top-1\n1602 609.31 79.9 84.8 67.6\n2242 291.05 81.8 86.7 71.5\n3202 134.13 82.7 87.2 71.9\n3842 85.87 83.1 87.7 72.4\nTable 10: Performance of DeiT trained at size 2242 for varying \ufb01netuning sizes\non ImageNet-1k, ImageNet-Real and ImageNet-v2 matched frequency.\nmentation (contrary to the dampened data augmentation of Touvron et al. [51]).\nWe also interpolate the positional embeddings: In principle any classical image\nscaling technique, like bilinear interpolation, could be used. However, a bilin-\near interpolation of a vector from its neighbors reduces its \u21132-norm compared\nto its neighbors. These low-norm vectors are not adapted to the pre-trained\ntransformers and we observe a signi\ufb01cant drop in accuracy if we employ use\ndirectly without any form of \ufb01ne-tuning. Therefore we adopt a bicubic interpo-\nlation that approximately preserves the norm of the vectors, before \ufb01ne-tuning\nthe network with either AdamW [36] or SGD. These optimizers have a similar\nperformance for the \ufb01ne-tuning stage, see Table 8.\nBy default and similar to ViT [15] we train DeiT models with at resolution\n224 and we \ufb01ne-tune at resolution 384. We detail how to do this interpolation\nin Section 3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1c33251-f787-41f9-9c02-75127bc54f68": {"__data__": {"id_": "d1c33251-f787-41f9-9c02-75127bc54f68", "embedding": null, "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "7ff723bcf6acd69821a2e24313cab548083886057a5a526070e3e0aff3c41d06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "087c61fa-a077-4685-abb8-113ddffd73e8", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "862fca713f16a53c9aed34fd6e7336237593394db5b0593845623a025f5b9472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db7b7e94-8d71-4422-8590-4c32a47334a3", "node_type": "1", "metadata": {}, "hash": "85a95a93eb32c484f7d2d8a191855bb364c4116b9e1fc38a2bdc7260bc7292cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These optimizers have a similar\nperformance for the \ufb01ne-tuning stage, see Table 8.\nBy default and similar to ViT [15] we train DeiT models with at resolution\n224 and we \ufb01ne-tune at resolution 384. We detail how to do this interpolation\nin Section 3. However, in order to measure the in\ufb02uence of the resolution we\nhave \ufb01netuned DeiT at different resolutions. We report these results in Table 10.\nTraining time. A typical training of 300 epochs takes 37 hours with 2 nodes\nor 53 hours on a single node for the DeiT-B.As a comparison point, a similar\ntraining with a RegNetY-16GF [40] (84M parameters) is 20% slower. DeiT-S and\nDeiT-Ti are trained in less than 3 days on 4 GPU. Then, optionally we \ufb01ne-tune\nthe model at a larger resolution. This takes 20 hours on a single node (8 GPU)\nto produce a FixDeiT-B model at resolution 384\u00d7384, which corresponds to 25\nepochs. Not having to rely on batch-norm allows one to reduce the batch size\nwithout impacting performance, which makes it easier to train larger models.\nNote that, since we use repeated augmentation [4, 25] with 3 repetitions, we\nonly see one third of the images during a single epoch3.\n7 Conclusion\nIn this paper, we have introduced DeiT, which are image transformers that\ndo not require very large amount of data to be trained, thanks to improved\n3Formally it means that we have 100 epochs, but each is 3x longer because of the repeated\naugmentations.", "mimetype": "text/plain", "start_char_idx": 1017, "end_char_idx": 2430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db7b7e94-8d71-4422-8590-4c32a47334a3": {"__data__": {"id_": "db7b7e94-8d71-4422-8590-4c32a47334a3", "embedding": null, "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "7ff723bcf6acd69821a2e24313cab548083886057a5a526070e3e0aff3c41d06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1c33251-f787-41f9-9c02-75127bc54f68", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "5d94178c394f0481004f805eeab94a4b3f2c2afa981b9de908dd67e6b9e28e6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that, since we use repeated augmentation [4, 25] with 3 repetitions, we\nonly see one third of the images during a single epoch3.\n7 Conclusion\nIn this paper, we have introduced DeiT, which are image transformers that\ndo not require very large amount of data to be trained, thanks to improved\n3Formally it means that we have 100 epochs, but each is 3x longer because of the repeated\naugmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the\neffective training time with and without repeated augmentation.\n17", "mimetype": "text/plain", "start_char_idx": 2030, "end_char_idx": 2582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b992d9ed-345b-4c22-861a-255f27489f43": {"__data__": {"id_": "b992d9ed-345b-4c22-861a-255f27489f43", "embedding": null, "metadata": {"page_label": "18", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a233e33-461e-4390-be11-fc954f157193", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "ce694ce5c9b64296f17dc393c01cf303fcddc799ca34eb381994fe60b2fbf35d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "training and in particular a novel distillation procedure. Convolutional neu-\nral networks have optimized, both in terms of architecture and optimization\nduring almost a decade, including through extensive architecture search that\nis prone to over\ufb01ting, as it is the case for instance for Ef\ufb01cientNets [51]. For\nDeiT we have started the existing data augmentation and regularization strate-\ngies pre-existing for convnets, not introducing any signi\ufb01cant architectural be-\nyond our novel distillation token. Therefore it is likely that research on data-\naugmentation more adapted or learned for transformers will bring further gains.\nTherefore, considering our results, where image transformers are on par\nwith convnets already, we believe that they will rapidly become a method of\nchoice considering their lower memory footprint for a given accuracy.\nWe provide an open-source implementation of our method. It is available\nat https://github.com/facebookresearch/deit.\nAcknowledgements\nMany thanks to Ross Wightman for sharing his ViT code and bootstrapping\ntraining method with the community, as well as for valuable feedback that\nhelped us to \ufb01x different aspects of this paper. Thanks to Vinicius Reis, Mannat\nSingh, Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other colleagues at\nFacebook for brainstorming and some exploration on this axis. Thanks to Ross\nGirshick and Piotr Dollar for constructive comments.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d97c231b-cb31-41f5-b4d7-f162d1a5c684": {"__data__": {"id_": "d97c231b-cb31-41f5-b4d7-f162d1a5c684", "embedding": null, "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ca4c11b6b744e21407d8637bd227d1e72fc7576e86c7dbc08b29e8f9155c23b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bc9ae56-de19-4255-b364-881a666e963a", "node_type": "1", "metadata": {}, "hash": "1007a2f7d58ca7e8d03e061a61edb3754950360b4601b4dd0c31c6f07b5c6bfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] Samira Abnar, Mostafa Dehghani, and Willem Zuidema. Transferring inductive\nbiases through knowledge distillation. arXiv preprint arXiv:2006.00555, 2020.\n[2] Jie Hu andLi Shen and Gang Sun. Squeeze-and-excitation networks. arXiv preprint\narXiv:1709.01507, 2017.\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\n[4] Maxim Berman, Herv \u00b4e J \u00b4egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs\nDouze. Multigrain: a uni\ufb01ed image embedding for classes and instances. arXiv\npreprint arXiv:1902.05509, 2019.\n[5] Lucas Beyer, Olivier J. H \u00b4enaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron\nvan den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\nKirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision, 2020.\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. Generative pretraining from pixels. In International Conference\non Machine Learning, 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bc9ae56-de19-4255-b364-881a666e963a": {"__data__": {"id_": "5bc9ae56-de19-4255-b364-881a666e963a", "embedding": null, "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ca4c11b6b744e21407d8637bd227d1e72fc7576e86c7dbc08b29e8f9155c23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d97c231b-cb31-41f5-b4d7-f162d1a5c684", "node_type": "1", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "9a21717130dbcb3c5cc7380561a6884571cfe69cb276d4171074b8e44e64a6c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7244ecde-0cf7-43c8-87b0-93fc1fd148cb", "node_type": "1", "metadata": {}, "hash": "8811c0e6c01542fb3c72d876a5c1ec83745229bdcca16daa09d7579930dbe414", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision, 2020.\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. Generative pretraining from pixels. In International Conference\non Machine Learning, 2020.\n[8] Yen-Chun Chen, Linjie Li, Licheng Yu, A. E. Kholy, Faisal Ahmed, Zhe Gan, Y.\nCheng, and Jing jing Liu. Uniter: Universal image-text representation learning. In\nEuropean Conference on Computer Vision, 2020.\n[9] J. H. Cho and B. Hariharan. On the ef\ufb01cacy of knowledge distillation. International\nConference on Computer Vision, 2019.\n[10] P . Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation\nfor long-tailed data. arXiv preprint arXiv:2008.03673, 2020.\n[11] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man \u00b4e, Vijay Vasudevan, and Quoc V .\nLe. Autoaugment: Learning augmentation policies from data. arXiv preprint\narXiv:1805.09501, 2018.\n[12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment:\nPractical automated data augmentation with a reduced search space.arXiv preprint\narXiv:1909.13719, 2019.\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.", "mimetype": "text/plain", "start_char_idx": 865, "end_char_idx": 2071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7244ecde-0cf7-43c8-87b0-93fc1fd148cb": {"__data__": {"id_": "7244ecde-0cf7-43c8-87b0-93fc1fd148cb", "embedding": null, "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ca4c11b6b744e21407d8637bd227d1e72fc7576e86c7dbc08b29e8f9155c23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bc9ae56-de19-4255-b364-881a666e963a", "node_type": "1", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "38c3ff95ce02af54355f7db32ed01901807765d346e2fb8b300da611116c3bc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2de6354-44ed-4350-9588-2174cab589fe", "node_type": "1", "metadata": {}, "hash": "2e54f69521436f5e221046b4ad181d1bd8d147e2fb23c18161c4a16324149bdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment:\nPractical automated data augmentation with a reduced search space.arXiv preprint\narXiv:1909.13719, 2019.\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[16] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth\non demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR\n2020.\n[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R \u00b4emi Gribonval,\nHerv\u00b4e J \u00b4egou, and Armand Joulin. Training with quantization noise for extreme\nmodel compression.", "mimetype": "text/plain", "start_char_idx": 1811, "end_char_idx": 3042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2de6354-44ed-4350-9588-2174cab589fe": {"__data__": {"id_": "d2de6354-44ed-4350-9588-2174cab589fe", "embedding": null, "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "3ca4c11b6b744e21407d8637bd227d1e72fc7576e86c7dbc08b29e8f9155c23b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7244ecde-0cf7-43c8-87b0-93fc1fd148cb", "node_type": "1", "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "f7efc4c03c6b86de9a6cfebe85f2c9789cb28cce41a8aed825c3447c84ea28ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reducing transformer depth\non demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR\n2020.\n[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R \u00b4emi Gribonval,\nHerv\u00b4e J \u00b4egou, and Armand Joulin. Training with quantization noise for extreme\nmodel compression. arXiv preprint arXiv:2004.07320, 2020.\n19", "mimetype": "text/plain", "start_char_idx": 2751, "end_char_idx": 3084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55cc781a-d5d2-417f-bb03-c381d3c5bc7e": {"__data__": {"id_": "55cc781a-d5d2-417f-bb03-c381d3c5bc7e", "embedding": null, "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d4658ef81f835eb1660bea6060cf1ab2d49d43c50fed558f1080855d9ccc76be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdea54e1-4276-452f-9d2d-7ad83bc90991", "node_type": "1", "metadata": {}, "hash": "6363971a8b50b68625cbcec791a4b624e3bc72cfe555f168fb5d8daa751126e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\nConvolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122 ,\n2017.\n[19] Priya Goyal, Piotr Doll \u00b4ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski,\nAapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large\nminibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n[20] Boris Hanin and David Rolnick. How to start training: The effect of initialization\nand architecture. NIPS, 31, 2018.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Conference on Computer Vision and Pattern Recognition ,\nJune 2016.\n[22] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag\nof tricks for image classi\ufb01cation with convolutional neural networks. In Conference\non Computer Vision and Pattern Recognition, 2019.\n[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415, 2016.\n[24] Geoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531, 2015.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdea54e1-4276-452f-9d2d-7ad83bc90991": {"__data__": {"id_": "bdea54e1-4276-452f-9d2d-7ad83bc90991", "embedding": null, "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d4658ef81f835eb1660bea6060cf1ab2d49d43c50fed558f1080855d9ccc76be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55cc781a-d5d2-417f-bb03-c381d3c5bc7e", "node_type": "1", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "c0623f6418850422145c41ee52c6d081d3bf1c607beff5eaa4564990d05a9422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22954da0-96af-4adf-97b2-1b84cfa13e0b", "node_type": "1", "metadata": {}, "hash": "836be06568bcf16d1d1fd565597b345a45b55ed141cab65450af01fa10399b38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415, 2016.\n[24] Geoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531, 2015.\n[25] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoe\ufb02er, and Daniel\nSoudry. Augment your batch: Improving generalization through instance repeti-\ntion. In Conference on Computer Vision and Pattern Recognition, 2020.\n[26] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.\narXiv preprint arXiv:1707.06642, 2018.\n[27] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.\narXiv preprint arXiv:1707.06642, 2019.\n[28] H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei. Relation networks for\nobject detection. Conference on Computer Vision and Pattern Recognition, 2018.\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep\nnetworks with stochastic depth. In European Conference on Computer Vision, 2016.", "mimetype": "text/plain", "start_char_idx": 930, "end_char_idx": 2133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22954da0-96af-4adf-97b2-1b84cfa13e0b": {"__data__": {"id_": "22954da0-96af-4adf-97b2-1b84cfa13e0b", "embedding": null, "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d4658ef81f835eb1660bea6060cf1ab2d49d43c50fed558f1080855d9ccc76be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdea54e1-4276-452f-9d2d-7ad83bc90991", "node_type": "1", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "6a428076006d708a3fc732a165f84fea9c9997b9225a523c60e34756759bf832", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "087f112a-772c-471a-83f6-b573310013b4", "node_type": "1", "metadata": {}, "hash": "f62fb62c19947f2a1e96d6fb1826f4ca069312330aea4d92567ac37bae8de3b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[28] H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei. Relation networks for\nobject detection. Conference on Computer Vision and Pattern Recognition, 2018.\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep\nnetworks with stochastic depth. In European Conference on Computer Vision, 2016.\n[30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations\nfor \ufb01ne-grained categorization. In 4th International IEEE Workshop on 3D Represen-\ntation and Recognition (3dRR-13), 2013.\n[31] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical\nreport, CIFAR, 2009.\n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS, 2012.\n[33] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visu-\nalBERT: a simple and performant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019.\n[34] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.\nConference on Computer Vision and Pattern Recognition, 2019.\n[35] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahen-\ndran, Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf.", "mimetype": "text/plain", "start_char_idx": 1810, "end_char_idx": 3079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "087f112a-772c-471a-83f6-b573310013b4": {"__data__": {"id_": "087f112a-772c-471a-83f6-b573310013b4", "embedding": null, "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "d4658ef81f835eb1660bea6060cf1ab2d49d43c50fed558f1080855d9ccc76be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22954da0-96af-4adf-97b2-1b84cfa13e0b", "node_type": "1", "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "0572c67a2e3617e808faa57b2f8281e324acec1ae3f7b60877699e005b2c97d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[34] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.\nConference on Computer Vision and Pattern Recognition, 2019.\n[35] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahen-\ndran, Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf. Object-\ncentric learning with slot attention. arXiv preprint arXiv:2006.15055, 2020.\n[36] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv\npreprint arXiv:1711.05101, 2017.\n20", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 3284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "826d7428-1112-4d56-ab37-f04d8b9a4604": {"__data__": {"id_": "826d7428-1112-4d56-ab37-f04d8b9a4604", "embedding": null, "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fbfff02e998e85dd1aff92612b2a8b2e6f7f3baeff32ad02fecf75cc21ca3e61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7c758b2-fc01-4e62-926a-209f35c25703", "node_type": "1", "metadata": {}, "hash": "e2f5a1fb327a291b9634670254a7ae6da75baba711ba7f86c072a858ff1d1e98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[37] Jiasen Lu, Dhruv Batra, D. Parikh, and Stefan Lee. Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks. In NIPS,\n2019.\n[38] M-E. Nilsback and A. Zisserman. Automated \ufb02ower classi\ufb01cation over a large\nnumber of classes. In Proceedings of the Indian Conference on Computer Vision, Graph-\nics and Image Processing, 2008.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-\ntorch: An imperative style, high-performance deep learning library. InAdvances in\nneural information processing systems, pages 8026\u20138037, 2019.\n[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr\nDoll\u00b4ar. Designing network design spaces.Conference on Computer Vision and Pattern\nRecognition, 2020.\n[41] B. Recht, Rebecca Roelofs, L. Schmidt, and V . Shankar. Do imagenet classi\ufb01ers\ngeneralize to imagenet? arXiv preprint arXiv:1902.10811, 2019.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7c758b2-fc01-4e62-926a-209f35c25703": {"__data__": {"id_": "f7c758b2-fc01-4e62-926a-209f35c25703", "embedding": null, "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fbfff02e998e85dd1aff92612b2a8b2e6f7f3baeff32ad02fecf75cc21ca3e61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "826d7428-1112-4d56-ab37-f04d8b9a4604", "node_type": "1", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "ef4dce0b5a63a48581253a6e138ebf1c47612e1f39d8238ffcd70e2c3bb59f7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f42976db-5741-44ea-a05a-ea0af7fcaaa1", "node_type": "1", "metadata": {}, "hash": "87b1bfe1dac9666747da141c475e3765214e60ef964ccd544cf63d370f767594", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1902.10811, 2019.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In-\nternational journal of Computer Vision, 2015.\n[43] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, and Ching-Hui\nChen. Global self-attention networks for image recognition. arXiv preprint\narXiv:2010.03019, 2020.\n[44] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations, 2015.\n[45] C. Sun, A. Myers, Carl Vondrick, Kevin Murphy, and C. Schmid. Videobert: A\njoint model for video and language representation learning.Conference on Computer\nVision and Pattern Recognition, 2019.\n[46] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting\nunreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE\ninternational conference on computer vision, pages 843\u2013852, 2017.\n[47] Christian Szegedy, V . Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking\nthe inception architecture for computer vision. Conference on Computer Vision and\nPattern Recognition, 2016.", "mimetype": "text/plain", "start_char_idx": 970, "end_char_idx": 2267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f42976db-5741-44ea-a05a-ea0af7fcaaa1": {"__data__": {"id_": "f42976db-5741-44ea-a05a-ea0af7fcaaa1", "embedding": null, "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fbfff02e998e85dd1aff92612b2a8b2e6f7f3baeff32ad02fecf75cc21ca3e61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7c758b2-fc01-4e62-926a-209f35c25703", "node_type": "1", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "0cfa0ed23013cec4f4a02ba25c8c7ea594d9971edc467f19fd17d55ef7b589e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1203057c-eef4-4951-b51f-9c02823121e6", "node_type": "1", "metadata": {}, "hash": "3d343db46005ce5141453c22f772a8b1af1ff299f75b658da9a0d69689ac7f4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Revisiting\nunreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE\ninternational conference on computer vision, pages 843\u2013852, 2017.\n[47] Christian Szegedy, V . Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking\nthe inception architecture for computer vision. Conference on Computer Vision and\nPattern Recognition, 2016.\n[48] Mingxing Tan and Quoc V . Le. Ef\ufb01cientnet: Rethinking model scaling for convo-\nlutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n[49] Hugo Touvron, Alexandre Sablayrolles, M. Douze, M. Cord, and H. J \u00b4egou. Gra\ufb01t:\nLearning \ufb01ne-grained image representations with coarse labels. arXiv preprint\narXiv:2011.12982, 2020.\n[50] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the\ntrain-test resolution discrepancy. NIPS, 2019.\n[51] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv \u00b4e J \u00b4egou. Fixing the\ntrain-test resolution discrepancy: Fixef\ufb01cientnet. arXiv preprint arXiv:2003.08237 ,\n2020.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn NIPS, 2017.", "mimetype": "text/plain", "start_char_idx": 1912, "end_char_idx": 3076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1203057c-eef4-4951-b51f-9c02823121e6": {"__data__": {"id_": "1203057c-eef4-4951-b51f-9c02823121e6", "embedding": null, "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "fbfff02e998e85dd1aff92612b2a8b2e6f7f3baeff32ad02fecf75cc21ca3e61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42976db-5741-44ea-a05a-ea0af7fcaaa1", "node_type": "1", "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "bfc20398395289832a4a6d15e58e6d544850ef25ab026d5bda85ce0c6cb0377a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fixing the\ntrain-test resolution discrepancy: Fixef\ufb01cientnet. arXiv preprint arXiv:2003.08237 ,\n2020.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn NIPS, 2017.\n[53] X. Wang, Ross B. Girshick, A. Gupta, and Kaiming He. Non-local neural networks.\nConference on Computer Vision and Pattern Recognition, 2018.\n21", "mimetype": "text/plain", "start_char_idx": 2802, "end_char_idx": 3225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59f40e0c-bad6-43bf-9686-e4cb6bebca32": {"__data__": {"id_": "59f40e0c-bad6-43bf-9686-e4cb6bebca32", "embedding": null, "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d49ea179-3a65-4890-85ec-6c790399b666", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4eaca4b3ac5d247fdbcf537b1db6ff5093908677e974a9808f79e082fe124764", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6c66802-ae28-4892-a195-8504f27703b2", "node_type": "1", "metadata": {}, "hash": "b335b9fc70f4b818db1a4a2ea39c78046652c1ecd6b2b82e5b0e32e5375f057a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[54] Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Cir-\ncumventing outliers of autoaugment with knowledge distillation. European Con-\nference on Computer Vision, 2020.\n[55] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\n[56] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi\nTomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image\nrepresentation and processing for computer vision.arXiv preprint arXiv:2006.03677,\n2020.\n[57] Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V . Le. Self-\ntraining with noisy student improves imagenet classi\ufb01cation. arXiv preprint\narXiv:1911.04252, 2019.\n[58] L. Yuan, F. Tay, G. Li, T. Wang, and Jiashi Feng. Revisit knowledge distillation:\na teacher-free framework. Conference on Computer Vision and Pattern Recognition ,\n2020.\n[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. Cutmix: Regularization strategy to train strong classi\ufb01ers with\nlocalizable features. arXiv preprint arXiv:1905.04899, 2019.\n[60] Hongyi Zhang, Moustapha Ciss \u00b4e, Yann N. Dauphin, and David Lopez-Paz. mixup:\nBeyond empirical risk minimization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6c66802-ae28-4892-a195-8504f27703b2": {"__data__": {"id_": "f6c66802-ae28-4892-a195-8504f27703b2", "embedding": null, "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d49ea179-3a65-4890-85ec-6c790399b666", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "4eaca4b3ac5d247fdbcf537b1db6ff5093908677e974a9808f79e082fe124764", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59f40e0c-bad6-43bf-9686-e4cb6bebca32", "node_type": "1", "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}, "hash": "87acd4da59f92cc9cd639786c7c9de5b83c7461df763971bd731eade5bd7eff2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cutmix: Regularization strategy to train strong classi\ufb01ers with\nlocalizable features. arXiv preprint arXiv:1905.04899, 2019.\n[60] Hongyi Zhang, Moustapha Ciss \u00b4e, Yann N. Dauphin, and David Lopez-Paz. mixup:\nBeyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n[61] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue\nSun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. Resnest:\nSplit-attention networks. arXiv preprint arXiv:2004.08955, 2020.\n[62] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random eras-\ning data augmentation. In AAAI, 2020.\n22", "mimetype": "text/plain", "start_char_idx": 986, "end_char_idx": 1615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"b144d770-ed66-439d-b796-339942c740a9": {"doc_hash": "df13153167bfa09cae6efeedd1523ccee99c5ae6e1b10771c194f1922eea3f0f", "ref_doc_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0"}, "883ba310-18f0-4fcb-ba31-17848a93338d": {"doc_hash": "e3a36628186d7bd0f8399ae1d38439c8a8d852a81ad1c7ea90597113b407faf0", "ref_doc_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0"}, "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66": {"doc_hash": "c2933915ce726f7894471f4af04395e49e349abe6030c8b4b496ded454450b59", "ref_doc_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0"}, "90cd2df7-b681-4b99-9a45-c4bbb01d4ade": {"doc_hash": "fb240eb810226a2cf35809ed42a4e2f4a5ba5e87f8dda5ebc19243e8aebc0029", "ref_doc_id": "8cf15bf6-16e4-42bc-a154-8fda27dfabd0"}, "563d970a-c800-4cee-8285-490e75ba2b8c": {"doc_hash": "45e044e0cf759102cb3e8b58bf7db120a079f3460d1e165694f065de8b61994e", "ref_doc_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c"}, "3f418125-d917-46db-9f20-6335ed7f06cb": {"doc_hash": "cb19b1dd7e15ae217fae393166f486501a7c5ef96279d5ffa565e1778d9e4707", "ref_doc_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c"}, "bf043cec-f9e4-4b0a-9293-9852b27bb969": {"doc_hash": "2c04e46613b5f4e832b7b68d7c6fbe50adc4c1201717736ad4ae73a8f4c317ff", "ref_doc_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c"}, "46620723-f0be-43c2-86b4-f9b00894b230": {"doc_hash": "f57364fdd673f6eb6435d20440cbc81cdc577d89c5c1399699b92e0b55ec006d", "ref_doc_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c"}, "2a103c75-ea57-4da0-a080-5de28ef59e30": {"doc_hash": "8996a5ec9691ff80e2ebef18010fa59dd6c763faad351a75a0025b9b3de18a48", "ref_doc_id": "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c"}, "50f0260c-d4e9-44ea-8ff3-275f316b76c1": {"doc_hash": "20fd0e71bc5cdf8bcb5511ea6161f4dfba1b2c363347124d6cdc91f644cdc15f", "ref_doc_id": "eeeb9665-8572-464a-9fff-49286047c93b"}, "640874fe-676f-47b9-a525-d40b98e5972e": {"doc_hash": "795d286d61dadb1681005a22a8e3daf802c3ee064dadbbf7e5f48e4bcce5fb11", "ref_doc_id": "eeeb9665-8572-464a-9fff-49286047c93b"}, "bdf4c6fa-3009-4557-b35f-e788dc2fda5c": {"doc_hash": "65e5dc8602f5bdfae92112e233e43d7468e4a5f6c7e93c4a1a514bece2873908", "ref_doc_id": "eeeb9665-8572-464a-9fff-49286047c93b"}, "5968fd51-f382-415e-bbb9-7e89100f6d69": {"doc_hash": "dcd2f1ad02c511152f0a5b996c13323ed78ab24757b40803fa60b6ff2340dfb2", "ref_doc_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e"}, "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd": {"doc_hash": "bf41ba3be0364308b94b85cf6ea588865b1a518a15fac296d64a4f8079f7f31f", "ref_doc_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e"}, "2ec7195e-73f2-4b0f-babb-6b01d8b7f778": {"doc_hash": "a5d21bddd5455b2e253f11b260c97ad9cd57849b8eeabddfb0782cc35fa6f85d", "ref_doc_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e"}, "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1": {"doc_hash": "edff1ea60978b2b22329bba3b0f23d3ed9d9c017e5aabb3b72cfb225c7487ffd", "ref_doc_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e"}, "ec59e1cd-2b6e-4d97-986c-660e3fdd1b21": {"doc_hash": "a49d8ac9bbdb890b2b69bc58737a88353a8b6f7ec1f437a9860a636a888eadc1", "ref_doc_id": "d9081159-e2a2-43a8-8d74-bc98a1c01c8e"}, "7f20c27c-c94e-4d83-8c72-fc616794e9cf": {"doc_hash": "a47117c76337c3c22e026bd893149049538c4873ed7bc135151b4d8b6da05cd0", "ref_doc_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9"}, "39608719-deac-44a6-b356-183e421e9b0b": {"doc_hash": "1e5d15ece21fc5de149495095f9463dfc10cecb799df80c6dd1158c4ae4ee0a9", "ref_doc_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9"}, "aa35cd7e-af44-4ddd-94ea-99718de1030a": {"doc_hash": "acc115e8a262e759b834d00aa4e9b370617335d941028e5c2edfefc3b4b948f2", "ref_doc_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9"}, "17637a6d-05f0-4e0d-ab34-71d637a33a80": {"doc_hash": "85a677b4c3c5b704688e46a2be625d89e4fb41ad11ba6abb1a1ae51a0193c05b", "ref_doc_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9"}, "b8eba80f-75db-4fc6-877e-3683541d81c9": {"doc_hash": "2c61f2ebe4b89415a9b3234f8c0f4e60aff5c124ae4c277465e64d10c3818ba5", "ref_doc_id": "a24fef4f-4cfc-4990-ba22-4d91a88365b9"}, "bdfa5eb3-e20b-4f60-bc25-a1a696e33700": {"doc_hash": "d2ef704ee55a01c9ddebe9b2b0348e36c16d84b603ee35a4d7b7a0d49b6f3f65", "ref_doc_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1"}, "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f": {"doc_hash": "e4a00f855d5b96ea94d492add2c5c5bece36c8d22027f480d9b7b537531f793e", "ref_doc_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1"}, "acf28545-6e3d-481f-9a02-b526848e8e86": {"doc_hash": "0fc470ae0995977a09fdd03ac4738df7c0194838f65dda25baa82ed7eedfd09b", "ref_doc_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1"}, "62a82209-0918-4048-b6a1-e01a25bf2b9e": {"doc_hash": "9c7d68e22a437a16460ce9463ecb025094ab593c43280dfcd0d23eba757c637d", "ref_doc_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1"}, "1123bc84-d94e-4869-9b2b-f784db425726": {"doc_hash": "e2eefd6a3da9986325c3f6eb4b2af5984620e365c6a83cc26b5d050f582efce3", "ref_doc_id": "33a643a7-0fc6-4c46-9454-852dec0fb9a1"}, "1d99ce8e-a37c-472d-bf41-5d546e0f16ee": {"doc_hash": "fb6a67086c366393355a9c930f72db5a1a629f8ffcb42151741a008cccb83693", "ref_doc_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47"}, "b23cd4da-dbaa-48a0-9916-63163abe75df": {"doc_hash": "52f19a861fb0f08ff490f4aa484bae9b28b7500bbd049aea2fb39697c0e593d9", "ref_doc_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47"}, "ee7bda4b-062b-4789-80c6-ec392050980d": {"doc_hash": "07036920a92b33c6d86ccbc596a62cf00341476ab06ca159f9a615bfa542d304", "ref_doc_id": "9e56f4e8-b59d-4e45-9b57-442a21f73e47"}, "f4c79ce4-b09c-4e94-899f-95d7e148a2e1": {"doc_hash": "22e7c630aac754e58aa28590694a4ccd5798448e8216eeae9ed92d0266c4e728", "ref_doc_id": "f8d01ed6-342b-427b-a352-ec7ff615d512"}, "42c3a271-0203-4a60-8301-c4d87ff68d46": {"doc_hash": "5156b8e0268eceec4106559a6bcd0b3ab577ea3f4c3dc6ad94b7905ac229b048", "ref_doc_id": "f8d01ed6-342b-427b-a352-ec7ff615d512"}, "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed": {"doc_hash": "6891ff47459d8ac8a2f4c13269830fc6564d623bc40a8ebd92a89c8a5bf1cfb6", "ref_doc_id": "f8d01ed6-342b-427b-a352-ec7ff615d512"}, "9b6a9438-8faf-4fac-aa12-af91063209fa": {"doc_hash": "43b5d82f388b78102c813d6b58f13eb0d9f8af282dd5591785903bf05ae13545", "ref_doc_id": "f8d01ed6-342b-427b-a352-ec7ff615d512"}, "b7762844-4e14-4c58-9088-788b80895a9b": {"doc_hash": "2abac4047f0e87cc8e45b40230b711e5c302583cd25718c0c6a2f2d0248daef2", "ref_doc_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df"}, "68969e8e-6781-420b-a298-b93aa173c0d8": {"doc_hash": "f97727c53b8e6649062db0b100010a22aad6650ee5e731e369e9302b717db286", "ref_doc_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df"}, "4950bbbb-4301-46e9-8575-5c98ac072755": {"doc_hash": "4ee7a8ca06c7a1a64ae11afb9b31e3850b68faf7d34fbb12926dac1395451463", "ref_doc_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df"}, "6d0f2cc9-191e-4aaa-a1b7-9642378d47b0": {"doc_hash": "3ff8596df0d6028882e37f1fe203260e3a617f58527ff4eb4d181b0704fbea1d", "ref_doc_id": "4eb2ac60-246c-4c71-b26c-92f3bda6f5df"}, "226c4cea-b963-4017-be6c-2e7735506f37": {"doc_hash": "b5eb5cee3f0049a6c8bd99ef7cc52678dad54fb597f95c8ea12fc08fa7b923e7", "ref_doc_id": "da764545-8a00-4201-b4f9-35dbc73b4625"}, "c8260ab2-ae39-42ca-a090-6010130e29a3": {"doc_hash": "2d63bd59b0b22235370880d97d9d3cf7ebd88099064c3ea8f6b448cc1462f27a", "ref_doc_id": "da764545-8a00-4201-b4f9-35dbc73b4625"}, "bb93f585-0c62-458c-972c-05a15b191fbb": {"doc_hash": "8230afb67254516b427f038c86d8f0072a641d2e63352c19168cbc5f61d6543e", "ref_doc_id": "da764545-8a00-4201-b4f9-35dbc73b4625"}, "fd242c0c-cebc-413d-8413-0aa0bdf0fd4c": {"doc_hash": "75d559a41accf66bb704c3b7ff39cc3818e409f0dae1ec25c73b436ff4606824", "ref_doc_id": "da764545-8a00-4201-b4f9-35dbc73b4625"}, "70beb800-2972-4a76-8389-d09dace825d2": {"doc_hash": "33e23c856868f15e1370db7738406addd4e333a8f305c64aa0ae78e2cf8f295c", "ref_doc_id": "52c69155-3db6-4504-b556-e7fde92a6d72"}, "3a6cfc93-1d0c-4914-862c-09d18094083b": {"doc_hash": "420e9cacaf5d68e96f716987e56e9f7faed0c8875390ff40c03da086145b98c9", "ref_doc_id": "52c69155-3db6-4504-b556-e7fde92a6d72"}, "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9": {"doc_hash": "228636087a0b830cc981c1ac1d47f35df4cda1b16f9c3284680efbb544626708", "ref_doc_id": "52c69155-3db6-4504-b556-e7fde92a6d72"}, "5b2725fb-0a74-412c-9959-823022bea0e0": {"doc_hash": "a98466043c8becffcf8c7c2ce6b89605eb9dbfefd6c7a6982e423ee7f633f262", "ref_doc_id": "52c69155-3db6-4504-b556-e7fde92a6d72"}, "821224bf-2c56-44e7-b0c7-042a2d75eb03": {"doc_hash": "a5b7d6ae3d8c0693070c18a472dffdfaf28774f4f12cf86da048bec82351e311", "ref_doc_id": "52c69155-3db6-4504-b556-e7fde92a6d72"}, "a4b79a84-0951-4e38-bcc6-18f662ab3a1f": {"doc_hash": "a3c0e57f86032bc89a3b16bfcfbd3d300815b5a5ce427d02f3a6189c2c633377", "ref_doc_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2"}, "df957665-53aa-4210-ba27-eb61f5709a8e": {"doc_hash": "66d619fa976620493f13903667e7f68bb9299af5712d49041223764b533e923a", "ref_doc_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2"}, "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9": {"doc_hash": "3ed1776b5215751d457719d9c8675f03fb18d8af56f3e1fd6f918e53660e7ff7", "ref_doc_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2"}, "526f086a-62ef-46a9-8769-d4815ee67a1c": {"doc_hash": "58be2caa2f8c2eb3476493871015dc5241d7432d69cda0f64e9be697c5ea1ba4", "ref_doc_id": "075a0632-ff4b-4e55-aec2-8f71da87ecf2"}, "1ff55443-bee6-42ab-aff4-584f69287f0b": {"doc_hash": "5f30eddee76a92b044d215c4d84ee0f3789c993781c3e8beca0b82139996428c", "ref_doc_id": "979a756d-2dac-425d-a204-36f18bae9b77"}, "2105de9b-9a8e-4186-8d5a-932beb3357fe": {"doc_hash": "d02a39527cd5b541a044f0d4133510bea076aad5599648718e17e363b7ca887d", "ref_doc_id": "979a756d-2dac-425d-a204-36f18bae9b77"}, "79374460-d5be-45d3-a4be-49c1d7f87947": {"doc_hash": "4d3549adb3362095f99a20961dad2675717a4468fd653c1c6170b28eef20aac1", "ref_doc_id": "979a756d-2dac-425d-a204-36f18bae9b77"}, "b48293f4-10e3-4010-8f15-423abe7d7c52": {"doc_hash": "754ff0ddce4f66f1c56760e0b38c8b4f79e9c35ad52f9d59dedf5ecb17b77ab0", "ref_doc_id": "979a756d-2dac-425d-a204-36f18bae9b77"}, "a28eceeb-84f8-4caa-baeb-e0fbd0e31426": {"doc_hash": "6bcbc0525a6445fdc850ec9053fda2b501505df7d5efd4adbff38a1d599082ad", "ref_doc_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e"}, "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d": {"doc_hash": "c95d5dcff6ff2ad2ab24715b5d862e55d1ff3d34a2198a95a58226e2b0f78fe3", "ref_doc_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e"}, "ce655fd1-14cd-4e11-95d3-dbd468f75e6f": {"doc_hash": "cf98eed6041f6a217f7f0fb548b5f6729612fa2e68edd51a6673954176b8c276", "ref_doc_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e"}, "8c2e9772-20ef-4633-a491-d408e31432a7": {"doc_hash": "1faf0823b713e58af9acbd49f6e0edf0933e8e82ec44be1bd2cdb2948539f9ce", "ref_doc_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e"}, "2fb9b60d-0823-4ba1-93a1-37eb87c5b219": {"doc_hash": "7cb6437df7ce5f9e6354bd91e217d784836df558bbfa00ba817475fb0d5cf750", "ref_doc_id": "e4974921-c777-4851-84ed-8b31fc1ffe9e"}, "1718dcb1-58f8-47c9-8738-895516d5a876": {"doc_hash": "ccdb7d0503ea22ac94a7b27ff565759e6ed096d5f71fa6c289fc04467309db6a", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "91351451-a9c2-42fc-bfb3-3113e0a95206": {"doc_hash": "cba68352414d0f9bbdc1a4523d3afe96f00cace531b073517251445615b0b41f", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55": {"doc_hash": "8072650ee15301e48c16fef1f4f6ad3488993588de5f975029489b4029dd36ab", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "992a0bab-e1f5-4230-a260-245648f5eaa2": {"doc_hash": "785d4c95d26dfd06c31c44646f897822df5591fda5992ac101865443f9d36b90", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "da39d666-015b-46a9-8c65-c34f3c9974e3": {"doc_hash": "658db5d2348d8e0bcc8740600cdf05c19892b130eae42f3a013327d2bdb0c9ce", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "b5813ad5-dff4-4b63-b678-ec76a140bc69": {"doc_hash": "d5e418944de77636240303aa5f87d8683230645d2dfb04a9231313316b2af903", "ref_doc_id": "d267b4a6-a619-43f6-9f4e-62443a87df40"}, "32d6da10-c900-4077-9625-b693c141f1c0": {"doc_hash": "af21465513857c683100799238f59366b69b5fa9ef41a22f93d7483a2d12c7e0", "ref_doc_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997"}, "645373d0-5558-48da-bf73-fcafbe968c49": {"doc_hash": "675b96c3a076607c39d5bc55ed6e73a58cc0319360a4f65ef876f3a34c504ef6", "ref_doc_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997"}, "16f7b813-12f9-4d37-9dd5-09130ad56d7a": {"doc_hash": "9c74bbf4740497219ed743b05807f236041af3194bef2c5357d8ee340c0957f9", "ref_doc_id": "a94769bc-1887-45fc-8edf-7aa7eb4cb997"}, "b6700605-08f4-4593-bc3a-1f329edff848": {"doc_hash": "9f6d005df39d10a6b53d11fbc83baecbfee7a3465356db36f9fafc132bdf699f", "ref_doc_id": "7ea8c924-e836-47ad-819f-823f299b261a"}, "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f": {"doc_hash": "b9569ff34196dc0ae5e4006d4176be7387173e605461be9c9626c4fd10dfa471", "ref_doc_id": "7ea8c924-e836-47ad-819f-823f299b261a"}, "ad6e0876-ee47-4e7b-b868-4da8672be7bc": {"doc_hash": "2b5a29a2bcfb4b963e1c3653e41e2ebc75b065df982223d66f283fb02bb6c98d", "ref_doc_id": "7ea8c924-e836-47ad-819f-823f299b261a"}, "33deac83-22b5-4747-9255-c26f1c3883d4": {"doc_hash": "4c2821b5ba93375a778ea80ecd4c17118b0899152bfded0ef45bb20f7c805b50", "ref_doc_id": "f052b318-2007-4f6f-a256-64edb03f91af"}, "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad": {"doc_hash": "d540bf997a4344529152c6455edf8c7b673552564744e25da9269a2ba85bd211", "ref_doc_id": "f052b318-2007-4f6f-a256-64edb03f91af"}, "84af7591-983f-44dc-a9cd-f773421acc3a": {"doc_hash": "524e7600c29d3e077a2a70ce39c449831a393b701377b85901a05d4d64724c7f", "ref_doc_id": "f052b318-2007-4f6f-a256-64edb03f91af"}, "b7d4a743-80c4-477e-a31b-3a3e8cf51375": {"doc_hash": "2f65fa427cda956922d6f7af1214c75dcc13dd411e001cfe2c12bffd702e51a0", "ref_doc_id": "add561e2-28e8-4222-b1c7-fdcd671a478d"}, "6fab223a-44f3-4dee-91ba-0fcbe2f1c064": {"doc_hash": "db22f3f9fb5be5d8706fe8d535b938bbbde348f66f6872734ee9645e73d7b268", "ref_doc_id": "add561e2-28e8-4222-b1c7-fdcd671a478d"}, "5d76ef4c-ffb6-4e12-9769-f662afccc150": {"doc_hash": "18c6e49f928428e16c6e0a09304c5379d06fb565a71f58087625aaa941923e7e", "ref_doc_id": "add561e2-28e8-4222-b1c7-fdcd671a478d"}, "249c1cb5-ce1a-47f5-9b25-ae9d70db3b8e": {"doc_hash": "7c4763e84ae4c2921999d53ca7ce51a28feb7c918a03bd1284f478b6712a258c", "ref_doc_id": "49062df5-b35f-400a-8a24-2525db09c4b6"}, "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95": {"doc_hash": "6745a6700fa1ef919f99d5b30c98a33868bd6574eff3bf2b4cf46cd80228fb5d", "ref_doc_id": "49062df5-b35f-400a-8a24-2525db09c4b6"}, "f93f969f-a5d9-4b14-9240-844f9273cc90": {"doc_hash": "feea34f5800a052488e93fdd0a3ea01b107d429a17588b73ac06b31dd6bd08b0", "ref_doc_id": "49062df5-b35f-400a-8a24-2525db09c4b6"}, "0d1c3c8d-db94-4efb-96e3-b9daa708e381": {"doc_hash": "dab1bc29085acaaabd3842aa6fa8c402513bb82f262630084d93040d18105ec2", "ref_doc_id": "8939eb83-bf35-46dd-aecc-7f2cd19055a7"}, "8c55e45f-c308-4e3e-a815-8dfdefd545e3": {"doc_hash": "7149787680da5bfd4064f7de630d9590c757f2371eee35d9ff6f28088b8c0b61", "ref_doc_id": "8939eb83-bf35-46dd-aecc-7f2cd19055a7"}, "c7b7ee55-485f-4153-8c07-d087c3dd8ae6": {"doc_hash": "4e8446b59dad3de5a9d4cb9acb85c7fcb5c0067d8b76bd874c131a075e1b1a13", "ref_doc_id": "6ef7c7ec-41fb-4b82-a752-27d76800397a"}, "0f0725b7-fe5b-4ef7-9d7f-3af4d2698922": {"doc_hash": "6db6199523bd180788f44f924c89600d07f81278a0117cac4f5f93d39c200860", "ref_doc_id": "6ef7c7ec-41fb-4b82-a752-27d76800397a"}, "0e96580f-5280-490c-b25d-a7b8a0ad729c": {"doc_hash": "ca4ce09c6c0a3c994407ed72df65e5111332213ddb9b8b1486d193ddca4a6286", "ref_doc_id": "0a6b0105-950e-4b44-aaf5-a0b68979032f"}, "c3c500d8-77b7-464e-a519-63ce7deeea99": {"doc_hash": "72e39850ed9e0897d19a97a911e883a4db48654d0dde1a22256f7dcda9fd1fb1", "ref_doc_id": "0a6b0105-950e-4b44-aaf5-a0b68979032f"}, "f748a6bd-a7e1-430b-8e44-8d0cd209d4ff": {"doc_hash": "242d36d38456b3d085fcfd758e502cfee43c2af14e165c3199bd3f9e96d88853", "ref_doc_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac"}, "fc955a05-8a63-4ae5-acde-f5d051e095e8": {"doc_hash": "f2679c9ca000057553181b582e5bba163e9aede7c24291e95ce199c3e6951b20", "ref_doc_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac"}, "19cf97cb-3a42-457c-a236-fc566a281e77": {"doc_hash": "f02101de992a5aa5794c61ecaf8742566662f1e493ae107186fad562610f1dfa", "ref_doc_id": "086534ac-7fe1-4abd-bec4-e7ef382f05ac"}, "83e7eb31-e0e6-47f7-97d8-329e33338fb2": {"doc_hash": "31d98c706d01620bbe63bb23d83a2fd9676b3e7e0f0b4fef2f5796aac3c92681", "ref_doc_id": "9ac5ff26-e88b-4026-bb66-af26e868e593"}, "b2ded5eb-b0ed-4570-9f2b-3758b13759d0": {"doc_hash": "5a382d8243972b4085d60b8c25f728bc0d411b3a3460684089ce6b981a26a8ef", "ref_doc_id": "d458d2e4-ec19-4d8e-b9c7-bed557725715"}, "8a4f80a5-a997-49dc-a560-971e91331c9f": {"doc_hash": "d99383725390c7e47caefefa421294a2dcfb423ffb2afb7555424176ff595a75", "ref_doc_id": "d458d2e4-ec19-4d8e-b9c7-bed557725715"}, "b432482c-efaa-4033-b476-4d479d475f37": {"doc_hash": "39a99196a74a98be674590a06de5516eaf02e93a1e821f3fa065f48bcb536824", "ref_doc_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f"}, "38a3ec1e-955f-4fd1-8c84-43542d8587c5": {"doc_hash": "295fef17643f47d19ff891a8315cc66e472a81824f0dfe1a47e41855583649b6", "ref_doc_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f"}, "4593f3cd-f137-4e14-be49-58babd346067": {"doc_hash": "917eb304a7a8316170c962771bc8c589938316a640fdafd3e34bd91e16d3c43a", "ref_doc_id": "8bcf4367-5daa-402b-a021-a2d62db1e77f"}, "2f7d4712-1fb1-4b9d-91b9-cd9c2e1d50b3": {"doc_hash": "b0135dea84a29f2dde67f0b049eb647e2ff6fe30c772a91749e5b16f77ced284", "ref_doc_id": "340da147-cbec-44de-8d6a-408061fab7ad"}, "5164f84f-80c3-4388-a6ee-f0519fad1a12": {"doc_hash": "f321cad0e3a88f33f19cdb194733551201dfe395ce74b8f0555a4699d97b84b2", "ref_doc_id": "340da147-cbec-44de-8d6a-408061fab7ad"}, "19e54b55-db0e-46b6-b903-546fe9fd4da7": {"doc_hash": "a4bcc62ef6faed5b88012d0d74655d25842820d7142691b78526c8b449c2c4cb", "ref_doc_id": "340da147-cbec-44de-8d6a-408061fab7ad"}, "822f888f-2a76-4aea-a5fd-8f8b27a7291d": {"doc_hash": "92d3e6215b517c8d9798e14cc4ebdf1ab2c6fd3eca847c692e6a8cee94e4e652", "ref_doc_id": "74a2e74e-aa84-428a-9941-d78449b36773"}, "dced52d2-77e2-46e9-a450-6244526fd64e": {"doc_hash": "2d8d5146094c96a1667dec6fd3381d2bf82361269259204e764170fb92aecedf", "ref_doc_id": "74a2e74e-aa84-428a-9941-d78449b36773"}, "8e0e2811-3e94-41e1-8fde-9621c5ccf686": {"doc_hash": "0294ef179b120dd27c19e9ff910940540cdcf4f782347188ab96f9652ce724cd", "ref_doc_id": "74a2e74e-aa84-428a-9941-d78449b36773"}, "c6622e9a-e276-46fe-b628-f23144968c34": {"doc_hash": "ba8f9caacc28ebf6fba202003952b5b21620701abae9c5cb6727ee2cd990bfb9", "ref_doc_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31"}, "377a72bb-a912-4dcf-af45-63f847084974": {"doc_hash": "702a09e64253432649ec5f5a17f26f3292733501376816f4ecc0623ee17836f6", "ref_doc_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31"}, "de4d97c3-46e1-4c23-9d05-3a2ec58c4835": {"doc_hash": "1c33b2162b0663cea88feac0fc2daf4658d1163926a358ed90fea3e30286cef1", "ref_doc_id": "a7b3b789-1ebb-4638-8289-dcf12f564a31"}, "fa58536d-cbc8-4ed0-b34d-00fec279b4b3": {"doc_hash": "c1a21d22ee8e6688e1a32a8768ffa50925e112326019dedd98f90e0d20f31dbb", "ref_doc_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb"}, "d422cf13-cb86-4873-be7f-abdf32b0d248": {"doc_hash": "580be3b6ea91cabef97e5ab3ed80b4c7ba75bd1f17055f64cca738128f97b8a0", "ref_doc_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb"}, "f7f43918-6198-4332-8e11-6505ba0c01ff": {"doc_hash": "3b96d8135b607dea620d391a52aa9d35dac7a98653273b99e5f640f2601b54b3", "ref_doc_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb"}, "166fd453-7e7a-4e19-b82a-d104916c8632": {"doc_hash": "dddf147522e159747c552c309a6708d9f78d029a77c7a3a181526e28c9b53365", "ref_doc_id": "276b1538-49ed-41a0-9fc5-063d958eb2bb"}, "4f084646-c2dc-4eec-bec3-9376ecdb8965": {"doc_hash": "8a4dde3b4d7d58dc5b7dab886d568e123079905fc091dff8ec4581d68f7df19d", "ref_doc_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8"}, "f08aefde-f665-47fc-b1bb-3dc532fcb72e": {"doc_hash": "080e7652fde35f6ae45b0f282fdd6cdf062114c69529efbeaa3656b58cd05596", "ref_doc_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8"}, "0ed194c9-0211-4aaa-97cf-6c86afdfec79": {"doc_hash": "fa5bcff01dfdee91d058a9062cd099d02a10d47fb4e17f217956a67809fd30e8", "ref_doc_id": "d720be88-3f77-4bed-9860-17b2bf8a85b8"}, "54cf1130-e1bc-46db-85fc-305e4431a37d": {"doc_hash": "69d810b10e92edc3c590bf24b78cc9f62c059ec585a75e2dcde4945467c59733", "ref_doc_id": "8520c4e3-0df9-429a-b4aa-202e06661a15"}, "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25": {"doc_hash": "695f43cb44c21ca8500f1697881717d3df5e09cd7688ad9b59e15f57cef42ea4", "ref_doc_id": "8520c4e3-0df9-429a-b4aa-202e06661a15"}, "c6f30924-ab67-4032-af15-9018fcbab6e7": {"doc_hash": "15f148c2e1bd71e29c16b831547f3c332a4f58385598e25c11e0c96b6359427c", "ref_doc_id": "8520c4e3-0df9-429a-b4aa-202e06661a15"}, "92880e25-c799-44c2-8141-4decf2bb4ec1": {"doc_hash": "30b8e4d353289182144f800238df58f0cb9a43b46ac3c05cf133154d447a854a", "ref_doc_id": "8520c4e3-0df9-429a-b4aa-202e06661a15"}, "8afe343c-7a2d-4db7-a179-886423a7dfb4": {"doc_hash": "dbda66336bafd662c23762c6bd05d88fbd8422b8161b7fa4c3bf4dbfa6bfbea9", "ref_doc_id": "5df396e7-e704-4f2c-87cd-26f26836bc55"}, "3db08707-3250-4d28-ba7d-ccb5b5743e3b": {"doc_hash": "3725a1a4e2902d5a7e7d65c96fbfae151922be423c3aeb37cde2e2a8ba90fd8f", "ref_doc_id": "5df396e7-e704-4f2c-87cd-26f26836bc55"}, "691090d0-5c1e-4f05-80b8-30d5f150ba33": {"doc_hash": "5f15dd8b2b63dc4ee7dcfd2636a0c6ad17995a988132b429361b35b78066fa1f", "ref_doc_id": "5df396e7-e704-4f2c-87cd-26f26836bc55"}, "1161c14e-cfeb-49a9-b8c7-791cd978bac1": {"doc_hash": "ec4dcdb94a1169b93a9d5643882861fcc3726bb1b0c1b8ddc735b37d858b259c", "ref_doc_id": "d562e8c9-4a52-44d4-a202-22643c69e2c8"}, "87054fef-106c-4066-974c-7772c83383e6": {"doc_hash": "f3747ff26d17719fcd399435afde219a567498c1205b76672f8d4c43b3099fdb", "ref_doc_id": "a35f71d0-6f82-47da-b623-7dc45d888860"}, "ee4efd4c-4b1f-43cd-b853-9ba0b0b08ee4": {"doc_hash": "e5be71eace598c1e4b9c2095e5be233c22710b688be6002f7d7d448295575e31", "ref_doc_id": "86552836-947b-40c4-855b-5105b35d0ee5"}, "03d47bd3-bc2d-4290-988f-65337cc71d9b": {"doc_hash": "4f37342cb8f202877c68899f508329537cea945bbc3c6682c839656f899e8b2d", "ref_doc_id": "560c2293-8732-4956-98bf-c39c108e1d95"}, "79ef22a0-af76-4a0c-99c4-4b462168df45": {"doc_hash": "951fe69d639b7aa9d7a0acec56d050b830cc5f8d31a767a38c87e79114e09fb4", "ref_doc_id": "560c2293-8732-4956-98bf-c39c108e1d95"}, "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf": {"doc_hash": "301f491b315eccd66e625625ecc44c0aeb806f257e5bbc8744117c5e1db554f0", "ref_doc_id": "560c2293-8732-4956-98bf-c39c108e1d95"}, "14982d34-ffc2-422b-8652-ad1d9ae7fec8": {"doc_hash": "1958ced91d7a5b59fe3b4f35832432023ed2321d10376657308e61320d6d1df4", "ref_doc_id": "560c2293-8732-4956-98bf-c39c108e1d95"}, "488dad6f-7414-45a9-b3fc-a8e9c9f796dd": {"doc_hash": "dc67e0db9cd82bbc8a4d24177619fce9b674c959f0a96d0a46fc1039b42e3c53", "ref_doc_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e"}, "72f12176-914c-4620-861c-678d50d5bd8b": {"doc_hash": "a0595907545508d9cb23c06dc864985be9a11deff274c267dc6c00f2b8994826", "ref_doc_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e"}, "7944edc1-9968-4708-9e7f-1ff39f28f721": {"doc_hash": "5aafe1c1f8058c7e6a6646fe0b39dc6901ac88b1154ec4764401a4186b8d6ab9", "ref_doc_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e"}, "1d1a925d-c4f5-41cb-8bbd-aacd58b25818": {"doc_hash": "753d703ee590d6ed96c946185fd36fce6591d1a55cdef722b9d8dee3f867d61d", "ref_doc_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e"}, "779702f8-33b3-4388-86db-e6877a5317b1": {"doc_hash": "bf64ff01c7daeaba5f3f9de15bf0655d4cc5440d02554093b83ae5095a8807fa", "ref_doc_id": "8593f4f6-e202-433e-9fa7-4e12dd416f3e"}, "03e7f862-198e-4a96-9756-b97865f55d6c": {"doc_hash": "2c28f17bc07f54faac92016a7a9c946a74d645343eaeb16238d7dc791d78ea68", "ref_doc_id": "197846d7-4db7-44b6-9e92-1cf00a44b087"}, "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4": {"doc_hash": "956766894dbe33650d842b2f44b877ad3e923c298a7da97683545c216f1b883c", "ref_doc_id": "197846d7-4db7-44b6-9e92-1cf00a44b087"}, "3dd01c88-c64f-444a-b5dc-12d186420111": {"doc_hash": "6081bedee2d035ab97326b8165a3fd17e750da8ec1fa847c27b2b2452f7e4157", "ref_doc_id": "197846d7-4db7-44b6-9e92-1cf00a44b087"}, "42c7e353-4d6a-4a6d-a182-3b52b822da15": {"doc_hash": "90e1c7f0a1689550440385b825bf423865177ebdb7ae7a9d5664b075dc373784", "ref_doc_id": "197846d7-4db7-44b6-9e92-1cf00a44b087"}, "c6f1618d-e6a6-4b55-8de9-8b66eff2f39d": {"doc_hash": "2a8db415d1310f9f78aaa55f7f43852293b9c05b7980957021e0d99f393ca0f5", "ref_doc_id": "197846d7-4db7-44b6-9e92-1cf00a44b087"}, "7f15dc0a-8f9a-49be-855c-7bbf52cc3223": {"doc_hash": "53d9b3fd03f8502a49b4eb0504bffcfa024c13ffd9818503399ddc98a0a2fc50", "ref_doc_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e"}, "d2108054-4a74-44fc-b9fd-03f8fa286714": {"doc_hash": "9a8d095d391f732e1b03134c7ac58cff9a7afab5d64d3fe902839cd3dbc5844b", "ref_doc_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e"}, "c984be96-b580-4e49-8207-c0667df3776d": {"doc_hash": "b24b6b0485039d1a1410e50ad334222640f24908d96d6aec25175b55b0dba38d", "ref_doc_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e"}, "dbd307c5-db4f-48f8-99a3-47f6d2f352ce": {"doc_hash": "6cb93418db1047cf631c2fe8142425725f6fcf205513815b0cbb7d6fbfc942a8", "ref_doc_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e"}, "d2bd2716-baf2-452a-8b2e-e532bd37be75": {"doc_hash": "23345a79dfd43a60348333e44d537428ffa3dde86001f11bc4b1c7acea8ee362", "ref_doc_id": "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e"}, "e5345077-fc58-4dac-84ba-1db24974d231": {"doc_hash": "4df503368f497f357481bd8cac1ee76802c63f74c3cc9a68e328c646b0c02faf", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "421e01ee-f83e-4c19-baa5-d6536a975fa4": {"doc_hash": "e303494a00bb9a59c0a50c9e9e3fc1170dd048b340d1c0ccc0183a24aa75fcfe", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "3622b161-5bff-4dbe-9aa1-e654e49c6d68": {"doc_hash": "cffadaa83ff0b4f1687f56b8be855c509680d967442a1508ddfd764906f4cb02", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "5fa7b669-b5e7-4f4f-9824-9950ae479e35": {"doc_hash": "c98a7ab3b58e40401c93e47a3a7169be4c4a412caa7cfacb197176c4d4f8a301", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "ab875183-f1f9-48c8-a7ac-6efc7e4784cd": {"doc_hash": "dd07431c450a07e3c020dcd8385458b8c92b7a2f06a1f6225949d12d96675ac8", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "93a2b0eb-cb7c-4e50-b90b-824f2078aca0": {"doc_hash": "99dc5f68a8c71e33615591587096e8bc2f7b4e612b879e62551e3bc46bcc3bbf", "ref_doc_id": "7801eaca-c0be-4680-aa81-1dbe51de1953"}, "8d1f3c3e-a6e0-4591-b81f-81c4814757d5": {"doc_hash": "4eb147bd11af25e050f3786e7e339749bd9e32f6332ed90b7660ea6858aa0129", "ref_doc_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33"}, "559a9f55-92bc-4642-ba38-e817686679c4": {"doc_hash": "80c1b0eaf76216ce6c7e98410b883ea47ba356a9cc28ee92f256292680ec1f44", "ref_doc_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33"}, "9c3685b0-088c-4656-893a-ff055d16a65a": {"doc_hash": "a24271abc689b6bee6e5f260f905ddd6f706ba6c26e2936fbfe2000065a0c1f4", "ref_doc_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33"}, "468c9229-1745-4f6d-8751-3ebae9f8e381": {"doc_hash": "d4649296f047810d840f1273629e8e53d202dbacde2345ba02ca5974c8326c2f", "ref_doc_id": "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33"}, "3f18fd33-ee73-4341-a086-5b65bb46f184": {"doc_hash": "0231c0d1b7c9a6fc9571569912f7d33ac451f84f3fbaad982814d038714e3c8c", "ref_doc_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57"}, "aefdcf9f-9336-4ce3-8551-80018fcb9a85": {"doc_hash": "0bb1f16d9782a5c429a0e914b773051b9ba40be8f72697cf825585ee2c1edb6d", "ref_doc_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57"}, "144e4d07-b1bc-401c-a262-a0e032f8466d": {"doc_hash": "79d4ee186f4aa07bf980b7790bcdc3e8cdcd1eff4e2de49316cca57e853d7f43", "ref_doc_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57"}, "1204fafc-0e56-4ddd-b01c-38b5ca1552e7": {"doc_hash": "958d50eb213b93dbd607c638c3de6f96f7012353b39c184ce8a6427b199c968b", "ref_doc_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57"}, "faaf2b13-e4ca-40d2-bcfd-2a20b5b8a7c9": {"doc_hash": "dd66c1b8f9b786b03e81728141b9749a0c6ff27f5244e14ef7d090dae1049ea6", "ref_doc_id": "2d8af6a1-95fc-4aad-bfee-6351b71c1e57"}, "289b1b8d-bb5c-4939-b900-f23057836e1b": {"doc_hash": "5429fd5cf9d0b3c23713934813fdf55741fdb81ffbd5c96159b519d13f6d5ef0", "ref_doc_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b"}, "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1": {"doc_hash": "57df12828b9d203cfc3f823649690326cb2b8e3ebd41796d8f1ce6f9b4ca9882", "ref_doc_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b"}, "4cf6a737-620f-4c2b-9f6a-54606f2e766c": {"doc_hash": "da8ca41e66e8bad6dca9d9419b543f7e63ecb21960f26a97f96e9dd06b0c55aa", "ref_doc_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b"}, "8f877bc0-942a-4367-8f6d-1d17c9e5cfb5": {"doc_hash": "b85a225e999d1ff58d6adc88df79e02f1a14ab29064fdffff51d504e2d53dd45", "ref_doc_id": "4004513e-fcf3-4219-a1e5-bca570eeee2b"}, "336fae26-fc57-4f09-9c89-1dd00909e1bc": {"doc_hash": "ff016407e4350fee2ea1cc03ce93cde954e6825b1dd1026e7a4466dbbd4243b6", "ref_doc_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214"}, "2808721c-1d34-40d9-b8bc-9c4d98524c7c": {"doc_hash": "3ecfc0c02ae839313217c2e420d1c257ed1844d0c2545a33e703a97968fc1403", "ref_doc_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214"}, "cc0bddbe-995a-44cf-a692-7544bf625727": {"doc_hash": "6ff3a2b36403fd927d8e4d3caa9ec261ca7a9271523f74ff09657a6d9dc4767e", "ref_doc_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214"}, "94769752-8058-40db-a40a-ce16cc007f73": {"doc_hash": "472a48f79439cdf73d749956faa5dee304900674e9ff5fba0866135864c98740", "ref_doc_id": "960533bc-fe8f-4224-aff1-73ed9c8ff214"}, "ecc2d542-15ad-42b7-a487-900c176a3fa6": {"doc_hash": "1b8a3069c789ec8c8f2add14a5499f1fda0b041600452d06d6c26f0d92894b32", "ref_doc_id": "83c18eab-5164-4adf-aca1-61f6095f0c62"}, "27c3b9d3-fc72-4590-aee2-949877aa7fd6": {"doc_hash": "ae9fe0843291e750e611cc168e839f83ceebb72b8a7ace11b3a5607b6e02f59a", "ref_doc_id": "83c18eab-5164-4adf-aca1-61f6095f0c62"}, "2253a3e4-3550-4eb3-835a-3d6fdde8c56c": {"doc_hash": "533156965f4251d0aa9f7a718d440e7b869b4620275118b520332b78b0666282", "ref_doc_id": "83c18eab-5164-4adf-aca1-61f6095f0c62"}, "35b7f3e9-ec82-4ebc-940f-cf924406685b": {"doc_hash": "f6775823952fd58bdf654bbe4a6b219d67f316c1e8449562abf49f1adae251ab", "ref_doc_id": "83c18eab-5164-4adf-aca1-61f6095f0c62"}, "d4655479-c6ba-4dec-b851-071409dc19b1": {"doc_hash": "16f6baa0730654ee9a6780ad026ff6b3728d15dff9a734e743bf4b44f4957e28", "ref_doc_id": "f6a53155-9852-451a-bb15-24cf4011bc7b"}, "aa59ac24-3d7a-4655-93c8-c8572df02294": {"doc_hash": "da384740e606e88c418bd7d715151fa770acea4f00a613829f1734654b701a76", "ref_doc_id": "f6a53155-9852-451a-bb15-24cf4011bc7b"}, "efd60397-3e10-40e5-aa1f-8742400f0663": {"doc_hash": "9375d6bb5a805e2471d279b823ea61ae10a29eeb08aacd2c36da5c65a6940f9a", "ref_doc_id": "f6a53155-9852-451a-bb15-24cf4011bc7b"}, "4519b263-4996-4fc9-ad1d-f08c81f36040": {"doc_hash": "7fb7d2cfed8dd5a8d3fee1ce4bede9583398cf27968280ddc4473f66e630bfd3", "ref_doc_id": "f6a53155-9852-451a-bb15-24cf4011bc7b"}, "2d6e3e63-f4a8-43bc-9ede-509ba6c18c5b": {"doc_hash": "fca15bd28359e22763121e3009951153450c72fc58765fc53b26505308065fda", "ref_doc_id": "f6a53155-9852-451a-bb15-24cf4011bc7b"}, "16d83b02-71da-4ed6-9dc9-74a9ec2c40cc": {"doc_hash": "94197c4bd199710abe209fa6b77c58f5ee84fcde8380c820347fe25823af3bf6", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "65d018d0-fa5d-4753-84a5-780cb3c538d2": {"doc_hash": "bbd995051c5503451c8e942ee94593d75ec9d982a7b5cd39f46f18104a977c7d", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "f6c6b166-d86e-43ba-b91d-6a9fe8532091": {"doc_hash": "1be7bf7e1eea37347364d189368390772664169d9383ac42f5ea70b832c602b6", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "3a80d112-7b87-4483-9783-b6f9ccb97aa9": {"doc_hash": "ebe3c6b3ee805010365932d39388622de764b9f5e5050afbc4cbcbcfa267f5fe", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "694162d9-2657-4d4d-b452-537cee00910b": {"doc_hash": "a303ae65498e21f0cb484b2643568f19a6737be026643dd1c62a88bde1f9dace", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "28a85989-adde-41dc-a6f6-f56b5cec086b": {"doc_hash": "ee7b9941d690cb5288f954fdfc85a0f16e31a3f07e9d405001f1f2e7daff76d0", "ref_doc_id": "89551c77-5741-478a-9a15-735f30a541fc"}, "7c75251b-8254-4b05-9c45-e0be7f499e3f": {"doc_hash": "21c93d8bedf887f9be16e3bc04739b64cf06e45df0bb5da57d2b09814dff9301", "ref_doc_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9"}, "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c": {"doc_hash": "7fec5c5335d0243a0284ce87b73df4ba17ee5ccb96c8352b3ca0733fbb885344", "ref_doc_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9"}, "46a8e9be-2a77-47fd-8bea-66b1e482ac65": {"doc_hash": "134d6a0acdd5b474b1961ffdb81c40eda1f3a00061a8910d696e6000112854c1", "ref_doc_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9"}, "3159b1cb-85a0-4df1-8173-f631dc6c973b": {"doc_hash": "81711a8de8d9fd038fbd8b622f0742292f7760fc38750518a8eeaa613e4fbce2", "ref_doc_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9"}, "ff40bcdf-f8f0-401c-a40e-b747052e70a4": {"doc_hash": "2e01de7764e97967eb13549a18e43b1333b274eec090de33d8c50e61c98579bd", "ref_doc_id": "9d36ed80-29b9-48b4-a6c3-a60aad1badb9"}, "e8e9092e-4033-4719-ab14-c692f2b058da": {"doc_hash": "f4781ded021095b8de34c232dc116e4fd98c1ded5e7d0f115c657c470a0d4eea", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "48988c3c-e1a3-4f36-88a8-148c7a38b1ee": {"doc_hash": "2596c60b357d3676d216d4c39fec23dc2905e92f0e8e3f85150448c6c4a28e3c", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "2b9422a1-b351-4f1e-a0c1-efbc92b95863": {"doc_hash": "271763fb0c83c3d354564bda13a2aa69bd19515d746a2fee4ecf389cbc44c1ac", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "95c9a21b-cd4a-45be-8bbb-74a1bff406fd": {"doc_hash": "c6c22bd187680d8e81968d9a48a788831983a7476f8ad738eda6a7d3fddf290e", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4": {"doc_hash": "7591e222ff37d4743da958f314b8aef8d2cb3d9a3b8624e7780201d51b16a260", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "ebd80897-b5dc-40c1-ab95-02f98c9c38ba": {"doc_hash": "6cfdfbfc95236afca9cf335b7fc46a60b3c94d413ae343ad03cbb27d5b789ebf", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "c6ccb94b-8de7-428a-9c60-89c0e5750933": {"doc_hash": "c82e9561b831ce793c0f6bf871c3ef9091bbdcbe562f16b203d4d1f2977c2058", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "a8c9d156-629a-4a51-8509-77a6bea84425": {"doc_hash": "eabd919073766ac695da11df976ca59014d211e4437eafd697a047f2b1710da5", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "f802c401-9854-445c-850c-66fe04a100d9": {"doc_hash": "1688139cb9c00ea013d805bb2c035b833299ba379c11cab2d4cd55a3e0476f57", "ref_doc_id": "71ffeb29-b789-4206-a34a-ecfa254d1f68"}, "9f4eb994-a455-4c67-a3f1-5c51af04d89d": {"doc_hash": "53cea49dca0ce08cf6ee14a29ecf1e3d29b5065dc86c575ece2e1d28fae54135", "ref_doc_id": "98bd6505-291a-43f8-849d-54681f25a6e6"}, "9eef1426-56c8-4d4e-9fba-6d5074a0e25c": {"doc_hash": "cf9c2cde5dc64619f8aeb9d69c0989bb833e0680a20f127c948905c41836bdee", "ref_doc_id": "98bd6505-291a-43f8-849d-54681f25a6e6"}, "ef357faf-228a-4caf-8007-a9c13493ca66": {"doc_hash": "735855347642e87040e145e06df63646d5e89b6f4d0689182921262e725eda3c", "ref_doc_id": "98bd6505-291a-43f8-849d-54681f25a6e6"}, "aa6268c5-875a-4fca-8150-1c428685b75c": {"doc_hash": "631269b33500f0045e7a2a6408830832296fddad182cbb944eac9706764de723", "ref_doc_id": "98bd6505-291a-43f8-849d-54681f25a6e6"}, "2e653e99-88c9-4408-b338-299410edd28b": {"doc_hash": "2005bd437c1295f2d5426e3f9af8675f4cb462f092929418a2470463f892ba02", "ref_doc_id": "98bd6505-291a-43f8-849d-54681f25a6e6"}, "1d775d94-255e-433e-abc4-464d8961a836": {"doc_hash": "3406eea01e8f6d07726de9274283a46df78313178f4887dcb107b56f7617e363", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "423b3ae5-ac1f-4768-b51a-631a1c7a3998": {"doc_hash": "733bc8b55bd41b8f9544fd5eeec82931cd409ee7a467eda1a0b12e8303818270", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1": {"doc_hash": "e15868a784f6edec0ad52d9c1042cd36c24570347d9b3270d42142fd8b97016e", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6": {"doc_hash": "a6b5cdf47790f810da9cd48c2669eae780ccc8d345232401380667064c0f7d00", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "f47292db-3a95-41f0-9573-b92ee8f6ab9d": {"doc_hash": "d3a8bcdafca77b4842d5d51b2496a170b111f41278e0c18cb8402c104cbe1c9e", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "7a05b865-dd80-4aa7-9408-a5e39688dea3": {"doc_hash": "79fa570ab3d29fe8cd6b103612885d5b23c0d867208aa1d431e419c6effd5cc1", "ref_doc_id": "f62de16f-a39c-4eab-b47a-47fd88fdea35"}, "e0081f9f-b637-42fa-8290-b70cf952a28d": {"doc_hash": "b4ca4c3e5c79d44aefad8638451271ad4f4521abaa19781ec420954cb9a62859", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "7abea2c3-fd8f-4a10-88f6-879cc97ac304": {"doc_hash": "868190973b5295292e5a64bc3e062530fc92a4e00eb5fdcccc6b038693bc52b8", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "92ccba74-3c14-4b84-bc2c-750d9f3ce26f": {"doc_hash": "42fa8a17121732488c30316e804d470e7a2fbce62027bf7bd4a5802042ac0b2e", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "5e868248-f7e2-4738-a625-77cbee2f64ca": {"doc_hash": "d008f99473dc1ba4cdbddb6f32033c424a9abc45b0a56a7f32e6024fa273659e", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b": {"doc_hash": "78689108ebad1840be7e76b63fb24ce87b3b195332910a50fe3ebb208027f231", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "d40ca56c-518e-43c8-b72a-7ec5916015a2": {"doc_hash": "f528c113c199a7da0dd1b90e7fc636017bc589f684fd47f61ea642c3804e6a19", "ref_doc_id": "f851d517-262c-4867-8fb6-6ccc065013c4"}, "eff43ae2-06e9-446b-9f43-6da5134dda8d": {"doc_hash": "3355b5abf6be5299408a132ad83240f804f95cce93ec1d99e3dd97758b7d817c", "ref_doc_id": "a8e9b03c-a557-4618-9aea-c8b50668052e"}, "cb52c700-f4f3-453d-b2a0-0d15b88e668c": {"doc_hash": "0676d03d688ec4be1b515de2f30f36aaacd896757cf8685b382ab1ac16894fae", "ref_doc_id": "a8e9b03c-a557-4618-9aea-c8b50668052e"}, "005375bc-2857-416e-bc4d-3fb74f38ecfc": {"doc_hash": "e7d86ce657150805ceb5c83491d9542666f985ebf5591f9989a22a78e2684c55", "ref_doc_id": "a8e9b03c-a557-4618-9aea-c8b50668052e"}, "d989f715-6a45-4fbe-b76a-f9db2cae5edc": {"doc_hash": "30ed10cce3c58167cfa152ef58dbddc4aaeab79de77ad8cfb575690899fc7ef9", "ref_doc_id": "a8e9b03c-a557-4618-9aea-c8b50668052e"}, "77ae92bc-aa0f-4d7f-b84b-7e8b857b8850": {"doc_hash": "971935d3834805a0e2c0291aed017f05f0c93cd12da6a5535849459295f6e3e3", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2": {"doc_hash": "7350759c7785e05944f7a4dec1b441ebb3f9a7b1f13be9f2199a819a7b22835d", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "cbc893f8-6257-47c9-814c-742e3dd11bbd": {"doc_hash": "840272b589b25661763d941f36e27c216427ec595dc5ed60e8c7672f4db84821", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "92f81b41-7398-47ab-b21c-9a2ba02950eb": {"doc_hash": "02d3706b7aa18b206bd99b1d8f4853726e447942d9ea72df73268b98e62fb89e", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "600a60da-6553-4936-92c4-9d515c550c99": {"doc_hash": "a3d7d3df5754ec3e4ecfefbe295c12846ed8c2333f8967ff677270f8c7042210", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "546b58de-8f1c-4e2c-997d-db783d9c972d": {"doc_hash": "8fa7f618cccc723496acfb84fa4d67aa511a083817ce62f6d86a4f82623793b5", "ref_doc_id": "997f1d27-2d1c-4520-a22a-3c94805b3f19"}, "5db35dee-7104-4d4d-916f-21047cc962f5": {"doc_hash": "2fff3b5696ca18765c1798e0573f4a20134570f4bc9dd05be75743c54f91a434", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "89915979-a4a9-43cb-b367-1a7e00521701": {"doc_hash": "9a3d3d30ba893aa9e983acb2c1c298c7fecff7251dcadcd6df390fcccaf46497", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0": {"doc_hash": "cd067d550fa4f0ae98e656a6dc5d3450c019549e978b24ffa8ba046e4cba3de1", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "bb33725a-8637-49b0-8f18-d9bd8abce73b": {"doc_hash": "1a64dcf19cd9037d7931f907968fb00ae5ab1e1bd20785f028ef24564f62c6e7", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc": {"doc_hash": "60b3d70749c27a8028f41a2f8273042f4372ecdb3f3abd92c5012621d890bd71", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "0bf48078-f2ea-493c-af47-45e803f76b5b": {"doc_hash": "d1eef5ed3ce2442a53b461daf1faf3fde09360b85b2cd08d440d81ed612282c3", "ref_doc_id": "138d5f78-d06f-49f4-a597-16bf4b6da2ff"}, "9189a21b-108c-4167-bc7b-a172abf40ac2": {"doc_hash": "3da57afd0ae70cafe6ceec8d15f7acdf924208e076012a08718af1793212b610", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "8fd3458f-f365-4f58-b487-e3e73c098fda": {"doc_hash": "23bf52d5918417af378df40586d9e25ab4fe7fe7e52c8646abde65b0e0f7831e", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "546859c1-ef43-4741-beed-59ab66a07fb9": {"doc_hash": "188eee6d9729531ed4bb263ddc0f14a775e10f1381560ca10a2ff7e056992696", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf": {"doc_hash": "bb7c8d2d21625f91ea0b77359e243a38b923039ce92065fc5eb9ee4e75d2c212", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "db5318e8-ab2d-413f-98dc-2ef577a257fa": {"doc_hash": "73cf84962288f6eda9ef53895b2052a5b8f4cb81ba65b46dcdfc1045468ec36f", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "5540dae0-4f31-4352-98a8-6767675b0fcb": {"doc_hash": "11054c6893a080d6a0666e505f125cdd0a82a683d4a00e35a49b57c85481a735", "ref_doc_id": "cd15bd1e-4414-4d65-bf59-5744ddfc059f"}, "ef9f03bc-8255-41cc-a32a-3c0e401d89a2": {"doc_hash": "4b3d072a7d38cb3da62e48f0a9040c0496b38d4d23f7ab77e2e324092d6c32d7", "ref_doc_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0"}, "e058be7d-e2d2-494e-b923-de8e15b59f8b": {"doc_hash": "77d87a2f04197481e76406592183748e5b0a73c825d2db3a0416fd36d6283991", "ref_doc_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0"}, "b7e14b8b-c189-47ea-a7f5-7b54899af439": {"doc_hash": "dfe9c6c272cbf7109129a6293a763761ae009e04f626a0eb503c9e91911794b9", "ref_doc_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0"}, "fe61e543-fef9-4dd6-9de7-75eb4c865356": {"doc_hash": "183e030324780f3a33876700cd1327c48a90c9197e76086cbb5eb6beb41640ef", "ref_doc_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0"}, "d727d551-a1cc-4ed1-8997-0ab9b621c36f": {"doc_hash": "f920c238778f6f61661d05abc158a1145a6a9ff275838e885196692291995409", "ref_doc_id": "e0de7cf6-791c-4d84-9de8-27afc3279cc0"}, "5fe3f32a-fdd9-4270-8d1c-ae924f51db4a": {"doc_hash": "b3257fd5f56199a937e14f4353c0bba94c08e3c72ebcddad731df7d5c67bca35", "ref_doc_id": "8447d82e-654b-4e6c-80ca-5a378f47741f"}, "31e29c65-2c1f-4769-8b50-c4b6b0d46aab": {"doc_hash": "b1e17ca5f0042fa8ccb5d99182fe8f104cf5a50f9b0aefc3ca5d40f4c9deb91e", "ref_doc_id": "8447d82e-654b-4e6c-80ca-5a378f47741f"}, "e68ee0c1-1035-4562-acff-2160207b1516": {"doc_hash": "93308d08dbb63c946d62b70225b377277df4921591a99c0cd158271050abba79", "ref_doc_id": "8447d82e-654b-4e6c-80ca-5a378f47741f"}, "bacc7868-af4b-44eb-ac46-c5af82b6b737": {"doc_hash": "87601fe3f30a3b7a453c0646d344bfc49d404de0fb12c8b71d34b47352041605", "ref_doc_id": "8447d82e-654b-4e6c-80ca-5a378f47741f"}, "0f5319cc-f3be-406b-8958-2da24b49901e": {"doc_hash": "fa73056ed35da8bb6261b3bde46bdf4e70396e0946c46b9da7da3b1a32ed846f", "ref_doc_id": "8447d82e-654b-4e6c-80ca-5a378f47741f"}, "bfde838b-685c-48fc-aa71-c4724cc8100e": {"doc_hash": "14a84e94a64646084c4e5582a5e5d9e5e2c083a8a4aff0cade1e94c993efa9d2", "ref_doc_id": "b0687058-84d4-4857-b828-363bf10890f0"}, "2b4d2cda-c0dc-4534-8c4c-e403eed972f7": {"doc_hash": "f50653d666bc864650935f7998729e94192db3814aa65d87a7f9530e5baea645", "ref_doc_id": "b0687058-84d4-4857-b828-363bf10890f0"}, "9c509ca0-3d15-4e9a-a841-405b12481f27": {"doc_hash": "eb438ae44d5b2a2bf255d082116281d9b305baf29b6ae178c09143cf6a3671f8", "ref_doc_id": "b0687058-84d4-4857-b828-363bf10890f0"}, "aa77525c-9b00-40ac-baa5-75cf1e19c06d": {"doc_hash": "f8999331b3a46b057a3b761f03b9879bad8a6f6aeea9120ba651534ea0767d19", "ref_doc_id": "b0687058-84d4-4857-b828-363bf10890f0"}, "cbfad1ef-ac41-407e-ae49-9784c666b863": {"doc_hash": "48e5c1d9147390b59fba2889ad8591ff461495cae7228ff00012a6b6efcb16f5", "ref_doc_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c"}, "907c69ce-8a05-4dc3-b4f9-26d5e52711a2": {"doc_hash": "14a7d0779e901a724d0b4943424173f8d1748397d9ee18a365a9a67b1872dc21", "ref_doc_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c"}, "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7": {"doc_hash": "48daaf494fa9539caa688ba8de5c64a8e3a5c050aa190e1746d2519a0dd4aa69", "ref_doc_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c"}, "94e8b838-d3f7-425e-8b15-ce1ebae63303": {"doc_hash": "ba11ac47ac8573530836249e24306ffb37b568813ab0a41bc97d51bc88ce6c56", "ref_doc_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c"}, "34462f83-2d9c-4a7f-9e74-3b5034a859e7": {"doc_hash": "aefceeaa5e339c3151f7e9391a13a2499b0bcea77149a5073608369d9d2f9d4d", "ref_doc_id": "55da47e4-2bb3-4edb-87e6-416a6558ae2c"}, "f9c7e71c-e04a-4a37-b9f8-30657480b6cb": {"doc_hash": "4cd90651edd6536040d6a96b128cfd7950fe91834e968747f7ad898351986598", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "4fa862e4-4fc5-46e1-9336-c2984bdc796b": {"doc_hash": "df0e94a79f17dd2e75b2a1507ba8cb901e2d58b771bbe41a6465bcd1600612bd", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "c8e94427-6955-4529-a647-7c09b51e5b45": {"doc_hash": "1535f01dd5ffd92c836b00fda161d24ef68bbf0e09b582fa42f14cc599061663", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "a254f9c4-ad73-4a09-a9c4-78285cfabf49": {"doc_hash": "803e41d5407d7314e18eb565105460f9acfc76a6fc508e34f1fe3cbcad59efd6", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "cce72ca5-1488-4d6f-8938-ad15d28efedd": {"doc_hash": "25330c0cfc4ce375003d292eab4cf6ec09779228a2879a1e6bd7bee266deef58", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "1165a9b2-3fd8-476f-a275-1301add68a23": {"doc_hash": "52b50cc1591c77907cfc0af692e1ff9c98c93921e734b9d22235e96544d4fd0c", "ref_doc_id": "f417e3b0-2b90-455b-9282-3c9889c9ff03"}, "f0f2b242-14ac-45f6-9834-6dfb33cd516c": {"doc_hash": "d4b60c5717f1fba2fad5471886e34cf3e4d01723bc122bc3b4d6e1504931844e", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "9b6a7402-536e-4512-b48a-f78e2f3833ba": {"doc_hash": "08b9bb80ae0af20925cc18fdd80525eedc2174c53f72931a6ed9e2a37582e4a7", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "1d211d9f-40b8-4995-b008-79ce8889a88f": {"doc_hash": "df1445a034c15bc08b7f99224ec6493b0564129161051d3cbdf24e3b5de6bbed", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "67b07245-49bb-44ab-984c-26e2cd9f2b11": {"doc_hash": "888f85065ed609c8637d1276d4f4b5a2382f5f9cda54b2fe9595799e31afb42c", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "28723e00-7566-4c40-837b-15ff158ca753": {"doc_hash": "e07ce51ed599c7fde5bfd4d92b38f9990485ea8309a6ce204256fac1aac625a8", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "23d4bff9-886a-477b-84ca-c08f40d4ea38": {"doc_hash": "acf6b566043819baa527a293ce03d62239a256160a829c0d97ee840456c082f4", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "5d33f7ca-6300-427a-b774-6290ee326e43": {"doc_hash": "3878a79c1c4c8b0849a9bbfc2e880f3d1ecbe35ca33e3cc3b1ad21f65027045b", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "8e6369ec-83ab-41fb-86ed-df86cf2a44d2": {"doc_hash": "1ece2562177dfb83a27f2a45bf6107d50e273dd358ff499054a86f06fbd5c117", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "dbdedb5d-830d-41d6-901c-9bf52167b7bd": {"doc_hash": "8589042cbc16ac9c046442cf5423c9d083dbb8ec43f59d1b043cd2d4f6a173d7", "ref_doc_id": "38f32560-9f7c-47ae-8ec8-39342886a365"}, "81ca14b4-4c57-4797-a6c9-406349cde7b9": {"doc_hash": "0e10981211000c6a0a9710f63db2d816303743ce0f6c2862d5a038963894fbf2", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "146b5908-3a87-4228-a108-6d7576a8a9d0": {"doc_hash": "a1e3493ca80087222641533df104c3cc9d668f243081b1a77a97f0df0e50e377", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "fb28101b-1754-4bbd-a0c8-ef34cd45e6af": {"doc_hash": "70ec459b0c79362ddfb9f61e7dc45e8a311bf53f9bb1641926210d8243522c64", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "6dac5908-de69-45eb-b98d-0dc1b198c1ad": {"doc_hash": "9c3902acf09b1133ea7e5dbdbdc88cfd3f43207898ed49dd58db939e3f25ccbf", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "ad128772-d46e-4256-bc31-81ad71f51ce8": {"doc_hash": "7a4f7e554351510e7f18ec807e5ed637cb084f1b5f16766a7fa54671b46af2a9", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "f6e9a70e-6a72-401e-bf1f-9129b76ea142": {"doc_hash": "8e5b1397ad1362b9773c1bb8b7bbea303ac60c7339a090a5d4945056bbbfddb8", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "ab501c3c-685e-4dec-8fbb-a834bd4f5e73": {"doc_hash": "760fe2b3f68cb237359d23d9697586a2389465d8d0c0935029123f231f3aa589", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "c7792e5c-1b7b-4831-8bf6-9817c9338395": {"doc_hash": "0ce639489f89d69c640b351892296d9772209baf3f46e507b34eccdda8b58877", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "d7a6bb0b-3ed8-4597-a07d-b7a1e6a303e3": {"doc_hash": "002a5f1d7336f5dc9230792e74f26787dcce0ea19fb3cb2d5c549398e6fdb8bd", "ref_doc_id": "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1"}, "5daf3360-184e-4a38-a691-509fb35e860d": {"doc_hash": "b4491ebb0ea53feee2bf4036860df31d0ef149716695c69db0619495506fe75d", "ref_doc_id": "59eef68f-016b-4d5d-87fc-59674048a3c2"}, "2da1461d-d860-4f75-9bf8-18b20f510508": {"doc_hash": "71c091d2b1d4a40cf68a8b3d16da220b9e61e40f8af7c7bd60bb5fb95e8171b5", "ref_doc_id": "59eef68f-016b-4d5d-87fc-59674048a3c2"}, "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406": {"doc_hash": "5b1f8129300f326b9442fa7a1ce2e6fdd07f5d05e9da6c27209699bfb32200f6", "ref_doc_id": "59eef68f-016b-4d5d-87fc-59674048a3c2"}, "9ab17625-df39-4728-9b21-0b82470b49d0": {"doc_hash": "18be73d52dc972e05e75ac7b239fc60546493b1ec7f2689a74da30461847a4bd", "ref_doc_id": "59eef68f-016b-4d5d-87fc-59674048a3c2"}, "fabd56dc-5425-42f3-8da3-4c08ee4e9107": {"doc_hash": "b5ac81972ef80af569708c0603b116cbfecc64dd8fe4963e2ae2fa9db0296970", "ref_doc_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345"}, "08317b88-fcf7-47cf-8bde-5a4a6c23705e": {"doc_hash": "dcd37f132a89bf2922bbe5c390249a0b18e11041ff32f1206ce20eb722f46dbd", "ref_doc_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345"}, "050258ec-7ae1-45ef-a38f-a7d3449c215f": {"doc_hash": "c9dd5e41e21282f0eb26819209de32ea7c26e38af0d552703f096e0c4e917bbf", "ref_doc_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345"}, "2ef40cf7-9c5c-470e-ba37-4d926b72ca18": {"doc_hash": "2054674e3b5c8774d2e472a3a6f29ad63da2b33b1f763d979659e08062ab4971", "ref_doc_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345"}, "7cbae6f0-59b3-4166-9d9d-7846cb093a6e": {"doc_hash": "f982f469ead846b53f2dbf85db18e85546d1d82685edf27cc8b80a82dc671709", "ref_doc_id": "dccd0a97-5ed5-4b6c-8f7e-85eea5316345"}, "9624ed82-27b2-4111-b2d2-48e4cabc77ad": {"doc_hash": "619e0efbe012be0bebbd5eaa5822a22510e51aab99efa79090c6e7427d080671", "ref_doc_id": "536708b9-e556-4812-9f4d-6d823e99f4e8"}, "bf768e2f-d631-4843-b4b7-b7d61cd91fdc": {"doc_hash": "cdb6e903c18c60a3b6a609c607488082183e8b5c8cc5298297f8fc514e30ea33", "ref_doc_id": "536708b9-e556-4812-9f4d-6d823e99f4e8"}, "399fda60-8bba-4abb-a099-0a96bf6bb0d0": {"doc_hash": "86d93d0edf5a7bbd3a99e6077dbbf770e1d2fa329d7aca5629bba67484bdcbf0", "ref_doc_id": "536708b9-e556-4812-9f4d-6d823e99f4e8"}, "60d14978-751a-4035-a64b-2c35a8774b7d": {"doc_hash": "6640d1d9e2bcf51a12f51e67ca7e3b27330388cc2c50c4cd319587617b3865d3", "ref_doc_id": "536708b9-e556-4812-9f4d-6d823e99f4e8"}, "cac01f70-5501-4a11-bbd1-0c911cc47565": {"doc_hash": "4a501fbc022e4d2674584426df060eba98ce08e8c57c34fb328ffab9a5997bea", "ref_doc_id": "536708b9-e556-4812-9f4d-6d823e99f4e8"}, "ae2475ef-2e4f-442e-8df5-f88ca66ea1cc": {"doc_hash": "f758b10d42d11ac244c0bb2b15caf83d3292a0629737f05993a7abb7a8b85d56", "ref_doc_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800"}, "d6eeea9b-7c28-44e1-8579-329a78e4526d": {"doc_hash": "e8a2c0b62753d42918d6f6bcd59a59f0d0a31be1f15ee365ba9a61b41c30a0df", "ref_doc_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800"}, "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22": {"doc_hash": "c7de0213c6e1b26ff1d679197b87dd360966cf41927713cb3e6f609be49410e1", "ref_doc_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800"}, "c7b12349-9fd0-41e8-80a6-44cc23a24014": {"doc_hash": "1d3f7f9962da1aa03f5779696d3622253afc0778c1630349a572b988150857ef", "ref_doc_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800"}, "988c203a-2456-4b32-84f7-9d3bc5f921b1": {"doc_hash": "1a525d96c03796efb7378bc9a1868d9107439c2a40188e75d0a1789c72e3df94", "ref_doc_id": "1a2b408b-98a6-4aa2-b477-cbf898b52800"}, "17b5b257-855f-4d5b-99fa-f52187320cf6": {"doc_hash": "58a70a9f219b2db2a300296963311b8ebf6ea490740b5ae21e7a594de51abce6", "ref_doc_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7"}, "bb89fdcd-438c-4fa5-929d-653fb15dabfa": {"doc_hash": "05b53e8986a4cebf7db37c5839e38c64272b1d1e63cad45d50ba357fccc7949d", "ref_doc_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7"}, "aeea271d-1dae-4742-9e69-2be174351555": {"doc_hash": "d416992d5cc5bac68d7e4d4b6ddd7184fb79284db2179a106ffbc1a4b8260565", "ref_doc_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7"}, "235d8915-3e9e-4c17-b1b7-94313b57761a": {"doc_hash": "982cf3aecc1bc121970c8cb63eb66e365a4d32bbe48a8c0998aa56bfa6560032", "ref_doc_id": "a1d704bc-cb06-4f3d-a8d0-46a3302071a7"}, "02c1c66e-2e62-4d83-b26f-3a6ad431f674": {"doc_hash": "1da7fb9dcc2a0fa593329e4a3425e5696c1f959d7f95c41e60f2fdda74921052", "ref_doc_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce"}, "32b1f121-e3e5-447a-bbea-88dc8eb1b014": {"doc_hash": "ed09b0f4ccf66b3e08ed689f832522dfa6cc83f6540059106fb107813a11af17", "ref_doc_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce"}, "2cfed885-3663-4885-af61-7ea22736305b": {"doc_hash": "db4bf8da8a4189576dfd6a42a22ee98dda05ad8fa3a9bed7fc27ca376272df2f", "ref_doc_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce"}, "8043580b-588a-4bd9-9544-a7250011982b": {"doc_hash": "5df48ca53dd3703ff48ad11669d8814a97d3f380e590874aa2db0b6beb4c552d", "ref_doc_id": "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce"}, "b9cfc59a-74d1-424d-8322-082dde4b290e": {"doc_hash": "9a467501488e09ff884c1f2a3a70994cc24f0fe73000dbc5a98baa5cc6189e4c", "ref_doc_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff"}, "4e3853e2-4806-4eb7-8120-cf5e0d4c903a": {"doc_hash": "1df6ee2df301d0c7769442fc5526e890679810c05e05f98a69e331a2b8534d09", "ref_doc_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff"}, "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225": {"doc_hash": "f739305c56310b9038a077b4def101b0ba133df8e83c07d447a0e37315a785b6", "ref_doc_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff"}, "f6bd5b5f-60a4-46a8-9f73-b26fd8880699": {"doc_hash": "95ad78ad6094165ad6fb0f2a2e5951aa5029dd9591035a1074fa53a4dcc99299", "ref_doc_id": "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff"}, "aa9e3d52-33cf-411b-a030-5b763d54f6b3": {"doc_hash": "85b1068b39aa7064924074bbbd08e49c2d11ec8bcd582a8465057b3d8f52f24c", "ref_doc_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799"}, "ed8f5ce9-293d-476b-b54e-4e72eac78dff": {"doc_hash": "e7d7374e2d3326a3a42e8076f7775b19772b520699e0a42a24600a9726413538", "ref_doc_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799"}, "45a48615-a087-4685-a35e-1ea3621b3a73": {"doc_hash": "00346bb59e5f2962dc7ed5dfebbb5da958f986159cf645774fd8108b18d8ef30", "ref_doc_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799"}, "d0b8f408-5af4-42c5-9311-a7dd34f519c6": {"doc_hash": "1e559975ab41d48dcb3bce71d4e5bfe3e99f4b145e9bfa9b232644db28f54a25", "ref_doc_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799"}, "829d0d83-dab0-43c9-8887-8059825939e3": {"doc_hash": "409b75f7c115599708a46d83f5578bfb0002b32d2f0caae5e4f163d33899876b", "ref_doc_id": "87dd0fda-7246-4d8b-bfee-7a48b86a1799"}, "d2f93468-b715-4847-a9af-24e305b43e4e": {"doc_hash": "2f8db80ede9e8d4654ccf82ebc571a3eb7f51ddcb979c7d14425e76833975321", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "6524f901-c391-4bd3-82a2-9afe1de48fb5": {"doc_hash": "c524de1f4fb104ef5ac2903a0c45e7bdc9ea498090999221047103c40011554d", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "0af99402-5274-4862-aaa7-7de365929725": {"doc_hash": "e38c5b5168f9e8b1fb22b46c60c9db1545e6c1fbf2c7568903a35c1641749b2f", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "193b9ae3-3431-43d5-880d-09d2ef130c82": {"doc_hash": "35ce06ce679687f1688f170bd7e6abeef85cdbd5cb2d5bf156bd2e7abcabe8d4", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5": {"doc_hash": "6a8cf7de5f984cb4f2bbf8c039c4c8655885b07399baaa9eaeaaf55f12d0375f", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "25d71e38-5247-4d10-bf4b-5afa4f0a1656": {"doc_hash": "feb6157da28449330ceeb3eed9ae93e338c05bb6e2893e1fb70685f2181625c1", "ref_doc_id": "02c8d105-1fab-4d81-bdc6-33dbf70a111b"}, "ea0f125e-91af-4ca3-a8c4-ce765d19dcb0": {"doc_hash": "da4d5bdfc1d748851cc31aaccda481a53c8a8a8789824bae8ce3f91ad0097e46", "ref_doc_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e"}, "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314": {"doc_hash": "03b1dfc59b4041eca15a027127786b04b9e506d03b6e8b3466fbf36a1f69f266", "ref_doc_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e"}, "2129011b-7d24-403b-baa1-b4b26de12f0f": {"doc_hash": "05b7ca14570e070715f63f9d4bc3c978d39ad3e00410e4326461dfc304f11bf1", "ref_doc_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e"}, "e70d9cdb-c1ca-4814-acd9-60a404b92b30": {"doc_hash": "d9b07b4789a318cb7cee1cc47eb448ea8cfeee85c20554ec183b1d5f56bb226e", "ref_doc_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e"}, "4e4f0a5d-cdc0-4d2e-aba7-653f1013c0a6": {"doc_hash": "2debbfadc037142819c6362005fce7c7631491c750eeff62e157b9c8b334f187", "ref_doc_id": "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e"}, "97ac92bf-f69e-4d9e-bbe7-e0db72980337": {"doc_hash": "f7fd143760ea7da174274b06f80fd08fd94ff101d08b7b88ac9dd70711bda30f", "ref_doc_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db"}, "a27d1161-6737-4ac4-b6fa-44e2f3b31db1": {"doc_hash": "5ebfbb57a418b59d1c756e021412c0d2f026f2e32fda1923fc143478810b3abc", "ref_doc_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db"}, "b349b525-3127-4814-a68a-088b10e9ad63": {"doc_hash": "c4d1e84357863c9891d912d0d715113745ed4d2dc02c52a0a88d950b313d62d6", "ref_doc_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db"}, "423c2908-9082-4753-850c-d644dfd7af54": {"doc_hash": "04622d9860417570dba6a3c8a491bed96a83766d25b4833d84d77378832c7a29", "ref_doc_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db"}, "d26239fa-4f54-4bd7-88c0-cb9c1000a81a": {"doc_hash": "0067ea1c87e326c432d8797eb993dcd92428ee5f1ad41f42f66835b4502238f6", "ref_doc_id": "d994ee85-dcc9-4a29-8b21-3cc141cbe8db"}, "cc0d8d1d-5d9a-4af7-ab29-a8fde5c3837a": {"doc_hash": "ec695d552410ea291dbbcdf5f26ddf08a4493eed0eda4930a6b9f5935bedf6d8", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "17b8883c-b686-419e-8d8e-0cbd44d4f27f": {"doc_hash": "33e4f12ebd374a0797008b3202d765f0b9aff749b63e10b33c3e92f0a652aa40", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "40a10066-c6ab-40d2-b2c1-0434d256200e": {"doc_hash": "e97fc521f248d35b5737a6587b4f9eca757923d224df0f5bfe0300ae296759fe", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "3594f721-d576-4d64-8f2e-589ef8f59690": {"doc_hash": "d1d3a41548bbc01504864e7b9bb882adb7af0a59647ad32ef48c6f84fd861f63", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "e05d602c-8c2d-476c-b180-cd06b58596af": {"doc_hash": "1e5e45a0aa7aada8ffac19ce4f4b591f2bc89b9b0f2b7bc34076b526873f12f4", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "cc254142-af5e-4ceb-8172-6157fa17e2cf": {"doc_hash": "ba6adb2031b584d4162e5c08de5c553c77ccc617a2ab2449dda87f4788316adf", "ref_doc_id": "6af92d4e-e412-43b7-923a-4e2ce0037ac6"}, "e7e541e3-7ee6-441d-b24b-a3e8048f6384": {"doc_hash": "db6d2440949d564da6d587f5b32f236d9799a7a9d696ef8e652dbfb4da29f799", "ref_doc_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d"}, "b97ba877-7824-4acc-8f9f-465e1472ea18": {"doc_hash": "e8a5742b506655ef2fd0a9129e568dd4483d8e8fac34c19645c47b6575dc08ea", "ref_doc_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d"}, "76ab8edb-6470-4688-99d5-833556bef64a": {"doc_hash": "d1166c252b9765a24e3338996ee330d2b844398bb9c7f377afa32100aa2cb9b4", "ref_doc_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d"}, "b5833bc4-80b0-4710-b1fb-16a5da2cd633": {"doc_hash": "8b5b7fcca151f4c2993cdec8e0c06b9c63dbf884719e60dd47dcea55d8e7391d", "ref_doc_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d"}, "62b74a8b-cd70-4c50-b298-223e5a6881c3": {"doc_hash": "1fdea26b7df037c7d77322b5bfaa4b89527ed12d0c3b45e4d910cbc888fbc953", "ref_doc_id": "3356041c-4d5c-4a10-b963-e3f4ff36da5d"}, "ee0f177b-9942-404f-8ed7-6758c5a5dfea": {"doc_hash": "3dbab6eb4cebc56d76119bee40f34107ce1eb62c4047dbd7a7ab450eb39ac108", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "6b0cce89-d72f-4e82-9d1a-9535ddbd0822": {"doc_hash": "69336c5088f8c68aedcc9d0a1530da7d62d1a6e7ae4c3ded3a8b5bdc6909f152", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "e6769ff1-0f97-4709-9dda-4afd6a1f094c": {"doc_hash": "4fcec467937c88a7f7c555e294b3f7ee2e76a70b82a947cf834bfbf7b5dd2b02", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "033463eb-6bfa-43b5-906c-ad1fb49c4b3f": {"doc_hash": "2d1a6f87df4675121ffeb5d879f5f433c4139890c7c16c098f2dcd60102eb04a", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "d94f2189-770d-4665-827b-3735f71a4d50": {"doc_hash": "224a60a87d1d1758d385f8b08a94ecd9e5459d622e3797859fc19e1281f6b182", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "6efd1c36-a551-4874-984b-a8a83ec83db7": {"doc_hash": "f57c0157723f8ce4f50ecf7a4a327ae8d0411a0d405faec2f4d38efa5f64d816", "ref_doc_id": "24efc6a7-13e0-4628-81e0-df1cc32f050c"}, "de2b97ff-76ca-4418-aca7-9d06049ab391": {"doc_hash": "ba7685133a1629b250e329306be3e22c6b49f475924afdae5c32f3bae37b0378", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "70289da0-b495-4391-bd6f-5c7aa86b6217": {"doc_hash": "d879ddc45b60acb022a0d0584bfbc2cef6b74d479b6dcced43047ff9a9668f92", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "14f089fa-3fb0-40ce-8faf-297a185f267a": {"doc_hash": "1b80491f1a8d05842918d448c69da02a22e2a55411484e17aa5358186245b484", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "9154366f-887e-47aa-b204-00f941f6e7f0": {"doc_hash": "1240159b61dc3a43781aca158318bb15f186b9ee68a6ad6e63cb03b98900651f", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7": {"doc_hash": "7985cc273072906796dddfd0d017712d70a87ca602da502188c80889d4686088", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "207c77b5-9d1f-4bb0-a419-5b72314f561b": {"doc_hash": "910186b5620387f44965dbdeb9ee05806e2964066fb727639870d662d64ca83c", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "17e7193b-cfff-48dc-bc30-99b0c64743d4": {"doc_hash": "64a2626ef21990b3997036c4ff94200c7cba16f4b3363c72f0ea003344006ad4", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "a8aa194b-9d72-4b7b-9c4a-e760442d85b1": {"doc_hash": "79de35e8963baa6234f05ee788ccce5fa0fd32a37805e7bf7b6818234c464a5d", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "ec687e40-ddd1-4202-a8d3-ab0e13274db8": {"doc_hash": "821ea946d5455deffc0a9a8ccb81acedcb26d954274d976bb7eee80d67162d29", "ref_doc_id": "95385bde-ed41-4a53-962b-559aeedd3586"}, "8dc98971-ff36-407f-baf4-857e4f7a0812": {"doc_hash": "83eebff1b2059f9e841c87d88391ddd012908e21ee13d7e7e805e419830068e2", "ref_doc_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d"}, "3a08a900-13ed-4fc1-ad37-ebda5e215ec0": {"doc_hash": "b04e4506e0bd2a2924a023e4ad6426150d2c8bb313380a36657d1e37ecf81d02", "ref_doc_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d"}, "9d0634a1-d312-49c9-8f8d-7fbc31d8f614": {"doc_hash": "3b17d90397d9321953cc3581169cadcc1e712240beea66c2ac07347e75e99f36", "ref_doc_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d"}, "3ac3e06e-fa9b-4ec5-a06a-fd548856600d": {"doc_hash": "f35cb79868165a09e1a88a7ed2d93f4e47e3284e6267df331a671051ea90f4c7", "ref_doc_id": "ba98efca-6d0a-4fc6-b6f1-275db2110d4d"}, "5461f41f-ac13-41b1-90d2-d5e64bc5e33c": {"doc_hash": "5a5f8265a5cb68e6d432f4e4b77ec5964e82c5ce27a32416401b18daa7d35757", "ref_doc_id": "f4e31095-9605-4685-afdd-66b105bcdd09"}, "dc579127-3253-49da-aeb3-062e737be11c": {"doc_hash": "01035c48417e99f1f496d2aa210aaedbdae0c9ec2d51fb7aab45a6d183f1ad77", "ref_doc_id": "f4e31095-9605-4685-afdd-66b105bcdd09"}, "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118": {"doc_hash": "b6f423f716a193c727cacd3730371747a06fcbd3fc188a0f46ca3f91b5c44b57", "ref_doc_id": "f4e31095-9605-4685-afdd-66b105bcdd09"}, "9c063cdd-32a6-415f-8862-92d5a4c888c4": {"doc_hash": "48c1e3da4b1b26196f8ae36e8aeec008b0d2a74ba2f0833bc4ff53de31b9f995", "ref_doc_id": "f4e31095-9605-4685-afdd-66b105bcdd09"}, "7a7251a5-bb5f-45b9-9465-cb4a43938c08": {"doc_hash": "5ca3a99d7fa4b92ab3aca292a70040025c75d8c8402d2230bc47f2370ff22fad", "ref_doc_id": "f4e31095-9605-4685-afdd-66b105bcdd09"}, "0e170b4b-3014-4c0a-93be-f9654d7a30f4": {"doc_hash": "41cfb289cf3803facabe677c9e506fcbb18a777aefc550cc0fa7e274065021db", "ref_doc_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8"}, "fb848190-f685-4699-9262-6cbda309dca9": {"doc_hash": "9e3f9b52442c0b7a780f0e0ea5f86374eaac9e9ce60e507cb4b1ecda1cd7924d", "ref_doc_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8"}, "ae05de58-4ee5-46b5-ae54-af30c173d2ba": {"doc_hash": "e2ff3b8161da0155046b4da2e306ed1d2b40dc537764edcc667b55e72ab9eeb9", "ref_doc_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8"}, "2fe777df-c74e-4f6e-a56a-45f4ffedfc82": {"doc_hash": "cacf760ecba69006767bcc5124f942ff7bdd44e4a9b00a0e941e7cb5358f532f", "ref_doc_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8"}, "114af520-8d01-4b74-b314-ffb9b9e29353": {"doc_hash": "72ad9fd4083b2f98eb8c63912ef74ae725bee17ef399a08aea57066abbdde6eb", "ref_doc_id": "e0081c88-4609-4b07-9045-7bb488ac4cc8"}, "a29f4449-2334-42ba-9faa-f4afaa7a408c": {"doc_hash": "a9513ae3d8403e76285f97d0d78091f8d3e94f68bf97a1b40259c49f0b706fa0", "ref_doc_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd"}, "cd6a53d3-a669-4546-9f1a-6b75bea6b480": {"doc_hash": "4f4a6a9e066fc619a4c1a69af433a927527cb881c3cb2d7fd82e5193f1b3b401", "ref_doc_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd"}, "03418609-31dd-4a95-bd2e-b6ebb7169b7c": {"doc_hash": "b057624955aac639ac1c1e11fb54427775d249b86254195ec7948193bb44b8fd", "ref_doc_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd"}, "3bc45822-d595-4fa4-91ee-9d600f6e9c92": {"doc_hash": "d7e3fa25e64570ca17503773cc30cb926cb3295c40d1fe969e2446c1b2112c41", "ref_doc_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd"}, "e2a00ccb-5e42-4139-b870-bcb41596224c": {"doc_hash": "3767408aead974e9a09c68ca3d9dbae012ace60d067af24dc9091b94ed5b16b7", "ref_doc_id": "6f03113a-842f-4e0f-b55f-a13081a66bbd"}, "f05dd48b-f840-437a-a67f-52a7a83d28a6": {"doc_hash": "92a0fc700c26b295f58f44b1ca1a6e8a1de4179a8f2dabb9f7e68a4fdafb7b81", "ref_doc_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306"}, "f19c4011-fad2-497a-9c01-a65ad2344ee2": {"doc_hash": "27a80b78abdb33a1c010e4cfc9a1d3904b171950839ec925ca8fa02f2eda2fc2", "ref_doc_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306"}, "af9a9926-097e-4c2f-956a-b9e1a9a5dc14": {"doc_hash": "80ffccd4ab888bd80c571b65483c9e4d273d8c66d484b5939fdaf3bab2e3e0bf", "ref_doc_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306"}, "c93ee1c4-2a46-4deb-b086-dd53e1688198": {"doc_hash": "e72a7efa441ac22bc16159ea4311301a45626f8511baac3e50751c3595665ede", "ref_doc_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306"}, "39ed0dbb-b52d-4367-900a-ecb2830721b6": {"doc_hash": "6368189e4ccf3f3f1238f7d00df2bcaec0f343bf49e40fc9cbd69818577e5fce", "ref_doc_id": "8e5e3d2a-db6f-494a-ae49-f8478478d306"}, "7e8a4bfa-0d20-49cd-899c-013297293e15": {"doc_hash": "ade11c41833eb0c6cb85eb413c9edc281a10a8a8fd0c9cb9ade4759a93b45cd3", "ref_doc_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3"}, "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff": {"doc_hash": "7728cef3676aa259d1b7c04539e0c2f47c739c1f2aaf917be7645afcc96629b7", "ref_doc_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3"}, "123ceb7d-069e-4260-a6cf-829b27c2e414": {"doc_hash": "cfd397736b37a0cc8e9ac75a7243140c3a73892fd06568566a890e572acb43a1", "ref_doc_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3"}, "98ba43b0-c79e-458a-8173-92f853e2f865": {"doc_hash": "64d57bb5f20b36b4da296863c97514441d0a87fd2e06b7b0413873a88dad40c4", "ref_doc_id": "cab177e0-b83a-4c40-acc7-e72a74eef0a3"}, "b9581539-9c27-4b14-b87a-b1f9453b595e": {"doc_hash": "90c8518c472735c90902011da3c20ba42bec45e3c3a0b0a87ff0b0c9caddc9c8", "ref_doc_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4"}, "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83": {"doc_hash": "8d3c7382e70543d00f1739f8c0efc4556971e02fc1b59fd09def5886993f4a9c", "ref_doc_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4"}, "bb03027e-2cb1-4acf-acfa-bd091b17925a": {"doc_hash": "8fd2269dc3add54af61718d051034c43706565daa094e090432f7f247e388675", "ref_doc_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4"}, "c09bbfad-90f5-4042-8348-efe554b42b10": {"doc_hash": "06d4203b1d7ddcea0abc7655512ef3bf772491f20586fa404dfae0566980ab1e", "ref_doc_id": "853809d8-6c17-4b1d-8a37-e0c33c02aee4"}, "a8c719d6-765d-450f-8802-d3ddf3999ee4": {"doc_hash": "19c2786648d75384bbff8e31269f55a870ef15840e711278fa5adccfe68c397c", "ref_doc_id": "80593f87-d286-44aa-8dad-b7b1ccd9284b"}, "bd223082-e5a3-46bf-b9e8-71b22a82d318": {"doc_hash": "7de61b58db49fcd3dc33ebc7556eed7bedf3ac518524ea3d6de41ffa457630fc", "ref_doc_id": "80593f87-d286-44aa-8dad-b7b1ccd9284b"}, "12d40e66-3d9c-4784-8e82-2081fdbdcf40": {"doc_hash": "850e5670534ca945438c21c5139d0fd98f8b0524297901701acd1b9d4b6ed457", "ref_doc_id": "dbf356b3-ba72-400a-a12e-d464101bcf72"}, "f14048be-df00-407c-b1e3-72ae8def3d15": {"doc_hash": "e7542dad71ac7f2b841e1be190083b22e50728bf76f762a6c7b91fce85e3fb27", "ref_doc_id": "dbf356b3-ba72-400a-a12e-d464101bcf72"}, "45fd1fd2-357e-41d0-8964-eeb46efd782e": {"doc_hash": "813c9043350428a42bb8e824efa57eed157ff86a5cb9fd9f05e2a328118b751c", "ref_doc_id": "37df766f-7beb-4a8c-8cce-46c85d739594"}, "2a068120-2c9e-4eb1-992c-cd851b2c880a": {"doc_hash": "59ed52540dd79c368d8cf0cb001f5cb5bcf2352ad1cf5ab28813ad9808e32dba", "ref_doc_id": "37df766f-7beb-4a8c-8cce-46c85d739594"}, "601e0d9e-562f-45f0-b7ea-ce42a33f8c29": {"doc_hash": "8fdab3821935ea7e70e70e1df98328d4b90139321c755c58268f24b9a1a407d0", "ref_doc_id": "37df766f-7beb-4a8c-8cce-46c85d739594"}, "655bdd10-e7f2-420f-b69d-714523e83cfe": {"doc_hash": "9aebbaaa6c6669e46f5e1aa5dc132bd09918753b7122d7deedef74dd1dd6ab53", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "95b38490-f17e-4157-8114-901692b8c2f0": {"doc_hash": "d5f08d31f5b5b08a9c491b047f4c96334991f8958bb54246aad2dc9199e3c40c", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "3223bc50-70e9-4965-93b0-f7ff73316f88": {"doc_hash": "44bb1814ccb19f10f450c5d2be9587ac5d880b69d185f74dd71a3097f4ac2154", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "9863ef36-c206-4404-b132-8a58ca2bad44": {"doc_hash": "a9d570113c92e72cf1bb2079bc7be26bbdbbd7df1a7e8cc449d35e151e4fdb6c", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "a70a5a57-8aee-4535-bff8-45f2838c8427": {"doc_hash": "00d7aae086b7d6c60a9b22f3c990c0eb672a57ddfc4f3ff101eaf9140f337d7d", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d": {"doc_hash": "b046e004fcb0278563f94e08ea0d52dc4ad03f50c70924431eb9178b78cfc04d", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "f4f132d1-b564-4a5f-9bf9-0f92bed4362b": {"doc_hash": "0071064461574047775cf38be987c8e3afc4ad33138bbff6d33f8a6fbcc2efeb", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "7b227e76-fb11-4d64-80bb-46e5a369f0b3": {"doc_hash": "83f3e001c65a770c4bfbefdff3b982f1089b74994f82b65fe6fce5aa42e4caf7", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "eeeefc92-f126-4be8-924a-9cd54b409613": {"doc_hash": "1620d9b577d829a17b2483b6928e111708d002ef425b1768ae1acee5d2382748", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "ec604294-391a-4a95-b2c8-af2be54bcd1b": {"doc_hash": "8ef4b81fadd51a3724d6cd160b1f6792f138530bc3106fd08f6da04a7b3001b7", "ref_doc_id": "576b46ff-2bef-427f-888a-3f943bc07c53"}, "2f088893-7aa2-49b1-b979-23278a0de780": {"doc_hash": "4ddc09cec2d66fde18e0a4e2d10827163795a5a0b2a31a62fb154c238f7fd584", "ref_doc_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b"}, "e3824c77-aa3c-4e01-bae2-ae5291aa8e62": {"doc_hash": "5f64c7b00019a5fc4bf91dc963e09e4a03473a8f8a2d8820dc84de6168c8328a", "ref_doc_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b"}, "62b31508-96ab-46ad-b9eb-a7dff8268e55": {"doc_hash": "f4bdcce25147b7aed36347d25a6b8fe26d11ee02d31e100cd7167d276a4c494f", "ref_doc_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b"}, "f68b309a-053d-486e-9355-47074dd4cb4e": {"doc_hash": "8e88599f89acd16116ca9e8f16476db67ca5986d19463c0d02b03e5d3759bf72", "ref_doc_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b"}, "d7d918eb-0368-452e-9178-e09c2fc5ec05": {"doc_hash": "f6df03b8441b9b31feb7f80fa297e8d0d247a27798a3ab54e896ddbbe95dff2a", "ref_doc_id": "c0c2b433-a1ef-49c8-abad-554bfe8edb5b"}, "d9da70df-7ac2-4a2c-8b9a-75e3d229b35b": {"doc_hash": "7335f4ad782fa3124c867a479d8b492d09f36515a2fc182a26d3253c6cc4cbc0", "ref_doc_id": "50ce1c55-a6be-43e5-b962-03da124cd818"}, "221aa72a-16a1-4638-9041-f14b562b942f": {"doc_hash": "a56b44cfccd600e07127e6990ab831c430ef20730f2f6e40e53af87c1c4dfcda", "ref_doc_id": "50ce1c55-a6be-43e5-b962-03da124cd818"}, "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0": {"doc_hash": "b213d1105d4d280fbbb7e1c440fc94daf47b06cd23644ea3b647c4c27fb6dace", "ref_doc_id": "50ce1c55-a6be-43e5-b962-03da124cd818"}, "640f38a9-0f69-4c65-b760-e17c82ea3cb0": {"doc_hash": "d9084b6d824e9a0b8e6644bf23987c2ffdab773d0d51496c54fd17ea8ff10370", "ref_doc_id": "50ce1c55-a6be-43e5-b962-03da124cd818"}, "9e3f9c73-9be0-4ccc-b52f-1346924d5d58": {"doc_hash": "c623b07ee36dc908c17453597071beba569a6f635d482d384d96179e3fd5ba68", "ref_doc_id": "b857565a-9b31-49ad-8efc-750a044471e2"}, "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc": {"doc_hash": "82dcb86031d221e19d52ed1d626eb38e686a9cccd553b1862a9ae3c06e7586d5", "ref_doc_id": "b857565a-9b31-49ad-8efc-750a044471e2"}, "12032502-67b6-4840-af2b-8859c9b891b8": {"doc_hash": "0fd7dd4610dec71a6a55471a3095ee6e58d10bb94f979b2e222ead3011da5ce2", "ref_doc_id": "b857565a-9b31-49ad-8efc-750a044471e2"}, "f9961334-20f5-45a9-ad38-0dbef61f6bb9": {"doc_hash": "da5339cf68b3d6e65c93c50d8b43b98f031c1db5f2b7be47da3d437731385dfd", "ref_doc_id": "b857565a-9b31-49ad-8efc-750a044471e2"}, "6c23e8af-3854-4dd6-96b7-7468324ce64f": {"doc_hash": "a4218180c7dc838c87fc75fc6f43e4bf90b2e71862be71d309e2ea1a944cf5bf", "ref_doc_id": "b857565a-9b31-49ad-8efc-750a044471e2"}, "61d5de64-924b-48b9-8d09-de3c39535ea6": {"doc_hash": "4ba00da643e944c0e8358e5b171b1c98782f8ebf5ac92e23d434594716cb5758", "ref_doc_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397"}, "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9": {"doc_hash": "a4cf0389b94b4c8fe0b7857e6d3c16e8d4f8ce94ba8063397301e7fbb3b5f1bc", "ref_doc_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397"}, "882487fa-dd27-4ac4-abfb-9e131b99b8b3": {"doc_hash": "a697dcc41d6592d6aa52e80791273523e44a59c9b4645dfdb8e9823007655a47", "ref_doc_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397"}, "373d57d1-8de6-4d76-85df-0606c58f68b5": {"doc_hash": "69bc8cd0975b26cb13417d9a4194cf009ef1921e7333c8a2976714ab3a7ac815", "ref_doc_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397"}, "a6b2c537-b1c4-4e47-9263-978274ca886e": {"doc_hash": "9c961eb6ae0688527c3c4d44f4cebec85934346530d50a5a41f29418041b0abf", "ref_doc_id": "63cac535-9ed8-4d00-ae8a-b0acf089d397"}, "b149d58f-d752-422a-8418-8258206ff9cc": {"doc_hash": "66bac4dd57fdd91f093d5b0449bb73d1d950527397923120e303013c35f64da6", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf": {"doc_hash": "c05b550061ca3d8f65a3161025aab999d5d9188275e47d940c5e83b91d0e9134", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "943d269b-0be7-48ad-86a1-9ab8196db174": {"doc_hash": "2cfd8b84e011c545ad7d6e6aed2d36da6d82a68ccbaa137d4c47270e8201ff2d", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f": {"doc_hash": "bcafc4708e1453beaacd024bf602834d5ab9557d43199abfba193dabf6240aad", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "378b365c-395e-459a-961c-846bd5f2c727": {"doc_hash": "9ba31f727651d083a67132a9a504c4ec9409a9fa893aad9b889fd592faeb026c", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "87ccae18-aff8-4c26-a145-0c3516a2818f": {"doc_hash": "c98239c53bf83abe0647ff37f87406b0937178a052e62167d3b34349db410208", "ref_doc_id": "67675d31-cf82-42b4-abbb-258c7dc5f285"}, "68e7cb03-b125-45ff-96b4-3d7512506ad8": {"doc_hash": "f8ead11e06fe1a0c032796a6aec6591ad5f8151e675f5ba8e5dbb125ff600376", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "8237a993-2c64-4f41-972b-950fda4551d1": {"doc_hash": "fe6b2486fb5843965d0074edc7b07720197ef820da980c66cd4f4e1d6a3ad21f", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "eabf95ad-ff91-48b8-b63a-52f4e460cf77": {"doc_hash": "0b2d86d57c4bc619e5de3dcc6b6dec0f9c20956fbb379ab80c347d59847722a0", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "92a799df-57fc-44a0-a904-36309541ed64": {"doc_hash": "511a3100a906e53b8ca06c607b6f8ff82446c47d6ebd2e4ff03d563f9e58f8e7", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "cea040a4-31df-40ee-8f15-563dfc620e40": {"doc_hash": "2d608d065d10a7622349a5ec967fb3cd5f71fdf1333dda24e21cb959c006f02d", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "f0de7b9d-4b9c-4207-b27a-df61da25339f": {"doc_hash": "d4126f3e5063d82423ed92e0b6123e1d0c5eb7d7f93dc7d76191a823ba290df7", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "4c0426b4-04c1-48cc-bd80-135b942abe0c": {"doc_hash": "7314f6b3cfbbf2b847258ac084a7cc082acbc162606bafb98bd06bf5b5b97294", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "c7347f1c-a447-47d5-9a63-fcbc8108c8fc": {"doc_hash": "3f647272c5c47d2301e398a53b0b3473be7512e2125728b290c7624a172278b5", "ref_doc_id": "21dcbe34-54e6-49f0-908c-c2ee0993e7d6"}, "bc9edbf6-0e73-4e9c-9a3e-389317270d4b": {"doc_hash": "cd18f10ce23950df0015e8c317f3f73b6fda95db0e67c34d3bf19510b8859c83", "ref_doc_id": "836f0f92-11db-468c-b612-489056a194ca"}, "93582014-c191-41b6-bac3-ea65c8b89adb": {"doc_hash": "16d0d5b9c4b7efd9d2f9feae2a02d141c0b3445c727c3bfffe4ee8a309b02be9", "ref_doc_id": "836f0f92-11db-468c-b612-489056a194ca"}, "8057992a-fd5b-4ed1-9135-289212cf8a68": {"doc_hash": "61cd96155d39e771c33a88a67a8796490ef2de965fa354fe7892e8e7b77c42c2", "ref_doc_id": "836f0f92-11db-468c-b612-489056a194ca"}, "56aea8ef-7a53-4807-addc-21f3f1539210": {"doc_hash": "d56054a9d725830bf57e7781f043753485c9b2090f7fb251f729bcc4329a06c0", "ref_doc_id": "836f0f92-11db-468c-b612-489056a194ca"}, "0e6c5ea1-c5d3-4c93-9488-e350660d73f4": {"doc_hash": "800d4707198f99249f1205ca13d6c47f91c646b917306e01bdd3a520a9d7ca67", "ref_doc_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4"}, "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95": {"doc_hash": "b63bbec1b12dd844622eb2a49bda9f478cbb2d23930dab7db0e70717d2ca43cc", "ref_doc_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4"}, "1cdd4e00-4c8f-411b-b39b-63f87193e09c": {"doc_hash": "b0aeaf1d9b3449ec3648797e19e81a0e4ebc9bf0e13f550a4f2e19caf8ea758e", "ref_doc_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4"}, "86993457-55fe-40fc-a436-a84463a1ab36": {"doc_hash": "ba33a1789e659cf8a631295a56f69780ca6e32a47c96f324979f9690ef3f7fa7", "ref_doc_id": "f03ceba3-fd20-4c33-9b51-ce430eb87aa4"}, "423ea5e4-3d44-49d8-8a8d-f035b1888e7e": {"doc_hash": "60d09505db734f9f90cb91bb55a5313e1321de1bf659ff0d186addab72e04423", "ref_doc_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9"}, "dc941632-bfc3-4337-aeaf-ed6fab3b518c": {"doc_hash": "e68aef5d1f0bd34bf934b9de22489f660c658d36ec7ecb5cdf7953ccc309785f", "ref_doc_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9"}, "8667874a-42fb-49d0-86f7-492365e5f8ea": {"doc_hash": "c22125e20770f02794132978152e712f12467d84b59d0ff86193f4c1c22138ae", "ref_doc_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9"}, "6e50a6da-6de7-4dde-a84c-e539cbe7c797": {"doc_hash": "ae0805aa4ea9ff693db764ad1608902357d03ebc0a4d18fc5623ba0a81cc829a", "ref_doc_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9"}, "2c270f5c-fe1f-45fc-8755-3201adfa935b": {"doc_hash": "631749a06cfd5c82229009e84ab24f87d7c58b1ed0a033b6b8e8119b36ae0934", "ref_doc_id": "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9"}, "4ff7253e-c5ff-4c06-affc-88d34d7cdb95": {"doc_hash": "29d2f6496f2fd4cc0da8079757381b0ee90b7b0b2aa197925d9bb1c838b3336f", "ref_doc_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89"}, "afddfe59-4e82-4684-b6fd-8379c37e512e": {"doc_hash": "4ddd76e2ddb351dfee579f59afe218173d79b19580863f339b868752bc47651c", "ref_doc_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89"}, "e1fe1997-ab22-4675-831d-eb9d5d5f7082": {"doc_hash": "a061b2a0e924e5f8c1dffa06d55da481fc109461f34f8efc54dff20ebead3159", "ref_doc_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89"}, "1d25d7e4-8ab1-4d26-b50d-d27364223436": {"doc_hash": "b9b1fa52486ce273a1b7cec7c51421cd1fd1c6d4faafb286e574ea8a59ff9b8e", "ref_doc_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89"}, "5f21446d-d189-4465-acf9-77d0866403a9": {"doc_hash": "fd385c7e1dc6d6cd7a52e9f57814e1a5d39eb9fb4d9eea46309eef3824f094a7", "ref_doc_id": "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89"}, "32181b2e-94d5-47ba-8d58-1714e8612b8f": {"doc_hash": "9e24dec869f919028c2f9639991b2502846cc72bdd4d2c13f04fb5924fa9401b", "ref_doc_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372"}, "70edd82f-8056-44d1-81fa-67db0367dbbf": {"doc_hash": "1e8fd79def5752f6672fb3a43ba3465ee519b0662481e893bdb330634c68e685", "ref_doc_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372"}, "e8ae14b5-c3f8-4638-be51-9bbc31f7971a": {"doc_hash": "1a6dd24efe98f4d811129e0f588bc54b496919b7b0fecde6f62d14557c79b9a9", "ref_doc_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372"}, "99a51fa7-311d-4292-9437-18b5a44d652d": {"doc_hash": "76c7503e7ccd233d3a5f044f126e52425f55366f6c3e653e2643d921b25d2f08", "ref_doc_id": "dd2e921b-bfc5-45f5-a6ff-e244337fe372"}, "59e60626-4fd3-4835-b905-084a1344928f": {"doc_hash": "6adc37773c609e57dff8f6d890773d4a46cb1ebf03a7e2eb33be841d73d8ebaf", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "06a7078d-dc86-4d21-9bd2-bb2445d89fd8": {"doc_hash": "f00b7e6e8adb12e4554b3539cf897df590344433783bd2835641fb59ff363fe5", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "c65cb3e1-dfa4-4d14-9a84-3fba479cda33": {"doc_hash": "f5711284092e8a40558bd53cad253cdda1ab0bbe10551423ffba5186628aac55", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "d50bbab5-6cf5-43dc-a386-c49daac9d6c2": {"doc_hash": "f17ea8eeec7d2bb3aac120f73cf9ef603f82c16496fc6044bc223737e9585027", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "1a524821-8bb5-44af-9a3b-4846d0b0767d": {"doc_hash": "353e886b79875fdcd536c381235a82ff39c371da90ce887e6bca47a6fff17d76", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "8d234997-a658-420a-a333-21fe3536aeb3": {"doc_hash": "044202f7274d0659ada7ece515b609711d68103e033886d139e8b8357f63a31d", "ref_doc_id": "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c"}, "bccf927f-2bd6-40b6-868c-0c4cc7193ad2": {"doc_hash": "0c7fc8676f7a2f13ad80318e42181c8ebbcb464ad9376ed733e39a2d20fd1104", "ref_doc_id": "6465db16-388e-4280-9be3-b7bb772a2365"}, "51d8b82b-51be-4501-8db0-f60bf3a9b389": {"doc_hash": "daf139c260fb0df6a61d33b6d61707acf9d9acdd947922f6d3b360cb8992a86d", "ref_doc_id": "6465db16-388e-4280-9be3-b7bb772a2365"}, "acfca739-f62a-447c-b84a-aac58ac59b1d": {"doc_hash": "8c410bad07828da7162fcc9f6c9fd286abe2a66cd5dd6eafaf958e8f08909479", "ref_doc_id": "6465db16-388e-4280-9be3-b7bb772a2365"}, "82e53b97-815f-4e82-acfa-2842a1acd529": {"doc_hash": "83628fc9190cbfd570c05747b3ce773c3f89da88b49102ff8f17097fc80679d9", "ref_doc_id": "6465db16-388e-4280-9be3-b7bb772a2365"}, "18dca209-9d59-47fd-a55d-66fefd858a26": {"doc_hash": "ce26f1403a9529471b6f7192dee71c44490836c0e04a4271ded9bf2d24451244", "ref_doc_id": "6465db16-388e-4280-9be3-b7bb772a2365"}, "922da7fb-431d-4cd4-8364-3e0406c28359": {"doc_hash": "5e3f19c6e610513bc0049c0e7dcc5722dc6b873e9698d12a02a50b9fccb21282", "ref_doc_id": "36529018-68c0-42bc-8369-da139934bcb5"}, "a2896f8f-b999-4c25-8b17-2e32fffafaf2": {"doc_hash": "9643b03852ca7476574102e86bf510bb7f69a3e457b8cf914a381dc7fd54c5b3", "ref_doc_id": "36529018-68c0-42bc-8369-da139934bcb5"}, "da2cacc8-f7c3-473b-bde2-848003fcac35": {"doc_hash": "498ea7bc1ba72c1d5334eb09d12d5824d92b635b9476824ab9e93252aeadee52", "ref_doc_id": "36529018-68c0-42bc-8369-da139934bcb5"}, "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0": {"doc_hash": "e443f23cddbca6648bb69b38773b66bc6d965b669a58f7d836f1bd660c93d2a5", "ref_doc_id": "36529018-68c0-42bc-8369-da139934bcb5"}, "03a7d3d5-135b-49d5-ba58-2b174f709eb4": {"doc_hash": "7043b6705ca5ad15c084fb9af88094ee91462d13f9342ea4e57f28c262faa310", "ref_doc_id": "36529018-68c0-42bc-8369-da139934bcb5"}, "63ee965a-2361-4478-9ee7-bd3563674599": {"doc_hash": "61040f5214e56dcf7bfb1208391670d5261da78bcb1f028bdaeb4f85232b075f", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "efa04065-a027-4aa8-a220-eb40177d28c0": {"doc_hash": "2f4893967062dd7f9bcd7401cc1f804181ead227d6c3b4dda0b9d1c71801e3eb", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "d4f342aa-04cc-445a-98fc-1e9069da2f43": {"doc_hash": "5781c1894f2ee30d7303b44532211db302fb1bcc2bb29c7614545d8b953d2e07", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "e6984cba-241b-4a14-adda-b02d918724f8": {"doc_hash": "703dd4934eceb9cd99ddc6581b8a5d14c18ebcaddbedcbf65579733e443cecfd", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "46f76247-981f-48ea-807c-15a942ef9446": {"doc_hash": "8304e6f8e1e9fe50f9fcd1b87c5618d599d5977a558bee8ab9a8b5b87e3e4c30", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2": {"doc_hash": "45b438d823d1056e58b86b032f9507a7e0be64bab849ab8d8fd12bb93f55093b", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "3aecdf73-4f6f-44ba-96c5-9358e73957e1": {"doc_hash": "2b8b78566b88d21b1396f68bec5ff526c0f35ece49899afb8fca786f4d5c624d", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "25ca17ed-8f48-4e41-84a7-8c2521a46492": {"doc_hash": "01ae2e247fece2e571b4a79692edcf47123501d2567b129ff46c666cc4acc30f", "ref_doc_id": "5662bdb9-10da-4a26-b339-469fd9bad27f"}, "48ba4ea2-cf51-4e69-89c3-4085304a6536": {"doc_hash": "421b33a32c751b4b2d049fa6913fb822994aeb42768e8386ac5eb30f952219c5", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb": {"doc_hash": "189de80c02027be80a06d63b6f6ffa2a599d0be46f6fa6081a0c805c80a99bab", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "93dcf9c9-87a6-411b-ad4f-004002f99851": {"doc_hash": "52087d62e24842a15ffe36c5c39712cae048bc16b3d64f0fb9499b2c9d1ac466", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "bc3a40ae-0e93-413c-a38d-83c572ae60c0": {"doc_hash": "ec8b94762f1b944aa22f2fcb4b4f93c79382e41e7ed1cf00ceda098d5305fcf9", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "feda05b9-2396-4b75-baa6-1dde06773c4f": {"doc_hash": "6901f8a9de30c82b288f308fdb76560ba9d5c88724f9cb3233f7bac98e99971c", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "e3fb2b73-bcb1-424d-b286-492354199179": {"doc_hash": "4ddcb493f5fa012f222c09c46f49e416ac935834e25ee0dd175dcc3c8c2e0479", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "82dd1d13-31ec-4178-bdb8-90f7af0e5e02": {"doc_hash": "95599cbe5689b250102e480db14744f7269f0853125e1cd2385b34299a2c18cd", "ref_doc_id": "fbbaaeef-d8fa-45cf-8d81-6672450ab87f"}, "87b3dae9-4044-40b8-8e51-f874216dedff": {"doc_hash": "5ec769ac46a4eeaab16a85f16fd2a027fa4ad9f2e8669bc3aaaad8139eca5747", "ref_doc_id": "2598b908-e27c-40e4-bff7-c4323558bdc3"}, "28cf8e56-f160-4ae1-a9bc-4d30601f8a03": {"doc_hash": "172852d4d2078f0916b8f7b6439e5b270bd5b5662ac7907442e81702e4eb73b4", "ref_doc_id": "2598b908-e27c-40e4-bff7-c4323558bdc3"}, "45ad2f8a-1035-409d-ab1f-7aedf845e2f8": {"doc_hash": "0f97d9417c5c0d718aa3e54004a2fd507bf8a042ae395da509b6ca7b7a525b79", "ref_doc_id": "79db41c1-c88e-42fe-bc5c-fd97f83e7883"}, "dd978134-eb9c-4158-8124-35979d4142dc": {"doc_hash": "9de324f2ba95b33e0fe649afbe19adc75a59fd9be60cd75c0ba0636867bceafc", "ref_doc_id": "79db41c1-c88e-42fe-bc5c-fd97f83e7883"}, "172c70e2-921b-464d-ab19-b0bc20b6d52a": {"doc_hash": "69aa8926bba6d5b7fabb12ac054ce4de82756f07d9477ae04fc0e540d7cdc13e", "ref_doc_id": "6470f693-69bb-401e-bcc7-f2affcf98b65"}, "401b98da-ca32-403a-8b62-45e0ca3ad2c2": {"doc_hash": "b3b6d4eaf703392241cc546b5852bcd92159a250f9c3726666a24ab7ff1466d2", "ref_doc_id": "6470f693-69bb-401e-bcc7-f2affcf98b65"}, "ae717b80-b3b7-4386-ba6b-1aca1cb0940d": {"doc_hash": "426d627aa28ffa46c8aa1d0bbcc8a317c992e0a6798315ceb511b0329d6ff41e", "ref_doc_id": "da4f5c87-39e0-44cb-9751-46269af21cca"}, "d772f735-cae7-4a46-bba6-9fd36dbf21db": {"doc_hash": "3dab9ba9caad4ca9f151f5671886cf04faca8b053bb44611a39e4e49b83e8dd8", "ref_doc_id": "da4f5c87-39e0-44cb-9751-46269af21cca"}, "8911a468-1ca2-4e2f-a83b-e31a3a85feac": {"doc_hash": "96584ef9e901e3d93a8bb26bf3bfba1ff2fe3b108b5547216f9641169ce59ed8", "ref_doc_id": "da4f5c87-39e0-44cb-9751-46269af21cca"}, "7fce9724-49f1-4f3e-93c9-264df1f916c1": {"doc_hash": "a402dc7d7ece028369d33bd291d133176bf2b702289400f1fcda3e25b83ab716", "ref_doc_id": "58be4cfa-322c-4520-ae71-69e0820fa62d"}, "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091": {"doc_hash": "d8bd78cf8ebcb373b6bacfd169f5d4ff9d7c2eda9c9227578c61ecb4d3033b80", "ref_doc_id": "58be4cfa-322c-4520-ae71-69e0820fa62d"}, "710b4bbe-d8cd-4096-8833-c9d060166697": {"doc_hash": "2cbc34c3a96ff7c3e381d9952908d4f861e06f329830933774d12819269e445d", "ref_doc_id": "58be4cfa-322c-4520-ae71-69e0820fa62d"}, "1d7f4586-2f4b-4e73-81d4-fe051ff2f232": {"doc_hash": "452eccb0ab22e16a3bdc9c8056cb50b81c50f863906768d88fc71df3dbbf96e7", "ref_doc_id": "57fc2f8a-3851-4edc-af16-e8795cccb141"}, "5f69760d-630f-4f13-a890-2a74867b3693": {"doc_hash": "f4b1d8cc302e27325f45ec91f33aad5b93c548e88a41b7f2a475c703698c5193", "ref_doc_id": "57fc2f8a-3851-4edc-af16-e8795cccb141"}, "9a68087a-5362-42d5-aa61-5c7a7ef32c63": {"doc_hash": "bd4b6a558fea43c36d33acd44b705168d9d4d84913856299a2bcc4db00e5994f", "ref_doc_id": "57fc2f8a-3851-4edc-af16-e8795cccb141"}, "eb0ba9ce-e452-4b71-815f-a1c70b89f786": {"doc_hash": "c188ae31d43437783265c39adb20725f7219824f7f297dc839e87e42bed08425", "ref_doc_id": "0e9f733e-b72d-49cb-8540-9f38f0d8df40"}, "9ae1c1ec-95e2-4a1e-8c2e-66be0db3dce3": {"doc_hash": "c4ec5ee5a84c55c33f78e6868abb2642a2644770f09cc498d7d5d0d2b51c8cec", "ref_doc_id": "1f7c4250-e675-4c6d-9aa8-f29582555fcb"}, "bc09ef96-c917-44f2-8c53-8082890d63ee": {"doc_hash": "45a1ef614715c72134cb31294b1eb1e82570b5f0cb9a136da74de7c6f0de9189", "ref_doc_id": "1f7c4250-e675-4c6d-9aa8-f29582555fcb"}, "3c845544-8fec-48f6-ac89-bfe8c66193a4": {"doc_hash": "e430c0605474edb764e160db9d85975df208c3a45ea62f4764c20aed6c994dea", "ref_doc_id": "294cde77-4760-4611-8af9-5c63d4a9bcb2"}, "dcd3662c-b435-4de0-81ba-9c74ff3c544c": {"doc_hash": "abb366675456774b194b62d6ecbf4ee007ec3b817278d9f76e2a2e47988cccec", "ref_doc_id": "294cde77-4760-4611-8af9-5c63d4a9bcb2"}, "79de1b2d-6fe9-4faf-95e7-048bcd9c48ff": {"doc_hash": "d1ba61d8dd2216bb016f312c9e5d563eb0750e02e80654e26013a7e63d6c2ced", "ref_doc_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2"}, "626ce9ad-357f-4db3-81f9-ef1b581fb00f": {"doc_hash": "fa5f8953e1905768ba442eb539906042e8f982129c44280e0eee3f0a3b183b03", "ref_doc_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2"}, "ff9cca8e-9e69-477f-b397-bfe8ae4b07ad": {"doc_hash": "d9fc8d46b7d6810fc475a618913ec09eba363367af86ecf8455468bd168cf174", "ref_doc_id": "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2"}, "574cf052-3ffe-42f0-a273-abadf79a11d6": {"doc_hash": "8ed5ebc90a5daef1c1331b627b5abd0f89ebfd370ed13b3a82e6ebb94c7286d9", "ref_doc_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62"}, "674e7130-eadf-433e-a5ab-23d28a07124a": {"doc_hash": "37c9cbe9d0de3a3b1324b4f6534ab248461252fa682ca984fa5337992d0ad10a", "ref_doc_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62"}, "330dd30f-e41c-4975-9b41-37f4f6d08565": {"doc_hash": "2de708f3d3378f58d635bf65c2e142088de592aa6b76a6be7bf37a1f76c52fb1", "ref_doc_id": "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62"}, "76309f74-3eac-424d-a104-5596fc3a323c": {"doc_hash": "8c0dc5cf44692db0153d19da75dd0b328a6e8cba0f18d922fc439cae99a2111e", "ref_doc_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40"}, "bdcf656f-a237-495e-92ee-d54f4a6a8542": {"doc_hash": "db97aaa36c056c3c6c9289f196237f097bc58d5f3d445adeebe24f29f673b576", "ref_doc_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40"}, "c366b789-6c1d-4064-8b47-f2eba02196fb": {"doc_hash": "e797681d938b9c477cddaf796fd1c53db4bcb6fff62e1ec912413e45cb149ede", "ref_doc_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40"}, "28447695-b388-4789-a2fc-37104a68e88e": {"doc_hash": "6462a783b39c2f6b18791e05c5a109e6da853f656b3747b7d0c935d83e379349", "ref_doc_id": "e886fe27-d76c-48b4-b13e-b76065cc6b40"}, "1007bb23-7ced-4885-8f14-fcbc7981bee2": {"doc_hash": "b60144b0cdd3cb2dc7be54a325ff1f444ee61d91e05f5c218e7d08e9a1996a4d", "ref_doc_id": "2185a971-8eb2-49e8-8439-c47cba87c9c6"}, "80e9038c-ef38-4952-9a2d-3adffc7475cb": {"doc_hash": "bd3f09fef9bff97f22eff20821d87efca546fb4b6b4e4509490ad209c2fc2f05", "ref_doc_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2"}, "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7": {"doc_hash": "f6daac8a49e4b4c07ef7ae0bd84d66471ef0489f553a9c316b4aa3eeb2b57d9d", "ref_doc_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2"}, "47d2b765-aec9-45aa-a1db-4504f761fb26": {"doc_hash": "9ca101f88f872e0e8536d22c54de501d0543dc63558a9d3d8b6c94be5fa7c430", "ref_doc_id": "a152ad86-b11e-4a30-8b9d-90dbedc61bc2"}, "f7451320-0b38-465e-8641-45e2fc37823c": {"doc_hash": "8be12092bea83574ae978daa262c76f7b7c6f43802e8de9f0f153005ec9d0a66", "ref_doc_id": "0020a1fd-69de-4308-9b30-cbec66818d18"}, "9c1cd556-0520-46ca-a237-933434d54356": {"doc_hash": "7b72b34ecdf71f7ae8acf870481783839e55c887f6bde706135e7390e97fe7c3", "ref_doc_id": "0020a1fd-69de-4308-9b30-cbec66818d18"}, "dd625431-0963-404c-a8d4-83bf8ccf4644": {"doc_hash": "481f2697b3e9c9565b0229d6c37b182afab8755d9f52f8e70d8075019b0ca17d", "ref_doc_id": "0020a1fd-69de-4308-9b30-cbec66818d18"}, "48bb1b04-df50-4bc9-b489-44a689eb3257": {"doc_hash": "676639e775d039ecb0576610d08da01f1c2069f5f454a5f215f2d376e3d147c8", "ref_doc_id": "f4eda8d9-bbc2-4c72-87e3-4c28064eb001"}, "ed7f4367-0b6f-4717-9ece-6a572acb0f0d": {"doc_hash": "a8174277eb3f4bf3a34347062962041ae6ef84fac9c8125d266da62dd2152952", "ref_doc_id": "f4eda8d9-bbc2-4c72-87e3-4c28064eb001"}, "087c61fa-a077-4685-abb8-113ddffd73e8": {"doc_hash": "862fca713f16a53c9aed34fd6e7336237593394db5b0593845623a025f5b9472", "ref_doc_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c"}, "d1c33251-f787-41f9-9c02-75127bc54f68": {"doc_hash": "5d94178c394f0481004f805eeab94a4b3f2c2afa981b9de908dd67e6b9e28e6e", "ref_doc_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c"}, "db7b7e94-8d71-4422-8590-4c32a47334a3": {"doc_hash": "54e4e9a6801b256e09487617f44b90f74b144cfb7673a5694fe5505c6733e19d", "ref_doc_id": "a2ebdd34-a194-42d4-8a89-efda343ed26c"}, "b992d9ed-345b-4c22-861a-255f27489f43": {"doc_hash": "fc6ca1b93e1d2d5485e6c3ff7fd209281d47c31a47aa053e82dec8d756da7767", "ref_doc_id": "5a233e33-461e-4390-be11-fc954f157193"}, "d97c231b-cb31-41f5-b4d7-f162d1a5c684": {"doc_hash": "9a21717130dbcb3c5cc7380561a6884571cfe69cb276d4171074b8e44e64a6c3", "ref_doc_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204"}, "5bc9ae56-de19-4255-b364-881a666e963a": {"doc_hash": "38c3ff95ce02af54355f7db32ed01901807765d346e2fb8b300da611116c3bc8", "ref_doc_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204"}, "7244ecde-0cf7-43c8-87b0-93fc1fd148cb": {"doc_hash": "f7efc4c03c6b86de9a6cfebe85f2c9789cb28cce41a8aed825c3447c84ea28ac", "ref_doc_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204"}, "d2de6354-44ed-4350-9588-2174cab589fe": {"doc_hash": "e97ffd5439a9795e55bc31e8a06819933887e32e87b22847d875ff41b6fc1842", "ref_doc_id": "de25a3d6-0a4c-47f7-9195-579f23ec0204"}, "55cc781a-d5d2-417f-bb03-c381d3c5bc7e": {"doc_hash": "c0623f6418850422145c41ee52c6d081d3bf1c607beff5eaa4564990d05a9422", "ref_doc_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b"}, "bdea54e1-4276-452f-9d2d-7ad83bc90991": {"doc_hash": "6a428076006d708a3fc732a165f84fea9c9997b9225a523c60e34756759bf832", "ref_doc_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b"}, "22954da0-96af-4adf-97b2-1b84cfa13e0b": {"doc_hash": "0572c67a2e3617e808faa57b2f8281e324acec1ae3f7b60877699e005b2c97d9", "ref_doc_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b"}, "087f112a-772c-471a-83f6-b573310013b4": {"doc_hash": "cdb9fffff9b1e6d88ef6a3b533d80d7bd9ee0c3f0c18e54bf25c3b9456aa374a", "ref_doc_id": "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b"}, "826d7428-1112-4d56-ab37-f04d8b9a4604": {"doc_hash": "ef4dce0b5a63a48581253a6e138ebf1c47612e1f39d8238ffcd70e2c3bb59f7c", "ref_doc_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada"}, "f7c758b2-fc01-4e62-926a-209f35c25703": {"doc_hash": "0cfa0ed23013cec4f4a02ba25c8c7ea594d9971edc467f19fd17d55ef7b589e1", "ref_doc_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada"}, "f42976db-5741-44ea-a05a-ea0af7fcaaa1": {"doc_hash": "bfc20398395289832a4a6d15e58e6d544850ef25ab026d5bda85ce0c6cb0377a", "ref_doc_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada"}, "1203057c-eef4-4951-b51f-9c02823121e6": {"doc_hash": "852bb42ced29a6b60d773cb5899220d0afdea8196887aea4f8dba3f5ffbbaac2", "ref_doc_id": "200cf1ce-c920-4d20-9410-3beaf5cf7ada"}, "59f40e0c-bad6-43bf-9686-e4cb6bebca32": {"doc_hash": "87acd4da59f92cc9cd639786c7c9de5b83c7461df763971bd731eade5bd7eff2", "ref_doc_id": "d49ea179-3a65-4890-85ec-6c790399b666"}, "f6c66802-ae28-4892-a195-8504f27703b2": {"doc_hash": "4fde0839cfec327014b6bb998ceccf8d7e576da6134f301fc3fb2c657705e235", "ref_doc_id": "d49ea179-3a65-4890-85ec-6c790399b666"}}, "docstore/ref_doc_info": {"8cf15bf6-16e4-42bc-a154-8fda27dfabd0": {"node_ids": ["b144d770-ed66-439d-b796-339942c740a9", "883ba310-18f0-4fcb-ba31-17848a93338d", "21cbb6ed-b70d-4eb7-a96e-eb8b93f26f66", "90cd2df7-b681-4b99-9a45-c4bbb01d4ade"], "metadata": {"page_label": "1", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "33c7ae4f-5536-40c8-a2ed-03b3f06ba98c": {"node_ids": ["563d970a-c800-4cee-8285-490e75ba2b8c", "3f418125-d917-46db-9f20-6335ed7f06cb", "bf043cec-f9e4-4b0a-9293-9852b27bb969", "46620723-f0be-43c2-86b4-f9b00894b230", "2a103c75-ea57-4da0-a080-5de28ef59e30"], "metadata": {"page_label": "2", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "eeeb9665-8572-464a-9fff-49286047c93b": {"node_ids": ["50f0260c-d4e9-44ea-8ff3-275f316b76c1", "640874fe-676f-47b9-a525-d40b98e5972e", "bdf4c6fa-3009-4557-b35f-e788dc2fda5c"], "metadata": {"page_label": "3", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "d9081159-e2a2-43a8-8d74-bc98a1c01c8e": {"node_ids": ["5968fd51-f382-415e-bbb9-7e89100f6d69", "0c71b7b1-6f3a-465b-b5b3-3c286aa984fd", "2ec7195e-73f2-4b0f-babb-6b01d8b7f778", "bd8f6c67-72ca-4be7-9b8f-aa201aac4fa1", "ec59e1cd-2b6e-4d97-986c-660e3fdd1b21"], "metadata": {"page_label": "4", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "a24fef4f-4cfc-4990-ba22-4d91a88365b9": {"node_ids": ["7f20c27c-c94e-4d83-8c72-fc616794e9cf", "39608719-deac-44a6-b356-183e421e9b0b", "aa35cd7e-af44-4ddd-94ea-99718de1030a", "17637a6d-05f0-4e0d-ab34-71d637a33a80", "b8eba80f-75db-4fc6-877e-3683541d81c9"], "metadata": {"page_label": "5", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "33a643a7-0fc6-4c46-9454-852dec0fb9a1": {"node_ids": ["bdfa5eb3-e20b-4f60-bc25-a1a696e33700", "a8cd5c3b-c4e5-429f-b2fb-2de12f04d58f", "acf28545-6e3d-481f-9a02-b526848e8e86", "62a82209-0918-4048-b6a1-e01a25bf2b9e", "1123bc84-d94e-4869-9b2b-f784db425726"], "metadata": {"page_label": "6", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "9e56f4e8-b59d-4e45-9b57-442a21f73e47": {"node_ids": ["1d99ce8e-a37c-472d-bf41-5d546e0f16ee", "b23cd4da-dbaa-48a0-9916-63163abe75df", "ee7bda4b-062b-4789-80c6-ec392050980d"], "metadata": {"page_label": "7", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "f8d01ed6-342b-427b-a352-ec7ff615d512": {"node_ids": ["f4c79ce4-b09c-4e94-899f-95d7e148a2e1", "42c3a271-0203-4a60-8301-c4d87ff68d46", "42d6bbd6-bdd6-492a-b4f7-5ee16f5a67ed", "9b6a9438-8faf-4fac-aa12-af91063209fa"], "metadata": {"page_label": "8", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "4eb2ac60-246c-4c71-b26c-92f3bda6f5df": {"node_ids": ["b7762844-4e14-4c58-9088-788b80895a9b", "68969e8e-6781-420b-a298-b93aa173c0d8", "4950bbbb-4301-46e9-8575-5c98ac072755", "6d0f2cc9-191e-4aaa-a1b7-9642378d47b0"], "metadata": {"page_label": "9", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "da764545-8a00-4201-b4f9-35dbc73b4625": {"node_ids": ["226c4cea-b963-4017-be6c-2e7735506f37", "c8260ab2-ae39-42ca-a090-6010130e29a3", "bb93f585-0c62-458c-972c-05a15b191fbb", "fd242c0c-cebc-413d-8413-0aa0bdf0fd4c"], "metadata": {"page_label": "10", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "52c69155-3db6-4504-b556-e7fde92a6d72": {"node_ids": ["70beb800-2972-4a76-8389-d09dace825d2", "3a6cfc93-1d0c-4914-862c-09d18094083b", "6d0a6db2-8807-45bd-b44c-7d3d7d13c5c9", "5b2725fb-0a74-412c-9959-823022bea0e0", "821224bf-2c56-44e7-b0c7-042a2d75eb03"], "metadata": {"page_label": "11", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "075a0632-ff4b-4e55-aec2-8f71da87ecf2": {"node_ids": ["a4b79a84-0951-4e38-bcc6-18f662ab3a1f", "df957665-53aa-4210-ba27-eb61f5709a8e", "ee18e0dd-9ac7-42a9-a1a2-438dfb8898f9", "526f086a-62ef-46a9-8769-d4815ee67a1c"], "metadata": {"page_label": "12", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "979a756d-2dac-425d-a204-36f18bae9b77": {"node_ids": ["1ff55443-bee6-42ab-aff4-584f69287f0b", "2105de9b-9a8e-4186-8d5a-932beb3357fe", "79374460-d5be-45d3-a4be-49c1d7f87947", "b48293f4-10e3-4010-8f15-423abe7d7c52"], "metadata": {"page_label": "13", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "e4974921-c777-4851-84ed-8b31fc1ffe9e": {"node_ids": ["a28eceeb-84f8-4caa-baeb-e0fbd0e31426", "55a1ba0e-ad4b-4b35-a6cb-dea02b9dda2d", "ce655fd1-14cd-4e11-95d3-dbd468f75e6f", "8c2e9772-20ef-4633-a491-d408e31432a7", "2fb9b60d-0823-4ba1-93a1-37eb87c5b219"], "metadata": {"page_label": "14", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "d267b4a6-a619-43f6-9f4e-62443a87df40": {"node_ids": ["1718dcb1-58f8-47c9-8738-895516d5a876", "91351451-a9c2-42fc-bfb3-3113e0a95206", "9f8ecb0c-85fd-43f2-a14e-0793b5e04e55", "992a0bab-e1f5-4230-a260-245648f5eaa2", "da39d666-015b-46a9-8c65-c34f3c9974e3", "b5813ad5-dff4-4b63-b678-ec76a140bc69"], "metadata": {"page_label": "15", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "a94769bc-1887-45fc-8edf-7aa7eb4cb997": {"node_ids": ["32d6da10-c900-4077-9625-b693c141f1c0", "645373d0-5558-48da-bf73-fcafbe968c49", "16f7b813-12f9-4d37-9dd5-09130ad56d7a"], "metadata": {"page_label": "16", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "7ea8c924-e836-47ad-819f-823f299b261a": {"node_ids": ["b6700605-08f4-4593-bc3a-1f329edff848", "51eaa66f-bd60-43ee-aa38-33eee4f5ce3f", "ad6e0876-ee47-4e7b-b868-4da8672be7bc"], "metadata": {"page_label": "17", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "f052b318-2007-4f6f-a256-64edb03f91af": {"node_ids": ["33deac83-22b5-4747-9255-c26f1c3883d4", "3ed1bf4e-f42c-4f91-978e-bdfa9b47afad", "84af7591-983f-44dc-a9cd-f773421acc3a"], "metadata": {"page_label": "18", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "add561e2-28e8-4222-b1c7-fdcd671a478d": {"node_ids": ["b7d4a743-80c4-477e-a31b-3a3e8cf51375", "6fab223a-44f3-4dee-91ba-0fcbe2f1c064", "5d76ef4c-ffb6-4e12-9769-f662afccc150"], "metadata": {"page_label": "19", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "49062df5-b35f-400a-8a24-2525db09c4b6": {"node_ids": ["249c1cb5-ce1a-47f5-9b25-ae9d70db3b8e", "02c0e807-e1f7-44bc-8abe-c5a3e9c23c95", "f93f969f-a5d9-4b14-9240-844f9273cc90"], "metadata": {"page_label": "20", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "8939eb83-bf35-46dd-aecc-7f2cd19055a7": {"node_ids": ["0d1c3c8d-db94-4efb-96e3-b9daa708e381", "8c55e45f-c308-4e3e-a815-8dfdefd545e3"], "metadata": {"page_label": "21", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "6ef7c7ec-41fb-4b82-a752-27d76800397a": {"node_ids": ["c7b7ee55-485f-4153-8c07-d087c3dd8ae6", "0f0725b7-fe5b-4ef7-9d7f-3af4d2698922"], "metadata": {"page_label": "22", "file_name": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf", "title": "ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE", "year": 2021, "author": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"}}, "0a6b0105-950e-4b44-aaf5-a0b68979032f": {"node_ids": ["0e96580f-5280-490c-b25d-a7b8a0ad729c", "c3c500d8-77b7-464e-a519-63ce7deeea99"], "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "086534ac-7fe1-4abd-bec4-e7ef382f05ac": {"node_ids": ["f748a6bd-a7e1-430b-8e44-8d0cd209d4ff", "fc955a05-8a63-4ae5-acde-f5d051e095e8", "19cf97cb-3a42-457c-a236-fc566a281e77"], "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "9ac5ff26-e88b-4026-bb66-af26e868e593": {"node_ids": ["83e7eb31-e0e6-47f7-97d8-329e33338fb2"], "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "d458d2e4-ec19-4d8e-b9c7-bed557725715": {"node_ids": ["b2ded5eb-b0ed-4570-9f2b-3758b13759d0", "8a4f80a5-a997-49dc-a560-971e91331c9f"], "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "8bcf4367-5daa-402b-a021-a2d62db1e77f": {"node_ids": ["b432482c-efaa-4033-b476-4d479d475f37", "38a3ec1e-955f-4fd1-8c84-43542d8587c5", "4593f3cd-f137-4e14-be49-58babd346067"], "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "340da147-cbec-44de-8d6a-408061fab7ad": {"node_ids": ["2f7d4712-1fb1-4b9d-91b9-cd9c2e1d50b3", "5164f84f-80c3-4388-a6ee-f0519fad1a12", "19e54b55-db0e-46b6-b903-546fe9fd4da7"], "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "74a2e74e-aa84-428a-9941-d78449b36773": {"node_ids": ["822f888f-2a76-4aea-a5fd-8f8b27a7291d", "dced52d2-77e2-46e9-a450-6244526fd64e", "8e0e2811-3e94-41e1-8fde-9621c5ccf686"], "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "a7b3b789-1ebb-4638-8289-dcf12f564a31": {"node_ids": ["c6622e9a-e276-46fe-b628-f23144968c34", "377a72bb-a912-4dcf-af45-63f847084974", "de4d97c3-46e1-4c23-9d05-3a2ec58c4835"], "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "276b1538-49ed-41a0-9fc5-063d958eb2bb": {"node_ids": ["fa58536d-cbc8-4ed0-b34d-00fec279b4b3", "d422cf13-cb86-4873-be7f-abdf32b0d248", "f7f43918-6198-4332-8e11-6505ba0c01ff", "166fd453-7e7a-4e19-b82a-d104916c8632"], "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "d720be88-3f77-4bed-9860-17b2bf8a85b8": {"node_ids": ["4f084646-c2dc-4eec-bec3-9376ecdb8965", "f08aefde-f665-47fc-b1bb-3dc532fcb72e", "0ed194c9-0211-4aaa-97cf-6c86afdfec79"], "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "8520c4e3-0df9-429a-b4aa-202e06661a15": {"node_ids": ["54cf1130-e1bc-46db-85fc-305e4431a37d", "edd1b2ea-826b-4cd0-86e1-77b5c6d0fe25", "c6f30924-ab67-4032-af15-9018fcbab6e7", "92880e25-c799-44c2-8141-4decf2bb4ec1"], "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "5df396e7-e704-4f2c-87cd-26f26836bc55": {"node_ids": ["8afe343c-7a2d-4db7-a179-886423a7dfb4", "3db08707-3250-4d28-ba7d-ccb5b5743e3b", "691090d0-5c1e-4f05-80b8-30d5f150ba33"], "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "d562e8c9-4a52-44d4-a202-22643c69e2c8": {"node_ids": ["1161c14e-cfeb-49a9-b8c7-791cd978bac1"], "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "a35f71d0-6f82-47da-b623-7dc45d888860": {"node_ids": ["87054fef-106c-4066-974c-7772c83383e6"], "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "86552836-947b-40c4-855b-5105b35d0ee5": {"node_ids": ["ee4efd4c-4b1f-43cd-b853-9ba0b0b08ee4"], "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "title": "Attention Is All You Need", "year": 2017, "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Attention Is All You Need.pdf"}}, "560c2293-8732-4956-98bf-c39c108e1d95": {"node_ids": ["03d47bd3-bc2d-4290-988f-65337cc71d9b", "79ef22a0-af76-4a0c-99c4-4b462168df45", "22d870cb-f76f-4da8-ba44-e1d0cbdbcadf", "14982d34-ffc2-422b-8652-ad1d9ae7fec8"], "metadata": {"page_label": "1", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}}, "8593f4f6-e202-433e-9fa7-4e12dd416f3e": {"node_ids": ["488dad6f-7414-45a9-b3fc-a8e9c9f796dd", "72f12176-914c-4620-861c-678d50d5bd8b", "7944edc1-9968-4708-9e7f-1ff39f28f721", "1d1a925d-c4f5-41cb-8bbd-aacd58b25818", "779702f8-33b3-4388-86db-e6877a5317b1"], "metadata": {"page_label": "2", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}}, "197846d7-4db7-44b6-9e92-1cf00a44b087": {"node_ids": ["03e7f862-198e-4a96-9756-b97865f55d6c", "cc1e4370-19eb-488c-9a08-b2c72a1ba9e4", "3dd01c88-c64f-444a-b5dc-12d186420111", "42c7e353-4d6a-4a6d-a182-3b52b822da15", "c6f1618d-e6a6-4b55-8de9-8b66eff2f39d"], "metadata": {"page_label": "3", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}}, "7f4ff029-1ddb-4cc8-a1a2-6233b68b074e": {"node_ids": ["7f15dc0a-8f9a-49be-855c-7bbf52cc3223", "d2108054-4a74-44fc-b9fd-03f8fa286714", "c984be96-b580-4e49-8207-c0667df3776d", "dbd307c5-db4f-48f8-99a3-47f6d2f352ce", "d2bd2716-baf2-452a-8b2e-e532bd37be75"], "metadata": {"page_label": "4", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}}, "7801eaca-c0be-4680-aa81-1dbe51de1953": {"node_ids": ["e5345077-fc58-4dac-84ba-1db24974d231", "421e01ee-f83e-4c19-baa5-d6536a975fa4", "3622b161-5bff-4dbe-9aa1-e654e49c6d68", "5fa7b669-b5e7-4f4f-9824-9950ae479e35", "ab875183-f1f9-48c8-a7ac-6efc7e4784cd", "93a2b0eb-cb7c-4e50-b90b-824f2078aca0"], "metadata": {"page_label": "5", "file_name": "Audio Transformer.pdf", "title": "AST: Audio Spectrogram Transformer", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Audio Transformer.pdf"}}, "00fdc6b6-ad27-4177-a9d6-5cdf9f828c33": {"node_ids": ["8d1f3c3e-a6e0-4591-b81f-81c4814757d5", "559a9f55-92bc-4642-ba38-e817686679c4", "9c3685b0-088c-4656-893a-ff055d16a65a", "468c9229-1745-4f6d-8751-3ebae9f8e381"], "metadata": {"page_label": "1", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}}, "2d8af6a1-95fc-4aad-bfee-6351b71c1e57": {"node_ids": ["3f18fd33-ee73-4341-a086-5b65bb46f184", "aefdcf9f-9336-4ce3-8551-80018fcb9a85", "144e4d07-b1bc-401c-a262-a0e032f8466d", "1204fafc-0e56-4ddd-b01c-38b5ca1552e7", "faaf2b13-e4ca-40d2-bcfd-2a20b5b8a7c9"], "metadata": {"page_label": "2", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}}, "4004513e-fcf3-4219-a1e5-bca570eeee2b": {"node_ids": ["289b1b8d-bb5c-4939-b900-f23057836e1b", "c007a5d4-e5c2-4726-b3c2-a256dc9a33e1", "4cf6a737-620f-4c2b-9f6a-54606f2e766c", "8f877bc0-942a-4367-8f6d-1d17c9e5cfb5"], "metadata": {"page_label": "3", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}}, "960533bc-fe8f-4224-aff1-73ed9c8ff214": {"node_ids": ["336fae26-fc57-4f09-9c89-1dd00909e1bc", "2808721c-1d34-40d9-b8bc-9c4d98524c7c", "cc0bddbe-995a-44cf-a692-7544bf625727", "94769752-8058-40db-a40a-ce16cc007f73"], "metadata": {"page_label": "4", "file_name": "CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf", "title": "Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection", "year": 2020, "author": ["Koichi Miyazaki", "Tatsuya Komatsu", "Tomoki Hayashi", "Shinji Watanabe", "Tomoki Toda", "Kazuya Takeda"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf"}}, "83c18eab-5164-4adf-aca1-61f6095f0c62": {"node_ids": ["ecc2d542-15ad-42b7-a487-900c176a3fa6", "27c3b9d3-fc72-4590-aee2-949877aa7fd6", "2253a3e4-3550-4eb3-835a-3d6fdde8c56c", "35b7f3e9-ec82-4ebc-940f-cf924406685b"], "metadata": {"page_label": "1", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}}, "f6a53155-9852-451a-bb15-24cf4011bc7b": {"node_ids": ["d4655479-c6ba-4dec-b851-071409dc19b1", "aa59ac24-3d7a-4655-93c8-c8572df02294", "efd60397-3e10-40e5-aa1f-8742400f0663", "4519b263-4996-4fc9-ad1d-f08c81f36040", "2d6e3e63-f4a8-43bc-9ede-509ba6c18c5b"], "metadata": {"page_label": "2", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}}, "89551c77-5741-478a-9a15-735f30a541fc": {"node_ids": ["16d83b02-71da-4ed6-9dc9-74a9ec2c40cc", "65d018d0-fa5d-4753-84a5-780cb3c538d2", "f6c6b166-d86e-43ba-b91d-6a9fe8532091", "3a80d112-7b87-4483-9783-b6f9ccb97aa9", "694162d9-2657-4d4d-b452-537cee00910b", "28a85989-adde-41dc-a6f6-f56b5cec086b"], "metadata": {"page_label": "3", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}}, "9d36ed80-29b9-48b4-a6c3-a60aad1badb9": {"node_ids": ["7c75251b-8254-4b05-9c45-e0be7f499e3f", "91f8b6fa-e02a-4ae0-a98f-50f277ff5a8c", "46a8e9be-2a77-47fd-8bea-66b1e482ac65", "3159b1cb-85a0-4df1-8173-f631dc6c973b", "ff40bcdf-f8f0-401c-a40e-b747052e70a4"], "metadata": {"page_label": "4", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}}, "71ffeb29-b789-4206-a34a-ecfa254d1f68": {"node_ids": ["e8e9092e-4033-4719-ab14-c692f2b058da", "48988c3c-e1a3-4f36-88a8-148c7a38b1ee", "2b9422a1-b351-4f1e-a0c1-efbc92b95863", "95c9a21b-cd4a-45be-8bbb-74a1bff406fd", "8e14479e-ebe6-425a-b43d-c1aacfd7f8c4", "ebd80897-b5dc-40c1-ab95-02f98c9c38ba", "c6ccb94b-8de7-428a-9c60-89c0e5750933", "a8c9d156-629a-4a51-8509-77a6bea84425", "f802c401-9854-445c-850c-66fe04a100d9"], "metadata": {"page_label": "5", "file_name": "Conformer: Convolution-augmented Transformer for Speech Recognition.pdf", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "year": 2020, "author": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf"}}, "98bd6505-291a-43f8-849d-54681f25a6e6": {"node_ids": ["9f4eb994-a455-4c67-a3f1-5c51af04d89d", "9eef1426-56c8-4d4e-9fba-6d5074a0e25c", "ef357faf-228a-4caf-8007-a9c13493ca66", "aa6268c5-875a-4fca-8150-1c428685b75c", "2e653e99-88c9-4408-b338-299410edd28b"], "metadata": {"page_label": "1", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "f62de16f-a39c-4eab-b47a-47fd88fdea35": {"node_ids": ["1d775d94-255e-433e-abc4-464d8961a836", "423b3ae5-ac1f-4768-b51a-631a1c7a3998", "ceec2a36-a3dc-4f8d-896e-8d969b1ad6e1", "cdaa5e24-5611-471c-9ecc-c992d2cc4bd6", "f47292db-3a95-41f0-9573-b92ee8f6ab9d", "7a05b865-dd80-4aa7-9408-a5e39688dea3"], "metadata": {"page_label": "2", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "f851d517-262c-4867-8fb6-6ccc065013c4": {"node_ids": ["e0081f9f-b637-42fa-8290-b70cf952a28d", "7abea2c3-fd8f-4a10-88f6-879cc97ac304", "92ccba74-3c14-4b84-bc2c-750d9f3ce26f", "5e868248-f7e2-4738-a625-77cbee2f64ca", "4cb26f0a-540d-44ce-8f9f-e44bfc5b066b", "d40ca56c-518e-43c8-b72a-7ec5916015a2"], "metadata": {"page_label": "3", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "a8e9b03c-a557-4618-9aea-c8b50668052e": {"node_ids": ["eff43ae2-06e9-446b-9f43-6da5134dda8d", "cb52c700-f4f3-453d-b2a0-0d15b88e668c", "005375bc-2857-416e-bc4d-3fb74f38ecfc", "d989f715-6a45-4fbe-b76a-f9db2cae5edc"], "metadata": {"page_label": "4", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "997f1d27-2d1c-4520-a22a-3c94805b3f19": {"node_ids": ["77ae92bc-aa0f-4d7f-b84b-7e8b857b8850", "2bd4294f-5f13-48d1-ac97-3f9c21aa80f2", "cbc893f8-6257-47c9-814c-742e3dd11bbd", "92f81b41-7398-47ab-b21c-9a2ba02950eb", "600a60da-6553-4936-92c4-9d515c550c99", "546b58de-8f1c-4e2c-997d-db783d9c972d"], "metadata": {"page_label": "5", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "138d5f78-d06f-49f4-a597-16bf4b6da2ff": {"node_ids": ["5db35dee-7104-4d4d-916f-21047cc962f5", "89915979-a4a9-43cb-b367-1a7e00521701", "0bfc2d8b-f988-4e3d-a878-9c0ef282f0b0", "bb33725a-8637-49b0-8f18-d9bd8abce73b", "d1f74d25-4c5d-4e2b-b38c-2e1b661e55bc", "0bf48078-f2ea-493c-af47-45e803f76b5b"], "metadata": {"page_label": "6", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "cd15bd1e-4414-4d65-bf59-5744ddfc059f": {"node_ids": ["9189a21b-108c-4167-bc7b-a172abf40ac2", "8fd3458f-f365-4f58-b487-e3e73c098fda", "546859c1-ef43-4741-beed-59ab66a07fb9", "e9ac1a4b-4fc6-4a89-8a15-fc7ca728dbaf", "db5318e8-ab2d-413f-98dc-2ef577a257fa", "5540dae0-4f31-4352-98a8-6767675b0fcb"], "metadata": {"page_label": "7", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "e0de7cf6-791c-4d84-9de8-27afc3279cc0": {"node_ids": ["ef9f03bc-8255-41cc-a32a-3c0e401d89a2", "e058be7d-e2d2-494e-b923-de8e15b59f8b", "b7e14b8b-c189-47ea-a7f5-7b54899af439", "fe61e543-fef9-4dd6-9de7-75eb4c865356", "d727d551-a1cc-4ed1-8997-0ab9b621c36f"], "metadata": {"page_label": "8", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "8447d82e-654b-4e6c-80ca-5a378f47741f": {"node_ids": ["5fe3f32a-fdd9-4270-8d1c-ae924f51db4a", "31e29c65-2c1f-4769-8b50-c4b6b0d46aab", "e68ee0c1-1035-4562-acff-2160207b1516", "bacc7868-af4b-44eb-ac46-c5af82b6b737", "0f5319cc-f3be-406b-8958-2da24b49901e"], "metadata": {"page_label": "9", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "b0687058-84d4-4857-b828-363bf10890f0": {"node_ids": ["bfde838b-685c-48fc-aa71-c4724cc8100e", "2b4d2cda-c0dc-4534-8c4c-e403eed972f7", "9c509ca0-3d15-4e9a-a841-405b12481f27", "aa77525c-9b00-40ac-baa5-75cf1e19c06d"], "metadata": {"page_label": "10", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "55da47e4-2bb3-4edb-87e6-416a6558ae2c": {"node_ids": ["cbfad1ef-ac41-407e-ae49-9784c666b863", "907c69ce-8a05-4dc3-b4f9-26d5e52711a2", "2e93c8fe-2a72-4f2e-a9ae-a0216c0addf7", "94e8b838-d3f7-425e-8b15-ce1ebae63303", "34462f83-2d9c-4a7f-9e74-3b5034a859e7"], "metadata": {"page_label": "11", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "f417e3b0-2b90-455b-9282-3c9889c9ff03": {"node_ids": ["f9c7e71c-e04a-4a37-b9f8-30657480b6cb", "4fa862e4-4fc5-46e1-9336-c2984bdc796b", "c8e94427-6955-4529-a647-7c09b51e5b45", "a254f9c4-ad73-4a09-a9c4-78285cfabf49", "cce72ca5-1488-4d6f-8938-ad15d28efedd", "1165a9b2-3fd8-476f-a275-1301add68a23"], "metadata": {"page_label": "12", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "38f32560-9f7c-47ae-8ec8-39342886a365": {"node_ids": ["f0f2b242-14ac-45f6-9834-6dfb33cd516c", "9b6a7402-536e-4512-b48a-f78e2f3833ba", "1d211d9f-40b8-4995-b008-79ce8889a88f", "67b07245-49bb-44ab-984c-26e2cd9f2b11", "28723e00-7566-4c40-837b-15ff158ca753", "23d4bff9-886a-477b-84ca-c08f40d4ea38", "5d33f7ca-6300-427a-b774-6290ee326e43", "8e6369ec-83ab-41fb-86ed-df86cf2a44d2", "dbdedb5d-830d-41d6-901c-9bf52167b7bd"], "metadata": {"page_label": "13", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "dbd5e04f-010b-467f-a5c5-0c45dd0a98c1": {"node_ids": ["81ca14b4-4c57-4797-a6c9-406349cde7b9", "146b5908-3a87-4228-a108-6d7576a8a9d0", "fb28101b-1754-4bbd-a0c8-ef34cd45e6af", "6dac5908-de69-45eb-b98d-0dc1b198c1ad", "ad128772-d46e-4256-bc31-81ad71f51ce8", "f6e9a70e-6a72-401e-bf1f-9129b76ea142", "ab501c3c-685e-4dec-8fbb-a834bd4f5e73", "c7792e5c-1b7b-4831-8bf6-9817c9338395", "d7a6bb0b-3ed8-4597-a07d-b7a1e6a303e3"], "metadata": {"page_label": "14", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "59eef68f-016b-4d5d-87fc-59674048a3c2": {"node_ids": ["5daf3360-184e-4a38-a691-509fb35e860d", "2da1461d-d860-4f75-9bf8-18b20f510508", "234cc7d5-1ac0-4b39-8ee3-48a3fd76e406", "9ab17625-df39-4728-9b21-0b82470b49d0"], "metadata": {"page_label": "15", "file_name": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "year": 2020, "author": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf"}}, "dccd0a97-5ed5-4b6c-8f7e-85eea5316345": {"node_ids": ["fabd56dc-5425-42f3-8da3-4c08ee4e9107", "08317b88-fcf7-47cf-8bde-5a4a6c23705e", "050258ec-7ae1-45ef-a38f-a7d3449c215f", "2ef40cf7-9c5c-470e-ba37-4d926b72ca18", "7cbae6f0-59b3-4166-9d9d-7846cb093a6e"], "metadata": {"page_label": "1", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "536708b9-e556-4812-9f4d-6d823e99f4e8": {"node_ids": ["9624ed82-27b2-4111-b2d2-48e4cabc77ad", "bf768e2f-d631-4843-b4b7-b7d61cd91fdc", "399fda60-8bba-4abb-a099-0a96bf6bb0d0", "60d14978-751a-4035-a64b-2c35a8774b7d", "cac01f70-5501-4a11-bbd1-0c911cc47565"], "metadata": {"page_label": "2", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "1a2b408b-98a6-4aa2-b477-cbf898b52800": {"node_ids": ["ae2475ef-2e4f-442e-8df5-f88ca66ea1cc", "d6eeea9b-7c28-44e1-8579-329a78e4526d", "d18b6f39-ebcb-4b1a-9244-2bf98fdf3f22", "c7b12349-9fd0-41e8-80a6-44cc23a24014", "988c203a-2456-4b32-84f7-9d3bc5f921b1"], "metadata": {"page_label": "3", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "a1d704bc-cb06-4f3d-a8d0-46a3302071a7": {"node_ids": ["17b5b257-855f-4d5b-99fa-f52187320cf6", "bb89fdcd-438c-4fa5-929d-653fb15dabfa", "aeea271d-1dae-4742-9e69-2be174351555", "235d8915-3e9e-4c17-b1b7-94313b57761a"], "metadata": {"page_label": "4", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "5d06a0c8-ab5f-4030-8df7-d73bbe7c77ce": {"node_ids": ["02c1c66e-2e62-4d83-b26f-3a6ad431f674", "32b1f121-e3e5-447a-bbea-88dc8eb1b014", "2cfed885-3663-4885-af61-7ea22736305b", "8043580b-588a-4bd9-9544-a7250011982b"], "metadata": {"page_label": "5", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "e75a4dfc-1838-4ca2-8a76-02b5fb8f8dff": {"node_ids": ["b9cfc59a-74d1-424d-8322-082dde4b290e", "4e3853e2-4806-4eb7-8120-cf5e0d4c903a", "d691b1da-2e8b-4f4d-88a5-bd40ee7c2225", "f6bd5b5f-60a4-46a8-9f73-b26fd8880699"], "metadata": {"page_label": "6", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "87dd0fda-7246-4d8b-bfee-7a48b86a1799": {"node_ids": ["aa9e3d52-33cf-411b-a030-5b763d54f6b3", "ed8f5ce9-293d-476b-b54e-4e72eac78dff", "45a48615-a087-4685-a35e-1ea3621b3a73", "d0b8f408-5af4-42c5-9311-a7dd34f519c6", "829d0d83-dab0-43c9-8887-8059825939e3"], "metadata": {"page_label": "7", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "02c8d105-1fab-4d81-bdc6-33dbf70a111b": {"node_ids": ["d2f93468-b715-4847-a9af-24e305b43e4e", "6524f901-c391-4bd3-82a2-9afe1de48fb5", "0af99402-5274-4862-aaa7-7de365929725", "193b9ae3-3431-43d5-880d-09d2ef130c82", "1b115cef-dbf4-4b4c-9a2a-fe76f38894f5", "25d71e38-5247-4d10-bf4b-5afa4f0a1656"], "metadata": {"page_label": "8", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "ced28a68-a0fb-4ddc-a6ed-ef93faaae29e": {"node_ids": ["ea0f125e-91af-4ca3-a8c4-ce765d19dcb0", "5e3c3c5d-5d5f-4d2b-a6e5-e59b85712314", "2129011b-7d24-403b-baa1-b4b26de12f0f", "e70d9cdb-c1ca-4814-acd9-60a404b92b30", "4e4f0a5d-cdc0-4d2e-aba7-653f1013c0a6"], "metadata": {"page_label": "9", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "d994ee85-dcc9-4a29-8b21-3cc141cbe8db": {"node_ids": ["97ac92bf-f69e-4d9e-bbe7-e0db72980337", "a27d1161-6737-4ac4-b6fa-44e2f3b31db1", "b349b525-3127-4814-a68a-088b10e9ad63", "423c2908-9082-4753-850c-d644dfd7af54", "d26239fa-4f54-4bd7-88c0-cb9c1000a81a"], "metadata": {"page_label": "10", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "6af92d4e-e412-43b7-923a-4e2ce0037ac6": {"node_ids": ["cc0d8d1d-5d9a-4af7-ab29-a8fde5c3837a", "17b8883c-b686-419e-8d8e-0cbd44d4f27f", "40a10066-c6ab-40d2-b2c1-0434d256200e", "3594f721-d576-4d64-8f2e-589ef8f59690", "e05d602c-8c2d-476c-b180-cd06b58596af", "cc254142-af5e-4ceb-8172-6157fa17e2cf"], "metadata": {"page_label": "11", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "3356041c-4d5c-4a10-b963-e3f4ff36da5d": {"node_ids": ["e7e541e3-7ee6-441d-b24b-a3e8048f6384", "b97ba877-7824-4acc-8f9f-465e1472ea18", "76ab8edb-6470-4688-99d5-833556bef64a", "b5833bc4-80b0-4710-b1fb-16a5da2cd633", "62b74a8b-cd70-4c50-b298-223e5a6881c3"], "metadata": {"page_label": "12", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "24efc6a7-13e0-4628-81e0-df1cc32f050c": {"node_ids": ["ee0f177b-9942-404f-8ed7-6758c5a5dfea", "6b0cce89-d72f-4e82-9d1a-9535ddbd0822", "e6769ff1-0f97-4709-9dda-4afd6a1f094c", "033463eb-6bfa-43b5-906c-ad1fb49c4b3f", "d94f2189-770d-4665-827b-3735f71a4d50", "6efd1c36-a551-4874-984b-a8a83ec83db7"], "metadata": {"page_label": "13", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "95385bde-ed41-4a53-962b-559aeedd3586": {"node_ids": ["de2b97ff-76ca-4418-aca7-9d06049ab391", "70289da0-b495-4391-bd6f-5c7aa86b6217", "14f089fa-3fb0-40ce-8faf-297a185f267a", "9154366f-887e-47aa-b204-00f941f6e7f0", "40913dcd-dc07-4daa-8c51-3cc9ccff3fb7", "207c77b5-9d1f-4bb0-a419-5b72314f561b", "17e7193b-cfff-48dc-bc30-99b0c64743d4", "a8aa194b-9d72-4b7b-9c4a-e760442d85b1", "ec687e40-ddd1-4202-a8d3-ab0e13274db8"], "metadata": {"page_label": "14", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "ba98efca-6d0a-4fc6-b6f1-275db2110d4d": {"node_ids": ["8dc98971-ff36-407f-baf4-857e4f7a0812", "3a08a900-13ed-4fc1-ad37-ebda5e215ec0", "9d0634a1-d312-49c9-8f8d-7fbc31d8f614", "3ac3e06e-fa9b-4ec5-a06a-fd548856600d"], "metadata": {"page_label": "15", "file_name": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf", "title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation", "year": 2021, "author": ["Yuan Gong", "Yu-An Chung", "James Glass"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf"}}, "f4e31095-9605-4685-afdd-66b105bcdd09": {"node_ids": ["5461f41f-ac13-41b1-90d2-d5e64bc5e33c", "dc579127-3253-49da-aeb3-062e737be11c", "1c41e26c-4694-4ed0-91a4-a5e1c6f3f118", "9c063cdd-32a6-415f-8862-92d5a4c888c4", "7a7251a5-bb5f-45b9-9465-cb4a43938c08"], "metadata": {"page_label": "1", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "e0081c88-4609-4b07-9045-7bb488ac4cc8": {"node_ids": ["0e170b4b-3014-4c0a-93be-f9654d7a30f4", "fb848190-f685-4699-9262-6cbda309dca9", "ae05de58-4ee5-46b5-ae54-af30c173d2ba", "2fe777df-c74e-4f6e-a56a-45f4ffedfc82", "114af520-8d01-4b74-b314-ffb9b9e29353"], "metadata": {"page_label": "2", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "6f03113a-842f-4e0f-b55f-a13081a66bbd": {"node_ids": ["a29f4449-2334-42ba-9faa-f4afaa7a408c", "cd6a53d3-a669-4546-9f1a-6b75bea6b480", "03418609-31dd-4a95-bd2e-b6ebb7169b7c", "3bc45822-d595-4fa4-91ee-9d600f6e9c92", "e2a00ccb-5e42-4139-b870-bcb41596224c"], "metadata": {"page_label": "3", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "8e5e3d2a-db6f-494a-ae49-f8478478d306": {"node_ids": ["f05dd48b-f840-437a-a67f-52a7a83d28a6", "f19c4011-fad2-497a-9c01-a65ad2344ee2", "af9a9926-097e-4c2f-956a-b9e1a9a5dc14", "c93ee1c4-2a46-4deb-b086-dd53e1688198", "39ed0dbb-b52d-4367-900a-ecb2830721b6"], "metadata": {"page_label": "4", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "cab177e0-b83a-4c40-acc7-e72a74eef0a3": {"node_ids": ["7e8a4bfa-0d20-49cd-899c-013297293e15", "ffe33ccb-da1f-4b6b-ad5d-cb9b312d7eff", "123ceb7d-069e-4260-a6cf-829b27c2e414", "98ba43b0-c79e-458a-8173-92f853e2f865"], "metadata": {"page_label": "5", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "853809d8-6c17-4b1d-8a37-e0c33c02aee4": {"node_ids": ["b9581539-9c27-4b14-b87a-b1f9453b595e", "5984d5e5-f8c3-4c5e-806f-c5070f7b1b83", "bb03027e-2cb1-4acf-acfa-bd091b17925a", "c09bbfad-90f5-4042-8348-efe554b42b10"], "metadata": {"page_label": "6", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "80593f87-d286-44aa-8dad-b7b1ccd9284b": {"node_ids": ["a8c719d6-765d-450f-8802-d3ddf3999ee4", "bd223082-e5a3-46bf-b9e8-71b22a82d318"], "metadata": {"page_label": "7", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "dbf356b3-ba72-400a-a12e-d464101bcf72": {"node_ids": ["12d40e66-3d9c-4784-8e82-2081fdbdcf40", "f14048be-df00-407c-b1e3-72ae8def3d15"], "metadata": {"page_label": "8", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "37df766f-7beb-4a8c-8cce-46c85d739594": {"node_ids": ["45fd1fd2-357e-41d0-8964-eeb46efd782e", "2a068120-2c9e-4eb1-992c-cd851b2c880a", "601e0d9e-562f-45f0-b7ea-ce42a33f8c29"], "metadata": {"page_label": "9", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "576b46ff-2bef-427f-888a-3f943bc07c53": {"node_ids": ["655bdd10-e7f2-420f-b69d-714523e83cfe", "95b38490-f17e-4157-8114-901692b8c2f0", "3223bc50-70e9-4965-93b0-f7ff73316f88", "9863ef36-c206-4404-b132-8a58ca2bad44", "a70a5a57-8aee-4535-bff8-45f2838c8427", "82ac2bea-8383-4c3d-bfdb-4c9f5d6d280d", "f4f132d1-b564-4a5f-9bf9-0f92bed4362b", "7b227e76-fb11-4d64-80bb-46e5a369f0b3", "eeeefc92-f126-4be8-924a-9cd54b409613", "ec604294-391a-4a95-b2c8-af2be54bcd1b"], "metadata": {"page_label": "10", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "c0c2b433-a1ef-49c8-abad-554bfe8edb5b": {"node_ids": ["2f088893-7aa2-49b1-b979-23278a0de780", "e3824c77-aa3c-4e01-bae2-ae5291aa8e62", "62b31508-96ab-46ad-b9eb-a7dff8268e55", "f68b309a-053d-486e-9355-47074dd4cb4e", "d7d918eb-0368-452e-9178-e09c2fc5ec05"], "metadata": {"page_label": "11", "file_name": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf", "title": "Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization", "year": 2020, "author": ["Qiuqiang Kong", "Yong Xu", "Wenwu Wang", "Mark D. Plumbley"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf"}}, "50ce1c55-a6be-43e5-b962-03da124cd818": {"node_ids": ["d9da70df-7ac2-4a2c-8b9a-75e3d229b35b", "221aa72a-16a1-4638-9041-f14b562b942f", "21be9f8a-4a9e-4f50-aef8-3a0a47d6c5f0", "640f38a9-0f69-4c65-b760-e17c82ea3cb0"], "metadata": {"page_label": "1", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}}, "b857565a-9b31-49ad-8efc-750a044471e2": {"node_ids": ["9e3f9c73-9be0-4ccc-b52f-1346924d5d58", "8afa5a65-48a9-43f4-b1fc-ce8a203ed9bc", "12032502-67b6-4840-af2b-8859c9b891b8", "f9961334-20f5-45a9-ad38-0dbef61f6bb9", "6c23e8af-3854-4dd6-96b7-7468324ce64f"], "metadata": {"page_label": "2", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}}, "63cac535-9ed8-4d00-ae8a-b0acf089d397": {"node_ids": ["61d5de64-924b-48b9-8d09-de3c39535ea6", "6c4b65a9-cc0b-4fb7-9cf1-ee6e590458b9", "882487fa-dd27-4ac4-abfb-9e131b99b8b3", "373d57d1-8de6-4d76-85df-0606c58f68b5", "a6b2c537-b1c4-4e47-9263-978274ca886e"], "metadata": {"page_label": "3", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}}, "67675d31-cf82-42b4-abbb-258c7dc5f285": {"node_ids": ["b149d58f-d752-422a-8418-8258206ff9cc", "b0bab8ab-7a44-49c2-a111-c1a2a98f84bf", "943d269b-0be7-48ad-86a1-9ab8196db174", "d354a9cc-8ba2-4a96-a9b0-f9b661a0271f", "378b365c-395e-459a-961c-846bd5f2c727", "87ccae18-aff8-4c26-a145-0c3516a2818f"], "metadata": {"page_label": "4", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}}, "21dcbe34-54e6-49f0-908c-c2ee0993e7d6": {"node_ids": ["68e7cb03-b125-45ff-96b4-3d7512506ad8", "8237a993-2c64-4f41-972b-950fda4551d1", "eabf95ad-ff91-48b8-b63a-52f4e460cf77", "92a799df-57fc-44a0-a904-36309541ed64", "cea040a4-31df-40ee-8f15-563dfc620e40", "f0de7b9d-4b9c-4207-b27a-df61da25339f", "4c0426b4-04c1-48cc-bd80-135b942abe0c", "c7347f1c-a447-47d5-9a63-fcbc8108c8fc"], "metadata": {"page_label": "5", "file_name": "Streaming keyword spotting on mobile devices.pdf", "title": "Streaming keyword spotting on mobile devices", "year": 2020, "author": ["Oleg Rybakov", "Natasha Kononenko", "Niranjan Subrahmanya", "Mirk\u00f3 Visontai", "Stella Laurenzo"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Streaming keyword spotting on mobile devices.pdf"}}, "836f0f92-11db-468c-b612-489056a194ca": {"node_ids": ["bc9edbf6-0e73-4e9c-9a3e-389317270d4b", "93582014-c191-41b6-bac3-ea65c8b89adb", "8057992a-fd5b-4ed1-9135-289212cf8a68", "56aea8ef-7a53-4807-addc-21f3f1539210"], "metadata": {"page_label": "1", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "f03ceba3-fd20-4c33-9b51-ce430eb87aa4": {"node_ids": ["0e6c5ea1-c5d3-4c93-9488-e350660d73f4", "a35aa3c3-d93a-4ae2-8ac5-b50b847c1d95", "1cdd4e00-4c8f-411b-b39b-63f87193e09c", "86993457-55fe-40fc-a436-a84463a1ab36"], "metadata": {"page_label": "2", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "6b9e01cf-3a0e-4b2f-a3c3-21d8446508d9": {"node_ids": ["423ea5e4-3d44-49d8-8a8d-f035b1888e7e", "dc941632-bfc3-4337-aeaf-ed6fab3b518c", "8667874a-42fb-49d0-86f7-492365e5f8ea", "6e50a6da-6de7-4dde-a84c-e539cbe7c797", "2c270f5c-fe1f-45fc-8755-3201adfa935b"], "metadata": {"page_label": "3", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "c102ab7e-27b1-40a7-90f9-0ad08ea9ae89": {"node_ids": ["4ff7253e-c5ff-4c06-affc-88d34d7cdb95", "afddfe59-4e82-4684-b6fd-8379c37e512e", "e1fe1997-ab22-4675-831d-eb9d5d5f7082", "1d25d7e4-8ab1-4d26-b50d-d27364223436", "5f21446d-d189-4465-acf9-77d0866403a9"], "metadata": {"page_label": "4", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "dd2e921b-bfc5-45f5-a6ff-e244337fe372": {"node_ids": ["32181b2e-94d5-47ba-8d58-1714e8612b8f", "70edd82f-8056-44d1-81fa-67db0367dbbf", "e8ae14b5-c3f8-4638-be51-9bbc31f7971a", "99a51fa7-311d-4292-9437-18b5a44d652d"], "metadata": {"page_label": "5", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "b569b1c3-c0aa-4ece-86ce-fff3fcd83c3c": {"node_ids": ["59e60626-4fd3-4835-b905-084a1344928f", "06a7078d-dc86-4d21-9bd2-bb2445d89fd8", "c65cb3e1-dfa4-4d14-9a84-3fba479cda33", "d50bbab5-6cf5-43dc-a386-c49daac9d6c2", "1a524821-8bb5-44af-9a3b-4846d0b0767d", "8d234997-a658-420a-a333-21fe3536aeb3"], "metadata": {"page_label": "6", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "6465db16-388e-4280-9be3-b7bb772a2365": {"node_ids": ["bccf927f-2bd6-40b6-868c-0c4cc7193ad2", "51d8b82b-51be-4501-8db0-f60bf3a9b389", "acfca739-f62a-447c-b84a-aac58ac59b1d", "82e53b97-815f-4e82-acfa-2842a1acd529", "18dca209-9d59-47fd-a55d-66fefd858a26"], "metadata": {"page_label": "7", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "36529018-68c0-42bc-8369-da139934bcb5": {"node_ids": ["922da7fb-431d-4cd4-8364-3e0406c28359", "a2896f8f-b999-4c25-8b17-2e32fffafaf2", "da2cacc8-f7c3-473b-bde2-848003fcac35", "1bc08df1-dd36-4802-ad4d-dc5c84ebeec0", "03a7d3d5-135b-49d5-ba58-2b174f709eb4"], "metadata": {"page_label": "8", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "5662bdb9-10da-4a26-b339-469fd9bad27f": {"node_ids": ["63ee965a-2361-4478-9ee7-bd3563674599", "efa04065-a027-4aa8-a220-eb40177d28c0", "d4f342aa-04cc-445a-98fc-1e9069da2f43", "e6984cba-241b-4a14-adda-b02d918724f8", "46f76247-981f-48ea-807c-15a942ef9446", "78afdf2b-6c68-4ecd-b0cf-cefd195a1eb2", "3aecdf73-4f6f-44ba-96c5-9358e73957e1", "25ca17ed-8f48-4e41-84a7-8c2521a46492"], "metadata": {"page_label": "9", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "fbbaaeef-d8fa-45cf-8d81-6672450ab87f": {"node_ids": ["48ba4ea2-cf51-4e69-89c3-4085304a6536", "a8ee5287-f68d-44bb-874d-fb2a7b5dfdbb", "93dcf9c9-87a6-411b-ad4f-004002f99851", "bc3a40ae-0e93-413c-a38d-83c572ae60c0", "feda05b9-2396-4b75-baa6-1dde06773c4f", "e3fb2b73-bcb1-424d-b286-492354199179", "82dd1d13-31ec-4178-bdb8-90f7af0e5e02"], "metadata": {"page_label": "10", "file_name": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "year": 2021, "author": ["Li Yuan", "Yunpeng Chen", "Tao Wang", "Weihao Yu", "Yujun Shi", "Zihang Jiang", "Francis E.H. Tay", "Jiashi Feng", "Shuicheng Yan"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf"}}, "2598b908-e27c-40e4-bff7-c4323558bdc3": {"node_ids": ["87b3dae9-4044-40b8-8e51-f874216dedff", "28cf8e56-f160-4ae1-a9bc-4d30601f8a03"], "metadata": {"page_label": "1", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "79db41c1-c88e-42fe-bc5c-fd97f83e7883": {"node_ids": ["45ad2f8a-1035-409d-ab1f-7aedf845e2f8", "dd978134-eb9c-4158-8124-35979d4142dc"], "metadata": {"page_label": "2", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "6470f693-69bb-401e-bcc7-f2affcf98b65": {"node_ids": ["172c70e2-921b-464d-ab19-b0bc20b6d52a", "401b98da-ca32-403a-8b62-45e0ca3ad2c2"], "metadata": {"page_label": "3", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "da4f5c87-39e0-44cb-9751-46269af21cca": {"node_ids": ["ae717b80-b3b7-4386-ba6b-1aca1cb0940d", "d772f735-cae7-4a46-bba6-9fd36dbf21db", "8911a468-1ca2-4e2f-a83b-e31a3a85feac"], "metadata": {"page_label": "4", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "58be4cfa-322c-4520-ae71-69e0820fa62d": {"node_ids": ["7fce9724-49f1-4f3e-93c9-264df1f916c1", "2a05b0ed-3c87-4e2b-af94-bbf7fc66b091", "710b4bbe-d8cd-4096-8833-c9d060166697"], "metadata": {"page_label": "5", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "57fc2f8a-3851-4edc-af16-e8795cccb141": {"node_ids": ["1d7f4586-2f4b-4e73-81d4-fe051ff2f232", "5f69760d-630f-4f13-a890-2a74867b3693", "9a68087a-5362-42d5-aa61-5c7a7ef32c63"], "metadata": {"page_label": "6", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "0e9f733e-b72d-49cb-8540-9f38f0d8df40": {"node_ids": ["eb0ba9ce-e452-4b71-815f-a1c70b89f786"], "metadata": {"page_label": "7", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "1f7c4250-e675-4c6d-9aa8-f29582555fcb": {"node_ids": ["9ae1c1ec-95e2-4a1e-8c2e-66be0db3dce3", "bc09ef96-c917-44f2-8c53-8082890d63ee"], "metadata": {"page_label": "8", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "294cde77-4760-4611-8af9-5c63d4a9bcb2": {"node_ids": ["3c845544-8fec-48f6-ac89-bfe8c66193a4", "dcd3662c-b435-4de0-81ba-9c74ff3c544c"], "metadata": {"page_label": "9", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "92abb9ea-fe6c-4fbd-8169-a3d1070d79c2": {"node_ids": ["79de1b2d-6fe9-4faf-95e7-048bcd9c48ff", "626ce9ad-357f-4db3-81f9-ef1b581fb00f", "ff9cca8e-9e69-477f-b397-bfe8ae4b07ad"], "metadata": {"page_label": "10", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "2d4f32a3-8144-40d3-bdd2-bebba6dd0b62": {"node_ids": ["574cf052-3ffe-42f0-a273-abadf79a11d6", "674e7130-eadf-433e-a5ab-23d28a07124a", "330dd30f-e41c-4975-9b41-37f4f6d08565"], "metadata": {"page_label": "11", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "e886fe27-d76c-48b4-b13e-b76065cc6b40": {"node_ids": ["76309f74-3eac-424d-a104-5596fc3a323c", "bdcf656f-a237-495e-92ee-d54f4a6a8542", "c366b789-6c1d-4064-8b47-f2eba02196fb", "28447695-b388-4789-a2fc-37104a68e88e"], "metadata": {"page_label": "12", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "2185a971-8eb2-49e8-8439-c47cba87c9c6": {"node_ids": ["1007bb23-7ced-4885-8f14-fcbc7981bee2"], "metadata": {"page_label": "13", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "a152ad86-b11e-4a30-8b9d-90dbedc61bc2": {"node_ids": ["80e9038c-ef38-4952-9a2d-3adffc7475cb", "c8462da8-1d84-44bc-bfbe-49ba9cfdcfe7", "47d2b765-aec9-45aa-a1db-4504f761fb26"], "metadata": {"page_label": "14", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "0020a1fd-69de-4308-9b30-cbec66818d18": {"node_ids": ["f7451320-0b38-465e-8641-45e2fc37823c", "9c1cd556-0520-46ca-a237-933434d54356", "dd625431-0963-404c-a8d4-83bf8ccf4644"], "metadata": {"page_label": "15", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "f4eda8d9-bbc2-4c72-87e3-4c28064eb001": {"node_ids": ["48bb1b04-df50-4bc9-b489-44a689eb3257", "ed7f4367-0b6f-4717-9ece-6a572acb0f0d"], "metadata": {"page_label": "16", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "a2ebdd34-a194-42d4-8a89-efda343ed26c": {"node_ids": ["087c61fa-a077-4685-abb8-113ddffd73e8", "d1c33251-f787-41f9-9c02-75127bc54f68", "db7b7e94-8d71-4422-8590-4c32a47334a3"], "metadata": {"page_label": "17", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "5a233e33-461e-4390-be11-fc954f157193": {"node_ids": ["b992d9ed-345b-4c22-861a-255f27489f43"], "metadata": {"page_label": "18", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "de25a3d6-0a4c-47f7-9195-579f23ec0204": {"node_ids": ["d97c231b-cb31-41f5-b4d7-f162d1a5c684", "5bc9ae56-de19-4255-b364-881a666e963a", "7244ecde-0cf7-43c8-87b0-93fc1fd148cb", "d2de6354-44ed-4350-9588-2174cab589fe"], "metadata": {"page_label": "19", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "ae03c1b8-ec1d-4b12-bc5c-8769f70b207b": {"node_ids": ["55cc781a-d5d2-417f-bb03-c381d3c5bc7e", "bdea54e1-4276-452f-9d2d-7ad83bc90991", "22954da0-96af-4adf-97b2-1b84cfa13e0b", "087f112a-772c-471a-83f6-b573310013b4"], "metadata": {"page_label": "20", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "200cf1ce-c920-4d20-9410-3beaf5cf7ada": {"node_ids": ["826d7428-1112-4d56-ab37-f04d8b9a4604", "f7c758b2-fc01-4e62-926a-209f35c25703", "f42976db-5741-44ea-a05a-ea0af7fcaaa1", "1203057c-eef4-4951-b51f-9c02823121e6"], "metadata": {"page_label": "21", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}, "d49ea179-3a65-4890-85ec-6c790399b666": {"node_ids": ["59f40e0c-bad6-43bf-9686-e4cb6bebca32", "f6c66802-ae28-4892-a195-8504f27703b2"], "metadata": {"page_label": "22", "file_name": "Training data-ef\ufb01cient image transformers & distillation through attention.pdf", "title": "Training data-efficient image transformers & distillation through attention", "year": 2021, "author": ["Hugo Touvron", "Matthieu Cordy", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou"], "file_path": "/home/dahong/programs/python/coauthor-interface-rag/backend/../contents/files/Training data-ef\ufb01cient image transformers & distillation through attention.pdf"}}}}