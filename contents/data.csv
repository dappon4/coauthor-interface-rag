path|hash|title|year|author|abstract|method|results|conclusion
../contents/files/AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf|6086d33fbc22546d3e60021b8caab81b10fe39c57dea4ea3b18fabec4a4a94ab|ANIMAGE IS WORTH 16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE|2021|['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby']|This paper explores the application of the Transformer architecture, traditionally used in natural language processing, to image classification tasks. The authors demonstrate that a pure Transformer, when applied directly to sequences of image patches, can achieve competitive results compared to state-of-the-art convolutional networks, especially when pre-trained on large datasets. The Vision Transformer (ViT) shows excellent performance on various benchmarks while requiring fewer computational resources for training.|The Vision Transformer (ViT) model processes images by dividing them into fixed-size patches, which are then linearly embedded and fed into a standard Transformer encoder. The model is trained in a supervised manner on image classification tasks, with a focus on large-scale pre-training on datasets like ImageNet-21k and JFT-300M. The architecture is designed to minimize image-specific inductive biases, relying instead on the Transformer’s ability to learn from data.|The ViT achieves state-of-the-art results on multiple image recognition benchmarks, including 88.55% accuracy on ImageNet and 94.55% on CIFAR-100, outperforming traditional CNNs while requiring significantly less computational resources for training. The model's performance improves substantially when pre-trained on larger datasets, demonstrating the importance of data scale in training effectiveness.|The study concludes that applying Transformers directly to image recognition tasks can yield competitive results without the need for convolutional networks. The Vision Transformer, particularly when pre-trained on large datasets, shows promise for future applications in computer vision, including tasks beyond classification. Further exploration into self-supervised learning methods and scaling the model is suggested as future work.
../contents/files/PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation.pdf|46f6b241d0c8a12de1002ee036fa8f83bfca713281ac86a641465c031b511a1a|PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation|2021|['Yuan Gong', 'Yu-An Chung', 'James Glass']|Audio tagging is an active research area with a wide range of applications. This work presents PSLA, a collection of model agnostic training techniques that can significantly boost model accuracy, including ImageNet pretraining, balanced sampling, data augmentation, label enhancement, and model aggregation. By training an EfficientNet with these techniques, we achieve state-of-the-art mean average precision (mAP) scores on AudioSet and FSD50K, outperforming previous best systems.|We propose a training pipeline that includes ImageNet pretraining, balanced sampling to address class imbalance, data augmentation techniques like time-frequency masking and mix-up training, label enhancement to improve label quality, and model aggregation through weight averaging and ensemble methods. We conduct extensive experiments to evaluate the impact of each technique on model performance.|Our single model achieves an mAP of 0.444 on AudioSet and 0.567 on FSD50K, while an ensemble model reaches an mAP of 0.474 on AudioSet. The proposed techniques lead to significant performance improvements, with ImageNet pretraining yielding a 5.8% relative improvement on the full AudioSet. The combination of balanced sampling and data augmentation results in an 11.6% relative improvement on the full set.|The proposed PSLA framework demonstrates that appropriate training techniques are crucial for improving audio tagging performance. By combining these techniques, we achieve substantial improvements over previous models, highlighting the importance of training strategies in addition to model architecture. This work serves as a recipe for future audio tagging research, providing a set of effective training techniques that can be applied across various model architectures.
../contents/files/PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.pdf|bed8f828f871c4afd08a217ff8cf75289a5b531c1097f881b47d283512942d7f|PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition|2020|['Qiuqiang Kong', 'Yin Cao', 'Turab Iqbal', 'Yuxuan Wang', 'Wenwu Wang', 'Mark D. Plumbley']|Audio pattern recognition is an important research topic in machine learning, encompassing tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification, and sound event detection. This paper introduces pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset, which are transferred to various audio-related tasks. We propose an architecture called Wavegram-Logmel-CNN, achieving a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, surpassing previous systems. PANNs demonstrate state-of-the-art performance across multiple audio pattern recognition tasks, and the source code and pretrained models are publicly available.|The study employs pretrained audio neural networks (PANNs) trained on the AudioSet dataset, which consists of 1.9 million audio clips across 527 sound classes. The architecture includes various convolutional neural networks, particularly the Wavegram-Logmel-CNN, which utilizes both log-mel spectrograms and waveforms as input features. The performance and computational complexity of these networks are analyzed, and transfer learning is applied to adapt PANNs to other audio tasks.|The best PANN system, Wavegram-Logmel-CNN, achieves a mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the previous best system with an mAP of 0.392. The PANNs are successfully transferred to six audio pattern recognition tasks, demonstrating state-of-the-art performance in several of these tasks. The study also highlights the effectiveness of data balancing and augmentation techniques in improving model performance.|The research presents PANNs as a significant advancement in audio pattern recognition, achieving state-of-the-art results on AudioSet tagging and other audio tasks. The findings suggest that PANNs can be effectively transferred to new tasks, particularly when fine-tuned on limited data. Future work will focus on extending PANNs to additional audio pattern recognition challenges.
../contents/files/CONVOLUTION-AUGMENTED TRANSFORMER FOR SEMI-SUPERVISED SOUND EVENT DETECTION.pdf|d6ec21dbebf49f1fe4adacf42834d3a51e9d99a32ce3eb3dbfcbed6a38527e48|Convolution-Augmented Transformer for Semi-Supervised Sound Event Detection|2020|['Koichi Miyazaki', 'Tatsuya Komatsu', 'Tomoki Hayashi', 'Shinji Watanabe', 'Tomoki Toda', 'Kazuya Takeda']|This technical report describes our submission system for DCASE2020 Task4: sound event detection and separation in domestic environments. Our model employs conformer blocks, which combine the self-attention and depth-wise convolution networks, to efficiently capture the global and local context information of an audio feature sequence. We further improve performance by utilizing a mean teacher semi-supervised learning technique, data augmentation, and post-processing optimized for each sound event class. The proposed method achieves an event-based macro F1 score of 50.7% on the validation set, significantly outperforming the baseline score of 34.8%.|We propose two neural network models for sound event detection: a Transformer-based model and a Conformer-based model. Both models utilize self-attention mechanisms to capture local and global context information. We implement semi-supervised learning using the mean teacher technique, data augmentation methods (time-shifting and mixup), and class-dependent post-processing. We also perform score fusion to improve generalization performance.|The proposed models significantly outperform the baseline system, achieving an event-based macro F1 score of 50.7% with score fusion. The Conformer-based model achieved the best performance with an F1 score of 41.7% without data augmentation, and the combination of time-shifting and mixup data augmentation methods further improved the performance to 46.0%.|Our submission system for DCASE2020 Task4 effectively utilizes self-attention architectures, data augmentation techniques, class-dependent post-processing, and score fusion to improve sound event detection performance. Future work will focus on class-wise performance analysis and integrating source separation techniques.
../contents/files/Training data-efﬁcient image transformers & distillation through attention.pdf|ed7c405a589921b27a1235cab2801bcbaa9d93fc760478b9c2dc416ec12c0e3c|Training data-efficient image transformers & distillation through attention|2021|['Hugo Touvron', 'Matthieu Cord', 'Matthijs Douze', 'Francisco Massa', 'Alexandre Sablayrolles', 'Hervé Jégou']|This work presents a method for training competitive convolution-free transformers for image classification using only the ImageNet dataset. The proposed vision transformer (DeiT) achieves a top-1 accuracy of 83.1% on ImageNet with 86M parameters, trained on a single machine in less than 3 days. A novel teacher-student distillation strategy is introduced, utilizing a distillation token that allows the student to learn from the teacher through attention, leading to competitive results against convolutional networks (convnets).|The authors trained a vision transformer (DeiT) on ImageNet using a single 8-GPU node in under 3 days. They introduced a teacher-student distillation strategy that employs a distillation token to facilitate learning from a teacher model, which can be a convnet. The training involved extensive data augmentation and hyperparameter tuning to optimize performance.|The DeiT model achieved a top-1 accuracy of 83.1% on ImageNet, with the distilled model (DeiT⚗) reaching up to 85.2% accuracy. The results showed that the distillation token improved performance significantly compared to traditional distillation methods. The models also performed competitively on various downstream tasks, demonstrating their generalization capabilities.|The study concludes that the proposed DeiT models can achieve competitive performance with convnets while being trained on a smaller dataset and with fewer resources. The introduction of the distillation token enhances the learning process, suggesting that transformers can be a viable alternative to convnets in image classification tasks. The authors emphasize the potential for further improvements through tailored data augmentation strategies for transformers.
../contents/files/Streaming keyword spotting on mobile devices.pdf|7cd4f4821eb9618f3c85d7cd21c1c1d6c7da833bb3ed3ae0df28c28f0e148a39|Streaming keyword spotting on mobile devices|2020|['Oleg Rybakov', 'Natasha Kononenko', 'Niranjan Subrahmanya', 'Mirkó Visontai', 'Stella Laurenzo']|This work explores the latency and accuracy of keyword spotting (KWS) models in streaming and non-streaming modes on mobile phones. We designed a Tensorflow/Keras based library for automatic conversion of non-streaming models to streaming ones, allowing for benchmarking multiple KWS models and demonstrating tradeoffs between latency and accuracy. We also introduced novel KWS models with multi-head attention, achieving a 10% reduction in classification error on Google speech commands datasets V2. The streaming library and experiments are open-sourced.|We implemented a Keras streaming wrapper layer for automatic conversion of Keras models to streaming inference. The process includes designing a model, training it, converting it to a streaming model, and benchmarking its performance on a mobile device. We evaluated various KWS architectures including DNN, CNN, RNN, and CRNN, and compared their performance in both streaming and non-streaming modes.|The results showed that the streaming models, particularly SVDF, CRNN, and GRU, achieved high accuracy with significantly lower latency compared to non-streaming models. The MHAtt-RNN model reduced classification error by 10% compared to the state-of-the-art models on datasets V2. The latency of streaming models was found to be approximately 10x lower than that of non-streaming models for convolutional architectures and around 20x lower for RNN models.|We successfully built a library for end-to-end model conversion to streaming inference on mobile devices, simplifying deployment and improving accuracy. The introduction of multi-head attention in the KWS models led to significant performance improvements. All code and experimental results are open-sourced.
../contents/files/Attention Is All You Need.pdf|e17b027654c756afb36febb51dcd6169521527829fe474052d30fc396db6dad7|Attention Is All You Need|2017|['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Łukasz Kaiser', 'Illia Polosukhin']|We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.|The Transformer model architecture consists of an encoder-decoder structure that uses stacked self-attention and point-wise fully connected layers. The encoder maps an input sequence to continuous representations, while the decoder generates an output sequence. The model employs multi-head attention mechanisms and position-wise feed-forward networks, with residual connections and layer normalization applied to each sub-layer.|The Transformer outperforms previous state-of-the-art models on the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.8 respectively. The model is trained significantly faster than architectures based on recurrent or convolutional layers, with the base model surpassing all previously published models and ensembles at a fraction of the training cost.|The Transformer is the first sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It achieves state-of-the-art results in translation tasks and demonstrates potential for application to other tasks, with plans to extend its use to different input and output modalities.
../contents/files/Conformer: Convolution-augmented Transformer for Speech Recognition.pdf|078def86eeb484df27fd84e6ae12a85c0f29f8ddb12682eb875be6054eabc116|Conformer: Convolution-augmented Transformer for Speech Recognition|2020|['Anmol Gulati', 'James Qin', 'Chung-Cheng Chiu', 'Niki Parmar', 'Yu Zhang', 'Jiahui Yu', 'Wei Han', 'Shibo Wang', 'Zhengdong Zhang', 'Yonghui Wu', 'Ruoming Pang']|This work proposes Conformer, a convolution-augmented transformer model for automatic speech recognition (ASR) that combines the strengths of convolutional neural networks (CNNs) and transformers. The model achieves state-of-the-art performance on the LibriSpeech benchmark, with a word error rate (WER) of 2.1%/4.3% without a language model and 1.9%/3.9% with an external language model. The Conformer model is designed to efficiently capture both local and global dependencies in audio sequences.|The Conformer architecture integrates convolutional modules with multi-headed self-attention in a sandwich structure, surrounded by feed-forward layers. The model is evaluated on the LibriSpeech dataset, with various configurations tested to optimize performance, including different numbers of attention heads and convolution kernel sizes.|The Conformer model outperforms previous state-of-the-art models on the LibriSpeech dataset, achieving a WER of 2.1%/4.3% without a language model and 1.9%/3.9% with one. The model shows competitive performance even with a smaller configuration of 10M parameters, achieving 2.7%/6.3% WER on the test/testother datasets.|The Conformer architecture effectively combines CNNs and transformers, leading to improved accuracy in speech recognition tasks with fewer parameters compared to previous models. The study highlights the importance of convolution modules in enhancing the performance of transformer-based architectures for ASR.
../contents/files/Audio Transformer.pdf|e0cda36fff509a5bee3f73bf579bffc614dbb5e8dd68fc29244bdbb9961fa609|AST: Audio Spectrogram Transformer|2021|['Yuan Gong', 'Yu-An Chung', 'James Glass']|In this paper, we introduce the Audio Spectrogram Transformer (AST), a convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, achieving state-of-the-art results on AudioSet, ESC-50, and Speech Commands V2. Our findings suggest that CNNs are not essential for audio classification tasks, and AST's architecture allows for variable-length inputs and faster convergence during training.|The AST model processes audio spectrograms by splitting them into overlapping patches, which are then embedded and input into a Transformer encoder. The model utilizes transfer learning from a Vision Transformer pretrained on ImageNet to enhance performance. We conduct experiments on multiple datasets, including AudioSet, ESC-50, and Speech Commands V2, using various training strategies and data augmentations.|AST achieves 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2, outperforming previous state-of-the-art models. The model demonstrates superior performance with fewer parameters and faster convergence compared to CNN-attention hybrid models.|The Audio Spectrogram Transformer (AST) demonstrates that convolutional neural networks are not necessary for effective audio classification. AST's purely attention-based architecture provides a simpler and more efficient alternative, achieving state-of-the-art results across multiple audio classification tasks.
../contents/files/Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization.pdf|e269fa19931e3a3c011c4b5470b7a3e3099ad67ca347622343f39840988ad207|Sound Event Detection of Weakly Labelled Data With CNN-Transformer and Automatic Threshold Optimization|2020|['Qiuqiang Kong', 'Yong Xu', 'Wenwu Wang', 'Mark D. Plumbley']|This paper addresses the challenge of sound event detection (SED) in weakly labelled datasets, where only audio tags are available without onset and offset times. We propose a convolutional neural network transformer (CNN-Transformer) for audio tagging and SED, which performs comparably to convolutional recurrent neural networks (CRNNs). Additionally, we introduce an automatic threshold optimization method to improve detection accuracy, achieving state-of-the-art results in audio tagging and SED tasks.|We compare segment-wise and clip-wise training methods for SED using a CNN-Transformer architecture. The first stage of our automatic threshold optimization method focuses on optimizing the system based on metrics independent of thresholds, while the second stage optimizes the thresholds based on metrics that depend on them. We evaluate our methods on the DCASE 2017 Task 4 dataset, which contains weakly labelled audio clips.|Our proposed CNN-Transformer achieves an audio tagging F1 score of 0.646, improving from 0.629 without threshold optimization. For sound event detection, we achieve an F1 score of 0.584, up from 0.564 without threshold optimization. The results indicate that our methods outperform previous systems in the DCASE 2017 Task 4 challenge.|The study demonstrates that the CNN-Transformer can effectively handle weakly labelled data for sound event detection, achieving competitive results compared to CRNNs. The automatic threshold optimization method significantly enhances performance, suggesting its potential for future applications in large-scale sound event detection tasks.
../contents/files/Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.pdf|86f0c6b406e192e27f6634c9db4dcce3ff163310477cc73068254d1eff997bc7|Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet|2021|['Li Yuan', 'Yunpeng Chen', 'Tao Wang', 'Weihao Yu', 'Yujun Shi', 'Zihang Jiang', 'Francis E.H. Tay', 'Jiashi Feng', 'Shuicheng Yan']|Transformers have been explored for vision tasks, but the Vision Transformer (ViT) struggles with performance when trained from scratch on datasets like ImageNet. This paper identifies two main limitations of ViT: inadequate modeling of local structures and a redundant attention backbone. To address these issues, we propose the Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates a layer-wise Tokens-to-Token transformation to better model local structures and an efficient backbone inspired by CNN architecture. T2T-ViT achieves significant performance improvements over ViT and comparable performance to ResNets and MobileNets while reducing parameters and computation costs.|The T2T-ViT model introduces a Tokens-to-Token (T2T) module that progressively transforms images into tokens by aggregating neighboring tokens, thus capturing local structures. The backbone of T2T-ViT is designed with a deep-narrow architecture, inspired by CNNs, to enhance feature richness and reduce redundancy. The model is trained from scratch on ImageNet, and various architecture designs are explored to optimize performance.|T2T-ViT demonstrates superior performance compared to ViT, achieving over 81.5% top-1 accuracy on ImageNet with significantly fewer parameters and MACs. It outperforms ResNets of similar size and achieves comparable results to MobileNets. For instance, T2T-ViT with 21.5M parameters achieves 83.3% accuracy at a resolution of 384x384. The model also shows effectiveness in transfer learning to downstream datasets like CIFAR10 and CIFAR100.|The T2T-ViT model effectively addresses the limitations of ViT by incorporating a novel tokenization process and an efficient backbone design. It achieves competitive performance against CNNs without requiring large-scale pretraining datasets, paving the way for future developments in transformer-based models for vision tasks.
